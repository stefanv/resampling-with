[["index.html", "Resampling statistics R edition", " Resampling statistics Julian Lincoln Simon Ian Nimmo-Smith Matthew Brett R edition There are two editions of this book; one with examples in the Python programming language,1 and another with examples in the R language.2 This is the R edition. The files on this website are free to view and download. We release the content under the Creative Commons Attribution / No Derivatives 4.0 License. If you’d like a physical copy of the book, you should be able to order it from Sage, when it is published. We wrote this book in RMarkdown with bookdown. It is automatically rebuilt from source by travis. https://www.python.org↩︎ https://www.r-project.org↩︎ "],["preface-to-the-third-edition.html", "Preface to the third edition", " Preface to the third edition The true novelty of resampling as teaching method Statistics without the agonizing pain Simon’s insight The rise of resampling in data science Programming and statistics Ian Nimmo-Smith Matthew Brett "],["preface-to-the-second-edition.html", "Preface to the second edition3 Brief history of the resampling method Brief history of statistics", " Preface to the second edition3 Brief history of the resampling method This book describes a revolutionary—but now fully accepted—approach to probability and statistics. Monte Carlo resampling simulation takes the mumbo-jumbo out of statistics and enables even beginning students to understand completely everything that is done. Before we go further, let’s make the discussion more concrete with an example. Ask a class: What are the chances that three of a family’s first four children will be girls? After various entertaining class suggestions about procreating four babies, or surveying families with four children, someone in the group always suggests flipping a coin. This leads to valuable student discussion about whether the probability of a girl is exactly half (there are about 105 males born for each 100 females), whether .5 is a satisfactory approximation, whether four coins flipped once give the same answer as one coin flipped four times, and so on. Soon the class decides to take actual samples of coin flips. And students see that this method quickly arrives at estimates that are accurate enough for most purposes. Discussion of what is “accurate enough” also comes up, and that discussion is valuable, too. The Monte Carlo method itself is not new. Near the end of World War II, a group of physicists at the Rand Corp. began to use random-number simulations to study processes too complex to handle with formulas. The name “Monte Carlo” came from the analogy to the gambling houses on the French Riviera. The application of Monte Carlo methods in teaching statistics also is not new. Simulations have often been used to illustrate basic concepts. What is new and radical is using Monte Carlo methods routinely as problem-solving tools for everyday problems in probability and statistics. From here on, the related term resampling will be used throughout the book. Resampling refers to the use of the observed data or of a data generating mechanism (such as a die) to produce new hypothetical samples, the results of which can then be analyzed. The term computer-intensive methods also is frequently used to refer to techniques such as these. The history of resampling is as follows: In the mid-1960’s, I noticed that most graduate students—among them many who had had several advanced courses in statistics—were unable to apply statistical methods correctly in their social science research. I sympathized with them. Even many experts are unable to understand intuitively the formal mathematical approach to the subject. Clearly, we need a method free of the formulas that bewilder almost everyone. The solution is as follows: Beneath the logic of a statistical inference there necessarily lies a physical process. The resampling methods described in this book allow us to work directly with the underlying physical model by simulating it, rather than describing it with formulae. This general insight is also the heart of the specific technique Bradley Efron felicitously labeled ‘the bootstrap’ (1982), a device I introduced in 1969 that is now the most commonly used, and best known, resampling method. The resampling approach was first tried with graduate students in 1966, and it worked exceedingly well. Next, under the auspices of the father of the “new math,” Max Beberman, I “taught” the method to a class of high school seniors in 1967. The word “taught” is in quotation marks because the pedagogical essence of the resampling approach is that the students discover the method for themselves with a minimum of explicit instruction from the teacher. The first classes were a success and the results were published in 1969 (Julian L Simon and Holmes 1969). Three PhD experiments were then conducted under Kenneth Travers’ supervision, and they all showed overwhelming superiority for the resampling method (Simon, Atkinson, and Shevokas 1976). Subsequent research has confirmed this success. The method was first presented at some length in the 1969 edition of my book Basic Research Methods in Social Science (Julian Lincoln Simon 1969) (third edition with Paul Burstein (1985)). For some years, the resampling method failed to ignite interest among statisticians. While many factors (including the accumulated intellectual and emotional investment in existing methods) impede the adoption of any new technique, the lack of readily available computing power and tools was an obstacle. (The advent of the personal computer in the 1980s changed that, of course.) Then in the late 1970s, Efron began to publish formal analyses of the bootstrap—an important resampling application. Interest among statisticians has exploded since then, in conjunction with the availability of easy, fast, and inexpensive computer simulations. The bootstrap has been the most widely used, but across-the-board application of computer intensive methods now seems at hand. As Noreen (1989) noted, “there is a computer-intensive alternative to just about every conventional parametric and nonparametric test.” And the bootstrap method has now been hailed by an official American Statistical Association volume as the only “great breakthrough” in statistics since 1970 (Kotz and Johnson 1992). It seems appropriate now to offer the resampling method as the technique of choice for beginning students as well as for the advanced practitioners who have been exploring and applying the method. Though the term “computer-intensive methods” is nowadays used to describe the techniques elaborated here, this book can be read either with or without the accompanying use of the computer. However, as a practical matter, users of these methods are unlikely to be content with manual simulations if a quick and simple computer-program alternative is available. The ultimate test of the resampling method is how well you, the reader, learn it and like it. But knowing about the experiences of others may help beginners as well as experienced statisticians approach the scary subject of statistics with a good attitude. Students as early as junior high school, taught by a variety of instructors and in other languages as well as English, have—in a matter of 6 or 12 short hours—learned how to handle problems that students taught conventionally do not learn until advanced university courses. And several controlled experimental studies show that, on average, students who learn this method are more likely to arrive at correct solutions than are students who are taught conventional methods. Best of all, the experiments comparing the resampling method against conventional methods show that students enjoy learning statistics and probability this way, and they don’t suffer statistics panic. This experience contrasts sharply with the reactions of students learning by conventional methods. (This is true even when the same teachers teach both methods as part of an experiment.) A public offer: The intellectual history of probability and statistics began with gambling games and betting. Therefore, perhaps a lighthearted but very serious offer would not seem inappropriate here: I hereby publicly offer to stake $5,000 in a contest against any teacher of conventional statistics, with the winner to be decided by whose students get the larger number of simple and complex numerical problems correct, when teaching similar groups of students for a limited number of class hours—say, six or ten. And if I should win, as I am confident that I will, I will contribute the winnings to the effort to promulgate this teaching method. (Here it should be noted that I am far from being the world’s most skillful or charming teacher. It is the subject matter that does the job, not the teacher’s excellence.) This offer has been in print for many years now, but no one has accepted it. The early chapters of the book contain considerable discussion of the resampling method, and of ways to teach it. This material is intended mainly for the instructor; because the method is new and revolutionary, many instructors appreciate this guidance. But this didactic material is also intended to help the student get actively involved in the learning process rather than just sitting like a baby bird with its beak open waiting for the mother bird to drop morsels into its mouth. You may skip this didactic material, of course, and I hope that it does not get in your way. But all things considered, I decided it was better to include this material early on rather than to put it in the back or in a separate publication where it might be overlooked. Brief history of statistics In ancient times, mathematics developed from the needs of governments and rich men to number armies, flocks, and especially to count the taxpayers and their possessions. Up until the beginning of the 20th century, the term statistic meant the number of something—soldiers, births, taxes, or what-have-you. In many cases, the term statistic still means the number of something; the most important statistics for the United States are in the Statistical Abstract of the United States . These numbers are now known as descriptive statistics. This book will not deal at all with the making or interpretation of descriptive statistics, because the topic is handled very well in most conventional statistics texts. Another stream of thought entered the field of probability and statistics in the 17th century by way of gambling in France. Throughout history people had learned about the odds in gambling games by repeated plays of the game. But in the year 1654, the French nobleman Chevalier de Mere asked the great mathematician and philosopher Pascal to help him develop correct odds for some gambling games. Pascal, the famous Fermat, and others went on to develop modern probability theory. Later these two streams of thought came together. Researchers wanted to know how accurate their descriptive statistics were—not only the descriptive statistics originating from sample surveys, but also the numbers arising from experiments. Statisticians began to apply the theory of probability to the accuracy of the data arising from sample surveys and experiments, and that became the theory of inferential statistics . Here we find a guidepost: probability theory and statistics are relevant whenever there is uncertainty about events occurring in the world, or in the numbers describing those events. Later, probability theory was also applied to another context in which there is uncertainty—decision-making situations. Descriptive statistics like those gathered by insurance companies—for example, the number of people per thousand in each age bracket who die in a five-year period—have been used for a long time in making decisions such as how much to charge for insurance policies. But in the modern probabilistic theory of decision-making in business, politics and war, the emphasis is different; in such situations the emphasis is on methods of combining estimates of probabilities that depend upon each other in complicated ways in order to arrive at the best decision. This is a return to the gambling origins of probability and statistics. In contrast, in standard insurance situations (not including war insurance or insurance on a dancer’s legs) the probabilities can be estimated with good precision without complex calculation, on the basis of a great many observations, and the main statistical task is gathering the information. In business and political decision-making situations, however, one often works with probabilities based on very limited information—often little better than guesses. There the task is how best to combine these guesses about various probabilities into an overall probability estimate. Estimating probabilities with conventional mathematical methods is often so complex that the process scares many people. And properly so, because its difficulty leads to errors. The statistics profession worries greatly about the widespread use of conventional tests whose foundations are poorly understood. The wide availability of statistical computer packages that can easily perform these tests with a single command, regardless of whether the user understands what is going on or whether the test is appropriate, has exacerbated this problem. This led John Tukey to turn the field toward descriptive statistics with his techniques of “exploratory data analysis” (Tukey 1977). These descriptive methods are well described in many texts. Probabilistic analysis also is crucial, however. Judgments about whether the government should allow a new medicine on the market, or whether an operator should adjust a screw machine, require more than eyeball inspection of data to assess the chance variability. But until now the teaching of probabilistic statistics, with its abstruse structure of mathematical formulas, mysterious tables of calculations, and restrictive assumptions concerning data distributions—all of which separate the student from the actual data or physical process under consideration—have been an insurmountable obstacle to intuitive understanding. Now, however, the resampling method enables researchers and decision-makers in all walks of life to obtain the benefits of statistics and predictability without the shortcomings of conventional methods, free of mathematical formulas and restrictive assumptions. Resampling’s repeated experimental trials on the computer enable the data (or a data-generating mechanism representing a hypothesis) to express their own properties, without difficult and misleading assumptions. So — good luck. I hope that you enjoy the book and profit from it. Julian Lincoln Simon 1997 References "],["introduction.html", "1 Introduction 1.1 Uses of Probability and Statistics 1.2 What kinds of problems shall we solve? 1.3 Probabilities and decisions 1.4 Types of statistics 1.5 Limitations of probability and statistics 1.6 Why is Statistics Such a Difficult Subject?", " 1 Introduction 1.1 Uses of Probability and Statistics This chapter introduces you to probability and statistics. First come examples of the kinds of practical problems that this knowledge can solve for us. Next this Introduction discusses the relationship of probabilities to decisions. Then comes a discussion of the two general types of statistics, descriptive and inferential. Following this is a discussion of the limitations of probability and statistics. And last is a discussion of why statistics can be such a difficult subject. Most important, this chapter describes the types of problems the book will tackle. Because the term “statistic” often scares and confuses people— and indeed, the term has several sorts of meanings—the chapter includes a short section on “Types of Statistics.” Descriptive statistics are numbers that summarize the information contained in a group of data. Inferential statistics are procedures to estimate unknown quantities; that is, these procedures infer estimates and conclusions based on whatever descriptive statistics are available. At the foundation of sound decision-making lies the ability to make accurate estimates of the probabilities of future events. Probabilistic problems confront everyone—from the business person considering plant expansion, to the scientist testing a new wonder drug, to the individual deciding whether to carry an umbrella to work. 1.2 What kinds of problems shall we solve? These are some examples of the kinds of problems that we can handle with the methods described in this book: You are a doctor trying to develop a cure for cancer. Currently you are working on a medicine labeled CCC. You have data from patients to whom medicine CCC was given. You want to judge on the basis of those results whether CCC really cures cancer or whether it is no better than a sugar pill. You are the campaign manager for the Republicrat candidate for President of the United States. You have the results from a recent poll taken in New Hampshire. You want to know the chance that your candidate would win in New Hampshire if the election were held today. You are the manager and part owner of one of several contractors providing ambulances to a hospital. You own 12 ambulances. The chance that any one ambulance will be unfit for service on any given day is about one in ten. You want to know the chance on a particular day— tomorrow—that three or more of them will be out of action. A machine gauged to produce screws 1.000 inches long produces a batch on Tuesday that averaged 1.010 inches. Given the record of screws produced by this machine over the past month, we want to know whether something about the machine has changed, or whether this unusual batch has occurred just by chance. The core of all these problems, and of the others that we will deal with in this book, is that you want to know the “chance” or “probability”—different words for the same idea—that some event will or will not happen, or that something is true or false. To put it another way, we want to answer questions about “What is the probability that...?”, given the body of information that you have in hand. The question “What is the probability that...?” is usually not the ultimate question that interests us at a given moment. Eventually, a person wants to use the estimated probability to help make a decision concerning some action one might take. These are the kinds of decisions, related to the questions about probability stated above, that ultimately we would like to make: Should you (the researcher) advise doctors to prescribe medicine CCC for patients, or, should you (the researcher) continue to study CCC before releasing it for use? A related matter: should you and other research workers feel sufficiently encouraged by the results of medicine CCC so that you should continue research in this general direction rather than turning to some other promising line of research? These are just two of the possible decisions that might be influenced by the answer to the question about the probability that medicine CCC cures cancer. Should you advise the Republicrat presidential candidate to go to New Hampshire to campaign? If the poll tells you conclusively that he or she will not win in New Hampshire, you might decide that it is not worthwhile investing effort to campaign there. Similarly, if the poll tells you conclusively that he or she surely will win in New Hampshire, you probably would not want to campaign further there. But if the poll is not conclusive in one direction or the other, you might choose to invest the effort to campaign in New Hampshire. Analysis of the chances of winning in New Hampshire based on the poll data can help you make this decision sensibly. Should your firm buy more ambulances ? Clearly the answer to this question is affected by the probability that a given number of your ambulances will be out of action on a given day. But of course this estimated probability will be only one part of the decision. Should we adjust the screw-making machine after it produces the batch of screws averaging 1.010 inches? If its performance has not changed, and the unusual batch we observed was just the result of random variability, adjusting the machine could render it more likely to produce off-target screws in the future. The kinds of questions to which we wish to find probabilistic and statistical answers may be found throughout the social, biological and physical sciences; in business; in politics; in engineering (concerning such spectacular projects as the flight to the moon); and in most other forms of human endeavor. 1.3 Probabilities and decisions There are two differences between questions about probabilities and the ultimate decision problems: Decision problems always involve evaluation of the consequences —that is, taking into account the benefits and the costs of the consequences—whereas pure questions about probabilities are estimated without evaluations of the consequences. Decision problems often involve a complex combination of sets of probabilities and consequences, together with their evaluations. For example: In the case of the contractor’s ambulances, it is clear that there will be a monetary loss to the contractor if she makes a commitment to have 9 ambulances available for tomorrow and then cannot produce that many. Furthermore, the contractor must take into account the further consequence that there may be a loss of goodwill for the future if she fails to meet her obligations tomorrow—and then again there may not be any such loss; and if there is such loss of goodwill it might be a loss worth $10,000 or $20,000 or $30,000. Here the decision problem involves not only the probability that there will be fewer than 9 ambulances tomorrow but also the immediate monetary loss and the subsequent possible losses of goodwill, and the valuation of all these consequences. Continuing with the decision concerning whether to do more research on medicine CCC: If you do decide to continue research on CCC, (a) you may, or (b) you may not, come up with an important general cure within, say, the next 3 years. If you do come up with such a general cure, of course it will have very great social benefits. Furthermore, (c) if you decide not to do further research on CCC now, you can direct your time and that of other people to research in other directions, with some chance that the other research will produce a less-general but nevertheless useful cure for some relatively infrequent forms of cancer. Those three possibilities have different social benefits. The probability that medicine CCC really has some curative effect on cancer, as judged by your prior research, obviously will influence your decision on whether or not to do more research on medicine CCC. But that judgment about the probability is only one part of the overall web of consequences and evaluations that must be taken into account when making your decision whether or not to do further research on medicine CCC. Why does this book limit itself to the specific probability questions when ultimately we are interested in decisions? A first reason is division of labor. The more general aspects of the decision-making process in the face of uncertainty are treated well in other books. This book’s special contribution is its new approach to the crucial process of estimating the chances that an event will occur. Second, the specific elements of the overall decision-making process taught in this book belong to the interrelated subjects of probability theory and inferential statistics . Though probabilistic and inferential-statistical theory ultimately is intended to be part of the general decision-making process, often only the estimation of probabilities is done systematically, and the rest of the decision-making process—for example, the decision whether or not to proceed with further research on medicine CCC—is done in informal and unsystematic fashion. This is regrettable, but the fact that this is standard practice is an additional reason why the treatment of inferential statistics and probability in this book is sufficiently complete. A third reason that this book covers only inferential statistics and not decision statistics is because most college and university statistics courses and books are limited to inferential statistics. 1.4 Types of statistics The term statistics sometimes causes confusion and therefore needs explanation. A statistic is a number . There are two kinds of statistics, summarization (descriptive) statistics and probability statistics. The most important summarization statistics are the total, the distribution, measures of the center of a distribution such as the mean and median, the range, and other measures of variation. Such statistics are nothing new to you; you have been using many of them all your life. Inferential statistics, which this book deals with, uses descriptive statistics as its input. Inferential statistics can be used for two purposes: to aid scientific understanding by estimating the probability that a statement is true or not, and to aid in making sound decisions by estimating which alternative among a range of possibilities is most desirable. 1.5 Limitations of probability and statistics Statistical testing is not equivalent to research, and research is not the same as statistical testing. Rather, statistical inference is a handmaiden of research, often but not always necessary in the research process. A working knowledge of the basic ideas of statistics, especially the elements of probability, is unsurpassed in its general value to everyone in a modern society. Statistics and probability help clarify one’s thinking and improve one’s capacity to deal with practical problems and to understand the world. To be efficient, a social scientist or decision-maker is almost certain to need statistics and probability. On the other hand, important research and top-notch decision-making have been done by people with absolutely no formal knowledge of statistics. And a limited study of statistics sometimes befuddles students into thinking that statistical principles are guides to research design and analysis. This mistaken belief only inhibits the exercise of sound research thinking. Kinsey long ago put it this way: … no statistical treatment can put validity into generalizations which are based on data that were not reasonably accurate and complete to begin with. It is unfortunate that academic departments so often offer courses on the statistical manipulation of human material to students who have little understanding of the problems involved in securing the original data. … When training in these things replaces or at least precedes some of the college courses on the mathematical treatment of data, we shall come nearer to having a science of human behavior. (Kinsey, Pomeroy, and Martin 1948, p 35). In much—even most—research in social and physical sciences, statistical testing is not necessary. Where there are large differences between different sorts of circumstances—for example, if a new medicine cures 90 patients out of 100 and the old medicine cures only 10 patients out of 100—we do not need refined statistical tests to tell us whether or not the new medicine really has an effect. And the best research is that which shows large differences, because it is the large effects that matter. If the researcher finds that s/he must use refined statistical tests to reveal whether there are differences, this sometimes means that the differences do not matter much. To repeat, then, some or even much research—especially in the physical and biological sciences—does not need the kind of statistical manipulation that will be described in this book. But most decision problems do need the kind of probabilistic and statistical input that is described in this book. Another matter: If the raw data are of poor quality, probabilistic and statistical manipulation cannot be very useful. In the example of the contractor and her ambulances, if the contractor’s estimate that a given ambulance has a one-in-ten chance of being unfit for service out-of-order on a given day is very inaccurate, then our calculation of the probability that three or more ambulances will be out of order on a given day will not be helpful, and may be misleading. To put it another way, one cannot make bread without flour, yeast, and water. And good raw data are the flour, yeast and water necessary to get an accurate estimate of a probability. The most refined statistical and probabilistic manipulations are useless if the input data are poor—the result of unrepresentative samples, uncontrolled experiments, inaccurate measurement, and the host of other ways that information gathering can go wrong. (See Simon and Burstein (1985) for a catalog of the obstacles to obtaining good data.) Therefore, we should constantly direct our attention to ensuring that the data upon which we base our calculations are the best it is possible to obtain. 1.6 Why is Statistics Such a Difficult Subject? Why is statistics such a tough subject for so many people? “Among mathematicians and statisticians who teach introductory statistics, there is a tendency to view students who are not skillful in mathematics as unintelligent,” say two of the authors of a popular introductory text (McCabe and McCabe 1989, p 2). As these authors imply, this view is out-and-out wrong; lack of general intelligence on the part of students is not the root of the problem. Scan this book and you will find almost no formal mathematics. Yet nearly every student finds the subject very difficult— as difficult as anything taught at universities. The root of the difficulty is that the subject matter is extremely difficult. Let’s find out why . It is easy to find out with high precision which movie is playing tonight at the local cinema; you can look it up on the web or call the cinema and ask. But consider by contrast how difficult it is to determine with accuracy: what will be the result of more than a hundred million Americans voting for president a month hence; the best attempt usually is a sample of 2000 people, selected in some fashion or another that is far from random, weeks before the election, asked questions that are by no means the same as the actual voting act, and so on; to assess, on the basis of the data on the prices of a single brand of liquor in 42 states, 16 of which have liquor distribution systems run by state government and 26 run privately, whether prices will be higher in a state newly admitted to the union if it chooses the private system; or how men feel about women and vice versa. The cleverest and wisest people have pondered for thousands of years how to obtain answers to questions like these, and made little progress. Dealing with uncertainty was completely outside the scope of the ancient philosophers. It was not until two or three hundred years ago that people began to make any progress at all on these sorts of questions, and it was only about one century ago that we began to have reasonably competent procedures—simply because the problems are inherently difficult. So it is no wonder that the body of these methods is difficult. So: The bad news is that the subject is extremely difficult. The good news is that you—and that means you —can understand it with hard thinking, even if you have no mathematical background beyond arithmetic and you think that you have no mathematical capability. That’s because the difficulty lies in such matters as pin-pointing the right question, but not in any difficulties of mathematical manipulation. References "],["about-the-technology.html", "2 About the technology The environment Running the code on your own computer", " 2 About the technology This version of the book uses the R4 programming language to implement resampling algorithms. The authors of the R language wrote it specifically for data analysis and statistics. If you are used to other programming languages, R won’t be very hard to get used to. If you are new to programming, there are many features in R that make it relatively easy to do many of the tasks we need for this book, such as loading data from files, manipulating sequence of data values, and doing plots. The environment For each chapter that has code, there is an matching R Notebook.5 In the web edition of this book, you can click on the Download link at the top of the page to download the chapter as a notebook. In the print version of the book, we point you to the web version, to get the links. Running the code on your own computer It is reasonably easy to set up your own computer so you can run the code in this book. First install the R language by going the download section of R website,6 downloading the installer for your computer, and running the installer. Next go to the R Studio website.7 Follow the links to the download section, then download and install the RStudio desktop. Choose the free version unless you have a reason not to. Now open the RStudio application and open the notebook that you downloaded from the website. You should now be able to run the code from the chapter on your own computer, and experiment by making changes. https://www.r-project.org↩︎ https://bookdown.org/yihui/rmarkdown/notebook.html↩︎ https://www.r-project.org↩︎ https://www.rstudio.com↩︎ "],["resampling-method.html", "3 The Resampling method 3.1 The resampling approach in action 3.2 Randomness from your computer 3.3 How resampling differs from the conventional approach 3.4 Resampling can make logic clearer.", " 3 The Resampling method This chapter is a brief introduction to the resampling method of solving problems in probability and statistics. A simple illustrative problem is stated, and then the step-by-step solution with resampling is shown, using both by-hand methods and a computer program. The older conventional formulaic approach to such a problem is then discussed. The conventional analytic method requires that you understand complex formulas, and too often one selects the wrong formula. In contrast, resampling requires that you first understand the physical problem fully. Then you simulate a statistical model of the physical problem with techniques that are intuitively obvious, and you estimate the probability with repeated random sampling. 3.1 The resampling approach in action Recall the problem from section 1.2 in which the contractor owns 12 ambulances. The chance that any one ambulance will be unfit for service on any given day is about 1 in 10, based on past experience. You want to know the probability that on a particular day—tomorrow— three or more ambulances will be out of action. The resampling approach produces the estimate as follows. 3.1.1 Randomness from physical methods We collect 10 coins, and mark one of them with a pen or pencil or tape as being the coin that represents “out-of-order;” the other nine coins stand for “in operation.” This set of 10 coins is a “model” of a situation where there is a one-in-ten chance—a probability of .10 (10 percent)—of one particular ambulance being out-of-order on a given day. Next, we put the coins into a little jar or bucket, draw out one coin, and mark down whether or not that coin is the coin marked “out-of-order.” That drawing of the single coin from the bucket represents the chance that any one given ambulance among our 12 ambulances (perhaps the one with the lowest license-plate number) will be out-of-order tomorrow. Then we put the drawn coin back in the bucket, shake all the coins, and again draw out a coin. We mark down whether that second-drawing coin is or is not the “out-of-order” coin, and that outcome stands for a second ambulance in the fleet. We do this 12 times to represent our 12 ambulances, replacing the coin after each drawing, of course. Those 12 drawings represent one day. At the end of the 12 draws we count how many out-of-orders we have got for that “day,” checking whether there are three or more out-of-orders. If there are three or more, we write down in another column “yes”; if not, we write “no.” The work we have done up to now represents one experimental trial of the model for a single day. Then we repeat perhaps 50 or 100 times the entire experiment described above. Each of those 50 or 100 experimental trials represents a single day. When we have collected evidence for 50 or 100 experimental days, we determine the proportion of the experimental days on which three or more ambulances are out of order. That proportion is an estimate of the probability that three or more ambulances will be out of order on a given day—the answer we seek. This procedure is an example of Monte Carlo simulation, which is the heart of the resampling method of statistical estimation. A more direct way to answer this question would be to examine the firm’s actual records for the past 100 days or 500 days to determine how many days had three or more ambulances out of order. But the resampling procedure described above gives us an estimate even if we do not have such long-term information. This is realistic; it is frequently the case in the workaday world that we must make estimates on the basis of insufficient history about an event. A quicker resampling method than the coins could be obtained with 12 ten-sided dice or spinners. Each one of the dice, marked with one of its ten sides as “out-of-order,” would indicate the chance of a single ambulance being out of order on a given day. A single pass with the 12 dice or spinners allows us to count whether three or more ambulances turn up out of order. So in a single throw of the 12 dice we can get an experimental trial that represents a single day. And in a hundred quick throws of the 12 dice—which probably takes less than 5 minutes—we can get a fast and reasonably-accurate answer to our question. But getting hold of ten-sided dice might be a nuisance. 3.2 Randomness from your computer Your computer gives you many convenient ways of getting random numbers for resampling. You can get random numbers from spreadsheet programs like Excel, Numbers or LibreOffice,8 or from websites like https://www.random.org. We can use these numbers to simulate our problem. For example, we can ask the computer to make us a random number between 0 and 9 (inclusive) to represent one ambulance. If we say that the digit 0 represents “out-of-order” and the digits 1 – 9 represent “in operation,” then any one random digit gives us a trial observation for a single ambulance. To get an experimental trial for a single day we look at 12 digits and count the number of zeros. If the number of zeros is three or more, then we write “yes.” We then look at one hundred or two hundred sets of 12 digits and count the proportion of sets whose 12 digits show three or more ambulances being “out-of-order.” Once again, that proportion estimates the probability that three or more ambulances will be out-of-order on any given day. Soon we will do all these steps with some R code, but for now, let’s say we have used one of the methods above to get 12 random numbers from 0 through 9, and we have done that 25 times, to simulate 25 days of 12 ambulances. We could arrange those numbers in a table like table 3.1: Table 3.1: 25 simulations of 12 ambulances T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 1 6 4 5 3 5 8 4 4 7 1 6 4 2 4 1 5 3 1 2 8 5 6 4 5 8 3 5 9 4 1 0 3 7 5 0 5 1 2 4 0 8 1 4 7 0 2 9 2 1 6 9 5 2 5 3 9 0 6 5 5 2 8 1 7 6 0 0 6 5 8 9 1 5 6 9 5 4 7 6 2 0 4 0 5 0 0 1 4 6 0 8 3 6 6 7 1 7 8 2 8 5 8 4 9 2 1 6 3 3 1 8 1 2 2 0 9 10 6 4 3 0 2 6 0 5 6 9 7 5 11 0 3 1 5 4 9 9 2 3 5 1 8 12 4 2 1 4 9 3 2 7 3 8 0 6 13 3 7 9 7 6 8 8 9 8 2 8 2 14 0 6 4 8 3 5 7 6 2 7 2 7 15 7 1 6 1 9 0 4 9 1 8 8 0 16 6 3 0 8 8 9 3 5 1 9 1 9 17 9 1 4 2 3 1 4 4 5 2 0 7 18 7 5 7 2 1 7 6 9 4 7 9 4 19 2 3 0 8 1 6 2 9 7 2 0 0 20 8 9 9 4 1 9 7 1 3 4 6 3 21 3 3 2 4 5 4 2 3 8 4 5 6 22 6 9 5 4 7 1 9 7 1 5 5 0 23 4 4 5 8 0 0 8 3 1 7 6 6 24 9 6 5 4 9 8 5 7 2 9 9 1 25 9 6 6 3 6 9 1 8 9 5 9 1 To get the answer for each day, we count the number of zeros in each row. The counts go in the final column called “#0” (for “number of zeros”). Table 3.2: 25 simulations of 12 ambulances, with counts T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 #0 1 6 4 5 3 5 8 4 4 7 1 6 4 0 2 4 1 5 3 1 2 8 5 6 4 5 8 0 3 5 9 4 1 0 3 7 5 0 5 1 2 2 4 0 8 1 4 7 0 2 9 2 1 6 9 2 5 2 5 3 9 0 6 5 5 2 8 1 7 1 6 0 0 6 5 8 9 1 5 6 9 5 4 2 7 6 2 0 4 0 5 0 0 1 4 6 0 5 8 3 6 6 7 1 7 8 2 8 5 8 4 0 9 2 1 6 3 3 1 8 1 2 2 0 9 1 10 6 4 3 0 2 6 0 5 6 9 7 5 2 11 0 3 1 5 4 9 9 2 3 5 1 8 1 12 4 2 1 4 9 3 2 7 3 8 0 6 1 13 3 7 9 7 6 8 8 9 8 2 8 2 0 14 0 6 4 8 3 5 7 6 2 7 2 7 1 15 7 1 6 1 9 0 4 9 1 8 8 0 2 16 6 3 0 8 8 9 3 5 1 9 1 9 1 17 9 1 4 2 3 1 4 4 5 2 0 7 1 18 7 5 7 2 1 7 6 9 4 7 9 4 0 19 2 3 0 8 1 6 2 9 7 2 0 0 3 20 8 9 9 4 1 9 7 1 3 4 6 3 0 21 3 3 2 4 5 4 2 3 8 4 5 6 0 22 6 9 5 4 7 1 9 7 1 5 5 0 1 23 4 4 5 8 0 0 8 3 1 7 6 6 2 24 9 6 5 4 9 8 5 7 2 9 9 1 0 25 9 6 6 3 6 9 1 8 9 5 9 1 0 Each value in the last column of 3.2 is the count of zeros in that row, and therefore, the result from our simulation of one day. We can estimate how often three or more ambulances would break down by looking for values of three or greater in the last column. We find there are 2 rows with three or more in the last column. Finally we divide this number of rows by the number of trials (25) to get an estimate of the proportion of days with three or more breakdowns. The result is 0.08. 3.3 How resampling differs from the conventional approach In the standard approach the student learns to choose and solve a formula. Doing the algebra and arithmetic is quick and easy. The difficulty is in choosing the correct formula. Unless you are a professional mathematician, it may take you quite a while to arrive at the correct formula—considerable hard thinking, and perhaps some digging in textbooks. More important than the labor, however, is that you may come up with the wrong formula, and hence obtain the wrong answer. Most students who have had a standard course in probability and statistics are quick to tell you that it is not easy to find the correct formula, even immediately after finishing a course (or several courses) on the subject. After leaving school, it is harder still to choose the right formula. Even many people who have taught statistics at the university level (including this writer) must look at a book to get the correct formula for a problem as simple as the ambulances, and then we are not always sure of the right answer. This is the grave disadvantage of the standard approach. In the past few decades, resampling and other Monte Carlo simulation methods have come to be used extensively in scientific research. But in contrast to the material in this book, simulation has mostly been used in situations so complex that mathematical methods have not yet been developed to handle them. Here are examples of such situations: For a spaceship that will travel to Mars, calculating the correct flight route involves a great many variables, too many to solve with formulas. Hence, the Monte Carlo simulation method is used. The Navy might want to know how long the average ship will have to wait for dock facilities. The time of completion varies from ship to ship, and the number of ships waiting in line for dock work varies over time. This problem can be handled quite easily with the experimental simulation method, but formal mathematical analysis would be difficult or impossible. What are the best tactics in baseball? Should one bunt? Should one put the best hitter up first, or later? By trying out various tactics with dice or random numbers, Earnshaw Cook (in his book Percentage Baseball ), found that it is best never to bunt, and the highest-average hitter should be put up first, in contrast to usual practice. Finding this answer would have been much more difficult with the analytic method. Which search pattern will yield the best results for a ship searching for a school of fish? Trying out “models” of various search patterns with simulation can provide a fast answer. What strategy in the game of Monopoly will be most likely to win? The simulation method systematically plays many games (with a computer) testing various strategies to find the best one. But those five examples are all complex problems. This book and its earlier editions break new ground by using this method for simple rather than complex problems , especially in statistics rather than pure probability, and in teaching beginning rather than advanced students to solve problems this way. (Here it is necessary to emphasize that the resampling method is used to solve the problems themselves rather than as a demonstration device to teach the notions found in the standard conventional approach . Simulation has been used in elementary courses in the past, but only to demonstrate the operation of the analytical mathematical ideas. That is very different than using the resampling approach to solve statistics problems themselves, as is done here.) Once we get rid of the formulas and tables, we can see that statistics is a matter of clear thinking, not fancy mathematics . Then we can get down to the business of learning how to do that clear statistical thinking, and putting it to work for you. The study of probability is purely mathematics (though not necessarily formulas) and technique. But statistics has to do with meaning . For example, what is the meaning of data showing an association just discovered between a type of behavior and a disease? Of differences in the pay of men and women in your firm? Issues of causation, acceptability of control, design of experiments cannot be reduced to technique. This is “philosophy” in the fullest sense. Probability and statistics calculations are just one input. Resampling simulation enables us to get past issues of mathematical technique and focus on the crucial statistical elements of statistical problems. If you intend to go on to advanced statistical work, the older standard method can be learned alongside resampling methods. Your introduction to the conventional method may thereby be made much more meaningful. 3.4 Resampling can make logic clearer. A problem in probability is at the heart of every problem in statistics. Problems in probability often can be very confusing to the intuition. And formulas often either do not aid the intuition, or lead to the wrong answer, or both. But simulation can often get you the right answer and also help you understand why it is correct. Let’s dramatize the point with puzzles: 3.4.1 The other child Problem: If a family has two children and one of them is a boy, what is the probability that the other will also be a boy? Most people—even professional statisticians—often quickly answer “one half.” That is not correct. The solution is not obvious even after the person has decided to tackle the problem by simulation. It is not unusual for a person to say: I flip one coin, and if it is a head (boy), I then flip another coin and see how often that will be a boy, also, and then actually flip the coin once and record the outcomes. But that experiment does not resemble the situation of interest. A proper modeling throws two coins, examines to see if there is a head on either , and then examines the other. Or consider table 3.3, where we asked the computer to do this work for us. The computer has done 50 trials, where one trial is one family of two children. It decided the gender of the two children, by choosing at random from “Boy” or “Girl”, and then classified the pair of children as to whether the other child was a boy. Where both children were girls, it leaves the classification blank. The first 50 lines are enough to suggest the correct probability, and also to make clear the mechanism: Two-girl pairs, a fourth of the cases, are excluded from the sample. And mixed pairs—which give a “No” answer—are two-thirds of the remaining pairs, whereas the only pairs that give “Yes” answers—two boys—are only a third of the remaining pairs. The point of presenting the puzzle is this: Simulation gets it right very quickly and easily, whereas the deductive method of mathematical logic can result in much confusion. Table 3.3: 50 simulations of a family with two children Trial First Second Other is boy? 1 Boy Boy Yes 2 Girl Girl 3 Boy Girl No 4 Girl Boy No 5 Girl Girl 6 Boy Girl No 7 Boy Girl No 8 Girl Girl 9 Girl Girl 10 Girl Boy No 11 Girl Girl 12 Boy Boy Yes 13 Boy Boy Yes 14 Boy Boy Yes 15 Boy Boy Yes 16 Girl Boy No 17 Boy Boy Yes 18 Boy Girl No 19 Boy Boy Yes 20 Girl Boy No 21 Boy Boy Yes 22 Boy Boy Yes 23 Girl Boy No 24 Girl Boy No 25 Girl Boy No Trial First Second Other is boy? 26 Boy Boy Yes 27 Girl Boy No 28 Boy Boy Yes 29 Boy Girl No 30 Boy Boy Yes 31 Boy Girl No 32 Boy Girl No 33 Boy Girl No 34 Boy Boy Yes 35 Boy Boy Yes 36 Boy Girl No 37 Boy Girl No 38 Boy Boy Yes 39 Girl Girl 40 Girl Boy No 41 Girl Girl 42 Girl Girl 43 Boy Boy Yes 44 Boy Girl No 45 Boy Boy Yes 46 Boy Boy Yes 47 Boy Girl No 48 Girl Boy No 49 Boy Boy Yes 50 Boy Boy Yes This puzzle illustrates the power of simulation. And it supports by analogy the general use of the resampling method in probability and statistics because it reveals the limitations of human deductive capacity, even among those who are mathematically adept. Someone might wonder whether formal mathematics can help us with this problem. Formal (even though not formulaic) analysis can certainly provide an answer. We can use what is known as the “sample space” approach which reasons from first principles; here it consists of making a list of the possibilities , and examining the proportion of “successes” to “failures” in that list. First we write down the equally-likely ways that two coins can fall: First coin Second coin 1 Heads Heads 2 Heads Tails 3 Tails Heads 4 Tails Tails Note that it is very easy to make a mistake in writing this list; great mathematicians have made such mistakes in the past even with problems as easy as this one (as we will see with the example of D’Alembert in just a minute). Now we notice that if we have observed at least one head, the list of possibilities for the ways the two coins fell shrinks to: First coin Second coin 1 Heads Heads 2 Heads Tails 3 Tails Heads And now we can see that in only one of three of these possibilities would the “other” coin be a head. So the probability we ask about is 1/3. The crucial question is: Does this formal approach make the problem harder or easier than the simulation approach? That depends on your mental makeup. In practice, it turns out that the formal method seems to lead to a higher rate of error if everyone employs it than does the simulation approach. Hence we emphasize the simulation approach here, to lead to the highest rate of success (and enjoyment). But it is best if everyone finds the mode that is best for him or her. Here’s a puzzle I call D’Alembert’s Misery. Great mathematicians have blundered on even simple problems in probability because they attacked them with only logical tools. One famous example was D’Alembert, living in the 18th Century (1717-1783; story in Schuh (1968), p 165). D’Alembert asked: What is the chance of throwing at least one head in two tosses of a coin? He reasoned that there are three cases: tail on both tosses, tail then head, head on the first toss (no second toss necessary). Of these three cases, two are successes, and therefore the probability sought is 2/3, he concluded. What’s the answer? Toss two coins, and record number of heads. Repeat (1) a hundred times. Count number of outcomes with one or two heads (or more easily, the number with two tails), and divide by 100 to find the probability sought. Compare D’Alembert’s conclusion to the result of your Monte Carlo experiment performed as above. Some other fascinating puzzles in probability are found in Chapter XXX. 3.4.2 The Monty Hall problem The Monty Hall Problem is a probability problem that is famous for its deceptive simplicity. It has its own long Wikipedia page: https://en.wikipedia.org/wiki/Monty_Hall_problem. Here is the problem in its most famous form; a letter to the columnist Marilyn vos Savant, published in Parade Magazine (1990): Suppose you’re on a game show, and you’re given the choice of three doors. Behind one door is a car, behind the others, goats. You pick a door, say #1, and the host, who knows what’s behind the doors, opens another door, say #3, which has a goat. He says to you, “Do you want to pick door #2?” Is it to your advantage to switch your choice of doors? In fact the first person to propose (and solve) this problem was Steve Selvin, a professor of public health at the University of California, Berkeley (Selvin 1975). Most people, including at least one of us, your humble authors, quickly come to the wrong conclusion. The most common but incorrect answer is that it will make no difference if you switch doors or stay with your original choice. The obvious intuition is that, after Monty opens his door, there are two doors that might have the car behind them, and therefore, a 50% chance it will be behind any one of the two. It turns out that answer is wrong; you will double your chances of winning by switching doors. Did you get the answer right? If you got the answer wrong, you are in excellent company. As you can see from the commentary in Savant (1990), many mathematicians wrote to Parade magazine to assert that the (correct) solution was wrong. Paul Erdős was one of the most famous mathematicians of the 20th century; he could not be convinced of the correct solution until he had seen a computer simulation (Vazsonyi 1999), of the type we will do below. To simulate a trial of this problem, we need to select a door at random to house the car, and another door at random, to be the door the contestant chooses. We number the doors 1, 2 and 3. Now we need two random choices from the options 1, 2 or 3, one for the door with the car, the other for the contestant door. To chose a door for the car, we could throw a die, and chose door 1 if the die shows 1 or 4, door 2 if the die shows 2 or 5, and door 3 for 3 or 6. Then we throw the die again to chose the contestant door. But throwing dice is a little boring; we have to find the die, then throw it many times, and record the results. Instead we can ask the computer to chose the doors at random. For this simulation, let us do 25 trials. We ask the computer to create two sets of 25 random numbers from 1 through 3. The first set is the door with the car behind it (“Car door”). The second set have the door that the contestant chose at random (“Our door”). We put these in a table, and make some new, empty columns to fill in later. The first new column is “Monty opens”. In due course, we will use this column to record the door that Monty Hall will open on this trial. The last two columns express the outcome. The first is “Stay wins”. This has “Yes” if we win on this trial by sticking to our original choice of door, and “No” otherwise. The last column is “Switch wins”. This has “Yes” if we win by switching doors, and “No” otherwise. See table 3.4. Table 3.4: 25 simulations of the Monty Hall problem Car door Our door Monty opens Stay wins Switch wins 1 3 3 2 3 1 3 2 3 4 1 1 5 3 3 6 3 2 7 1 3 8 2 3 9 1 2 10 1 3 11 2 3 12 1 1 13 1 2 14 1 1 15 3 3 16 2 2 17 3 2 18 3 1 19 3 3 20 1 2 21 1 3 22 3 1 23 2 3 24 3 3 25 3 1 In the first trial in 3.4, the computer selected door 3 for car, and door 3 for the contestant. Now Monty must open a door, and he cannot open our door (door 3) so he has the choice of opening door 1 or door 2; he chooses randomly, and opens door 2. On this trial, we win if we stay with our original choice, and we lose if we change to the remaining door, door 1. Now we go the second trial. The computer chose door 3 for the car, and door 1 for our choice. Monty cannot choose our door (door 1) or the door with the car behind it (door 3), so he must open door 2. Now if we stay with our original choice, we lose, but if we switch, we win. You may want to print out table 3.4, and fill out the blank columns, to work through the logic. After doing a few more trials, and some reflection, you may see that there are two different situations here: the situation when our initial guess was right, and the situation where our initial guess was wrong. When our initial guess was right, we win by staying with our original choice, but when it was wrong, we always win by switching. The chance of our initial guess being correct is 1/3 (one door out of three). So the chances of winning by staying are 1/3, and the chances of winning by switching are 2/3. But remember, you don’t need to follow this logic to get the right answer. As you will see below, the resampling simulation shows us that the Switch strategy wins. Table 3.5 is a version of table 3.4 for which we have filled in the blank columns using the logic above. Table 3.5: 25 simulations of the Monty Hall problem, filled out Car door Our door Monty opens Stay wins Switch wins 1 3 3 2 Yes No 2 3 1 2 No Yes 3 2 3 1 No Yes 4 1 1 2 Yes No 5 3 3 1 Yes No 6 3 2 1 No Yes 7 1 3 2 No Yes 8 2 3 1 No Yes 9 1 2 3 No Yes 10 1 3 2 No Yes 11 2 3 1 No Yes 12 1 1 2 Yes No 13 1 2 3 No Yes 14 1 1 2 Yes No 15 3 3 2 Yes No 16 2 2 1 Yes No 17 3 2 1 No Yes 18 3 1 2 No Yes 19 3 3 1 Yes No 20 1 2 3 No Yes 21 1 3 2 No Yes 22 3 1 2 No Yes 23 2 3 1 No Yes 24 3 3 2 Yes No 25 3 1 2 No Yes The proportion of times “Stay” wins in these 25 trials is 0.36. The proportion of times “Switch” wins is 0.64; the Switch strategy wins about twice as often as the Stay strategy. Doing these simulations has two large benefits. First, it gives us the right answer, saving us from making a mistake. Second, the process of simulation forces us to think about how the problem works. This can give us better understanding, and make it easier to reason about the solution. We will soon see that these same advantages also apply to reasoning about statistics. References "],["resampling-with-code.html", "4 Resampling with code", " 4 Resampling with code Chapter 3 used simulation and resampling from tables of random numbers, dice and coins. Making random choices in this way can make it easier to understand the process, but of course, physical methods of making random outcomes can be slow and boring. On the other hand, short computer programs can do a huge number of resampling trials in a few seconds. The flexibility of a programming language makes it possible to simulate many different outcomes and tests. In this chapter, we begin using the R language to build up tables of random numbers, and do simple tasks like counting the number of values in a row, and taking proportions. With these simple tools, we can simulate many problems in probability and statistics. Here is our first R example program. Do not expect to follow all of it straightaway. For now, read the code below to get an idea of how it implements the procedure above. We will be coming back to the specifics later. The key to reading code is to think about what the computer will do, when it sees the code. Start of resampling_with_code notebook. Download notebook A simple place to start is the comment. A comment is a statement that the computer will ignore. It is text that we put in the program for our benefit, to explain what is going on to a human reader. This is an example of a comment: # This is a comment. It doesn&#39;t have any effect. The comment starts with a hash character #. This character tells the computer that the rest of the line is a comment, and therefore, that it can ignore everything on that line that follows the #. The core of the program to solve the ambulances problem above begins with this command to the computer: a = sample(1:10, 12, replace=TRUE) This uses the sample function to generate random integers (counting numbers) from 1 through 10. The 12 in the command tells R to generate 12 of these numbers. replace=TRUE tells R to sample with replacement. For example, the chances of getting a particular result - such as a 3 - for the first random number - are 1 in 10, or p= 1/10 = 0.1. If we resample with replacement then the chances of getting 3 in the second number are unchanged, at p=0.1. It is as if we put 10 balls into a bucket, numbered one through ten, and then selected 12 balls from the bucket; but after we have selected a ball, we record the number and replace it in the bucket, and shake up the bucket again. So — the command above orders the computer to randomly generate 12 numbers between “1” and “10.” Inasmuch as each ambulance has a 1 in 10 chance of being defective, we decide arbitrarily that a “1” stands for a defective ambulance, and the other nine numbers (from “2” to “10”) stand for a not-defective ambulance. The command orders the computer to store the results of the random drawing in a location in the computer’s memory to which we give a name such as a or ambulances. When we run a statement like the one above, a is a variable - the name a refers to the value, which is the sequence of random numbers the computer created using sample. We can show the value of the variable a in the notebook or interactive terminal by using the print function: print(a) #&gt; [1] 7 5 6 4 6 9 5 5 8 2 7 5 This shows the 12 random values that we got from sample. The next key element in the core of the program is: b = sum(a == 1) This command orders the computer to count the number of “1’s” among the 12 numbers that are in location a following the random drawing carried out by the sample operation. The result of the count will be somewhere between 0 and 12, the number of ambulances that might be out-of-order on a given day. The result is then placed in another location in the computer’s memory that we label b. # Show the value of b print(b) #&gt; [1] 0 Now let us place the commands to generate the random numbers, and count how many 1s we get, within the entire program that we use to solve this problem, which is: # Make an array that has 400 elements. # We will use this to store the counts for our 400 repetitions results &lt;- numeric(400) # Repeat the simulation 400 times for (i in 1:400) { # The commands between the { above and the } below are the procedure for # one trial. # The computer runs these commands from first to last, for each trial. # Generate 12 numbers, each between &quot;1&quot; and &quot;10,&quot; and put them in vector a. # Each number will represent an ambulance, and we let 1 represent # a defective ambulance. a &lt;- sample(1:10, 12, replace=TRUE) # Count the number of defective ambulances, and put the result in b. b &lt;- sum(a == 1) # Keep track of each trial&#39;s result in &quot;results&quot;. results[i] &lt;- b # End this trial, then go back and repeat the process until all 400 trials # are complete. } # Now we have finished the 400 trials. # Determine how many trials resulted in more than 3 ambulances out of order. bad_day_count &lt;- sum(results &gt; 3) # Convert to a proportion. bad_day_prop &lt;- bad_day_count / 400 # Print the result. print(bad_day_prop) #&gt; [1] 0.0275 End of resampling_with_code notebook. The results[i] &lt;- b statement that follows the b = sum(a == 1) operation simply keeps track of the results of each trial, placing the number of defective ambulances that occur in each trial in a location that we usually call “results”. This is done in each of the 400 trials that we make, and the result eventually is a “vector” with 400 numbers in it. A vector is just a sequence of numbers. In order to make 400 repetitions of our experiment—we could have decided to make a thousand or some other number of repetitions—we put for (i in 1:400) { before the statements that generate the random numbers, count how many of these numbers are 1, representing a defective ambulance, and store this result of a single trial. Then we complete each repetition “loop” by adding a closing } bracket. Since our aim is to count the number of days in which more than 3 (4 or more) defective ambulances occur, we use the bad_day_count = sum(results &gt; 3) command to count how many times in the 400 days recorded in our results vector at the end of the 400 trials more than 3 defects occurred, and we place the result in still another location “bad_day_count”. This gives us the total number of days where 4 or more defective ambulances are seen to occur. Then we divide the number in “bad_day_count” by 400, the number of trials. Thus we obtain an estimate of the chance, expressed as a probability between 0 and 1, that 4 or more ambulances will be defective on a given day. And we store that result in a location that we decide to call “bad_day_prop” so that it will be there when the computer receives the next command to print that result on the screen. Can you see how each of the operations that the computer carries out are analogous to the operations that you yourself executed when you solved this problem using a labeled coins or a random-number table? This is exactly the procedure that we will use to solve every problem in probability and statistics that we must deal with. Either we will use a device such as coins or a random number table as an analogy for the physical process we are interested in (ambulances becoming defective, in this case), or we will simulate the analogy on the computer using the R program above. Simple as it is, the program above may not seem simple to you at first glance. But we think you will find, over the course of this book, that these programs become much simpler to understand than the older conventional approach to such problems that has routinely been taught to students for decades. "],["basic-concepts-in-probability-and-statistics-part-1.html", "5 Basic Concepts in Probability and Statistics, Part 1 5.1 Introduction 5.2 The nature and meaning of the concept of probability 5.3 The “Meaning” of “Probability” 5.4 Digression about Operational Definitions 5.5 Back to Proxies 5.6 The various ways of estimating probabilities", " 5 Basic Concepts in Probability and Statistics, Part 1 “Uncertainty, in the presence of vivid hopes and fears, is painful, but must be endured if we wish to live without the support of comforting fairy tales.” Bertrand Russell, A History of Western Philosophy (New York: Simon and Schuster, 1945, p. xiv) 5.1 Introduction The central concept for dealing with uncertainty is probability. Hence we must inquire into the “meaning” of the term probability. (The term “meaning” is in quotes because it can be a confusing word.) You have been using the notion of probability all your life when drawing conclusions about what you expect to happen, and in reaching decisions in your public and personal lives. You wonder: Will the footballer’s kick from the 45 yard line go through the uprights? How much oil can you expect from the next well you drill, and what value should you assign to that prospect? Will you be the first one to discover a completely effective system for converting speech into computer-typed output? Will the next space shuttle end in disaster? Your answers to these questions rest on the probabilities you estimate. And you act on the basis of probabilities: You place your blanket on the beach where there is a low probability of someone’s kicking sand on you. You bet heavily on a poker hand if there is a high probability that you have the best hand. A hospital decides not to buy another ambulance when the administrator judges that there is a low probability that all the other ambulances will ever be in use at once. NASA decides whether or not to send off the space shuttle this morning as scheduled. This chapter discusses what is meant by such key terms as “probability,” “conditional” and “unconditional” probability, “independence,” “sample,” and “universe.” It discusses the nature and the usefulness of the concept of probability as used in this book, and it touches on the source of basic estimates of probability that are the raw material of statistical inferences. The chapter also distinguishes between probability theory and inferential statistics. (Descriptive statistics, the other main branch of statistics, was discussed briefly in the previous chapter.) 5.2 The nature and meaning of the concept of probability The common meaning of the term “probability” is as follows: Any particular stated probability is an assertion that indicates how likely you believe it is that an event will occur . It is confusing and unnecessary to inquire what probability “really” is. (Indeed, the terms “really” and “is,” alone or in combination, are major sources of confusion in statistics and in other logical and scientific discussions, and it is often wise to avoid their use.) Various concepts of probability—which correspond to various common definitions of the term—are useful in particular contexts (see Ayer, 1965; Schlaifer, 1961, Chapter 1; for perhaps the best summary of the probability concept, see Barnett, 1973, Chapter 3). This book contains many examples of the use of probability. Work with them will gradually develop a sound understanding of the concept. There are two major concepts and points of view about probability—frequency and belief. Each is useful in some situations but not in others. Though they may seem incompatible in principle, there almost never is confusion about which is appropriate in a given situation. Frequency . The probability of an event can be said to be the proportion of times that the event has taken place in the past, usually based on a long series of trials. Insurance companies use this when they estimate the probability that a thirty-five-year-old postman will die during a period for which he wants to buy an insurance policy. (Notice this shortcoming: Sometimes you must bet upon events that have never or only infrequently taken place before, and so you cannot reasonably reckon the proportion of times they occurred one way or the other in the past.) Belief . The probability that an event will take place or that a statement is true can be said to correspond to the odds at which you would bet that the event will take place. (Notice a shortcoming of this concept: You might be willing to accept a five-dollar bet at 2-1 odds that your team will win the game, but you might be unwilling to bet a hundred dollars at the same odds.) The connection between gambling and immorality or vice troubles some people about gambling examples. On the other hand, the racy aspects can give the subject a special tang. There are several reasons why statistics use so many gambling examples—and especially tossing coins, throwing dice, and playing cards: Historical . The theory of probability began with gambling examples of dice analyzed by Cardano, Galileo, and then by Pascal and Fermat. Generality . These examples are not related to any particular walk of life, and therefore they can be generalized to applications in any walk of life. Students in any field—business, medicine, science—can feel equally at home with gambling examples. Sharpness . These examples are particularly stark, and unencumbered by the baggage of particular walks of life or special uses. Universality . Every other text uses these same examples, and therefore the use of them connects up this book with the main body of writing about probability and statistics. Often we’ll begin with a gambling example and then consider an example in one of the professional fields—such as business and other decision-making activities, biostatistics and medicine, social science and natural science—and everyday living. People in one field often can benefit from examples in others; for example, medical students should understand the need for business decision-making in terms of medical practice, as well as the biostatistical examples. And social scientists should understand the decision-making aspects of statistics if they have any interest in the use of their work in public policy. 5.3 The “Meaning” of “Probability” A probability estimate of .2 indicates that you think there is twice as great a chance of the event happening as if you had estimated a probability of .1. This is the rock-bottom interpretation of the term “probability,” and the heart of the concept. The idea of probability arises when you are not sure about what will happen in an uncertain situation—that is, when you lack information and therefore can only make an estimate. For example, if someone asks you your name, you do not use the concept of probability to answer; you know the answer to a very high degree of surety. To be sure, there is some chance that you do not know your own name, but for all practical purposes you can be quite sure of the answer. If someone asks you who will win tomorrow’s ball game, however, there is a considerable chance that you will be wrong no matter what you say. Whenever there is a reasonable chance that your prediction will be wrong, the concept of probability can help you. The concept of probability helps you to answer the question, “How likely is it that…?” The purpose of the study of probability and statistics is to help you make sound appraisals of statements about the future, and good decisions based upon those appraisals. The concept of probability is especially useful when you have a sample from a larger set of data—a “universe”—and you want to know the probability of various degrees of likeness between the sample and the universe. (The universe of events you are sampling from is also called the “population,” a concept to be discussed below.) Perhaps the universe of your study is all high school seniors in 1997. You might then want to know, for example, the probability that the universe’s average SAT score will not differ from your sample’s average SAT by more than some arbitrary number of SAT points—say, ten points. I’ve said that a probability statement is about the future. Well, usually. Occasionally you might state a probability about your future knowledge of past events—that is, “I think I’ll find out that...”—or even about the unknown past. (Historians use probabilities to measure their uncertainty about whether events occurred in the past, and the courts do, too, though the courts hesitate to say so explicitly.) Sometimes one knows a probability, such as in the case of a gambler playing black on an honest roulette wheel, or an insurance company issuing a policy on an event with which it has had a lot of experience, such as a life insurance policy. But often one does not know the probability of a future event. Therefore, our concept of probability must include situations where extensive data are not available. All of the many techniques used to estimate probabilities should be thought of as proxies for the actual probability. For example, if Mission Control at Space Central simulates what should and probably will happen in space if a valve is turned aboard a space craft just now being built, the test result on the ground is a proxy for the real probability of what will happen in space. In some cases, it is difficult to conceive of any data that can serve as a proxy. For example, the director of the CIA, Robert Gates, said in 1993 “that in May 1989, the CIA reported that the problems in the Soviet Union were so serious and the situation so volatile that Gorbachev had only a 50-50 chance of surviving the next three to four years unless he retreated from his reform policies” ( The Washington Post , January 17, 1993, p. A42). Can such a statement be based on solid enough data to be more than a crude guess? The conceptual probability in any specific situation is an interpretation of all the evidence that is then available . For example, a wise biomedical worker’s estimate of the chance that a given therapy will have a positive effect on a sick patient should be an interpretation of the results of not just one study in isolation, but of the results of that study plus everything else that is known about the disease and the therapy. A wise policymaker in business, government, or the military will base a probability estimate on a wide variety of information and knowledge. The same is even true of an insurance underwriter who bases a life-insurance or shipping-insurance rate not only on extensive tables of long-time experience but also on recent knowledge of other kinds. The choice of a method of estimating a probability constitutes an operational definition of probability. 5.4 Digression about Operational Definitions An operation definition is the all-important intellectual procedure that Einstein employed in his study of special relativity to sidestep the conceptual pitfalls into which discussions of such concepts as probability also often slip. An operational definition is to be distinguished from a property or attribute definition, in which something is defined by saying what it consists of. For example, a crude attribute definition of a college might be “an organization containing faculty and students, teaching a variety of subjects beyond the high-school level.” An operational definition of university might be “an organization found in The World Almanac’s listing of ‘Colleges and Universities.’” (Simon, 1969, p. 18.) P. W. Bridgman, the inventor of operational definitions, stated that “the proper definition of a concept is not in terms of its properties but in terms of actual operations.” It was he who explained that definitions in terms of properties had held physics back and constituted the barrier that it took Albert Einstein to crack (Bridgman, 1927, pp. 6-7). A formal operational definition of “operational definition” may be appropriate. “A definition is an operational definition to the extent that the definer (a) specifies the procedure (including materials used) for identifying or generating the definiendum and (b) finds high reliability for \\[consistency in application of\\] his definition” (Dodd, in Dictionary of Social Science , p. 476). A. J. Bachrach adds that “the operational definition of a dish ... is its recipe” (Bachrach, 1962, p. 74). The language of empirical scientific research is made up of instructions that are descriptions of sets of actions or operations (for instance, “turn right at the first street sign”) that someone can follow accurately. Such instructions are called an “operational definition.” An operational definition contains a specification of all operations necessary to achieve the same result. The language of science also contains theoretical terms (better called “hypothetical terms”) that are not defined operationally. 5.5 Back to Proxies Example of a proxy: The “probability risk assessments” (PRAs) that are made for the chances of failures of nuclear power plants are based, not on long experience or even on laboratory experiment, but rather on theorizing of various kinds— using pieces of prior experience wherever possible, of course. A PRA can cost a nuclear facility $5 million. Another example: If a manager looks at the sales of radios in the last two Decembers, and on that basis guesses how likely it is that he will run out of stock if he orders 200 radios, then the last two years’ experience is serving as a proxy for future experience. If a sales manager just “intuits” that the odds are 3 to 1 (a probability of .75) that the main competitor will not meet a price cut, then all his past experience summed into his intuition is a proxy for the probability that it will really happen. Whether any proxy is a good or bad one depends on the wisdom of the person choosing the proxy and making the probability estimates. How does one estimate a probability in practice? This involves practical skills not very different from the practical skills required to estimate with accuracy the length of a golf shot, the number of carpenters you will need to build a house, or the time it will take you to walk to a friend’s house; we will consider elsewhere some ways to improve your practical skills in estimating probabilities. For now, let us simply categorize and consider in the next section various ways of estimating an ordinary garden variety of probability, which is called an “unconditional” probability. 5.6 The various ways of estimating probabilities Consider the probability of drawing an even-numbered spade from a deck of poker cards (consider the queen as even and the jack and king as odd). Here are several general methods of estimation, the specifics of which constitute an operational definition of probability in this particular case: Experience. The first possible source for an estimate of the probability of drawing an even-numbered spade is the purely empirical method of experience . If you have watched card games casually from time to time, you might simply guess at the proportion of times you have seen even-numbered spades appear— say, “about 1 in 15” or “about 1 in 9” (which is almost correct) or something like that. (If you watch long enough you might come to estimate something like 6 in 52.) General information and experience are also the source for estimating the probability that the sales of radios this December will be between 200 and 250, based on sales the last two Decembers; that your team will win the football game tomorrow; that war will break out next year; or that a United States astronaut will reach Mars before a Russian astronaut. You sim- ply put together all your relevant prior experience and knowledge, and then make an educated guess. Observation of repeated events can help you estimate the probability that a machine will turn out a defective part or that a child can memorize four nonsense syllables correctly in one attempt. You watch repeated trials of similar events and record the results. Data on the mortality rates for people of various ages in a particular country in a given decade are the basis for estimating the probabilities of death, which are then used by the actuaries of an insurance company to set life insurance rates. This is systematized experience —called a frequency series . No frequency series can speak for itself in a perfectly objective manner. Many judgments inevitably enter into compiling every frequency series—deciding which frequency series to use for an estimate, choosing which part of the frequency series to use, and so on. For example, should the insurance company use only its records from last year, which will be too few to provide as much data as is preferable, or should it also use death records from years further back, when conditions were slightly different, together with data from other sources? (Of course, no two deaths—indeed, no events of any kind—are exactly the same. But under many circumstances they are practically the same, and science is only interested in such “practical” considerations.) In view of the necessarily judgmental aspects of probability estimates, the reader may prefer to talk about “degrees of belief” instead of probabilities. That’s fine, just as long as it is understood that we operate with degrees of belief in exactly the same way as we operate with probabilities; the two terms are working synonyms. There is no logical difference between the sort of probability that the life insurance company estimates on the basis of its “frequency series” of past death rates, and the manager’s estimates of the sales of radios in December, based on sales in that month in the past two years. The concept of a probability based on a frequency series can be rendered meaningless when all the observations are repetitions of a single magnitude—for example, the case of all successes and zero failures of space-shuttle launches prior to the Challenger shuttle tragedy in the 1980s; in those data alone there was no basis to estimate the probability of a shuttle failure. (Probabilists have made some rather peculiar attempts over the centuries to estimate probabilities from the length of a zero-defect time series—such as the fact that the sun has never failed to rise (foggy days aside!)—based on the undeniable fact that the longer such a series is, the smaller the probability of a failure; see e.g., Whitworth, 1897/1965, pp. xix-xli. However, one surely has more information on which to act when one has a long series of observations of the same magnitude rather than a short series). Simulated experience. A second possible source of probability estimates is empirical scientific investigation with repeated trials of the phenomenon. This is an empirical method even when the empirical trials are simulations. In the case of the even-numbered spades, the empirical scientific procedure is to shuffle the cards, deal one card, record whether or not the card is an even-number spade, replace the card, and repeat the steps a good many times. The proportions of times you observe an even-numbered spade come up is a probability estimate based on a frequency series. You might reasonably ask why we do not just count the number of even-numbered spades in the deck of fifty-two cards. No reason at all. But that procedure would not work if you wanted to estimate the probability of a baseball batter getting a hit or a cigarette lighter producing flame. Some varieties of poker are so complex that experiment is the only feasible way to estimate the probabilities a player needs to know. The resampling approach to statistics produces estimates of most probabilities with this sort of experimental “Monte Carlo” method. More about this later. Sample space analysis and first principles. A third source of probability estimates is counting the possibilities —the quintessential theoretical method. For example, by examination of an ordinary die one can determine that there are six different numbers that can come up. One can then determine that the probability of getting (say) either a “1” or a “2,” on a single throw, is 2/6 = 1/3, because two among the six possibilities are “1” or “2.” One can similarly determine that there are two possibilities of getting a “1” plus a “6” out of thirty-six possibilities when rolling two dice, yielding a probability estimate of 2/36 = 1/18. Estimating probabilities by counting the possibilities has two requirements: 1) that the possibilities all be known (and there- fore limited), and few enough to be studied easily; and 2) that the probability of each particular possibility be known, for example, that the probabilities of all sides of the dice coming up are equal, that is, equal to 1/6. Mathematical shortcuts to sample-space analysis. A fourth source of probability estimates is mathematical calculations . If one knows by other means that the probability of a spade is 1/4 and the probability of an even-numbered card is 6/13, one can then calculate that the probability of turning up an even-numbered spade is 6/52 (that is, 1/4 x 6/13). If one knows that the probability of a spade is 1/4 and the probability of a heart is 1/4, one can then calculate that the probability of getting a heart or a spade is 1/2 (that is 1/4 + 1/4). The point here is not the particular calculation procedures, but rather that one can often calculate the desired probability on the basis of already-known probabilities. It is possible to estimate probabilities with mathematical calculation only if one knows by other means the probabilities of some related events. For example, there is no possible way of mathematically calculating that a child will memorize four nonsense syllables correctly in one attempt; empirical knowledge is necessary. Kitchen-sink methods. In addition to the above four categories of estimation procedures, the statistical imagination may produce estimates in still other ways such as a) the salesman’s seat-of-the-pants estimate of what the competition’s price will be next quarter, based on who-knows-what gossip, long-time acquaintance with the competitors, and so on, and b) the probability risk assessments (PRAs) that are made for the chances of failures of nuclear power plants based, not on long experience or even on laboratory experiment, but rather on theorizing of various kinds— using pieces of prior experience wherever possible, of course. Any of these methods may be a combination of theoretical and empirical methods. Consider the estimation of the probability of failure for the tragic flight of the Challenger shuttle, as described by the famous physicist Nobelist Richard Feynman. This is a very real case that includes just about every sort of complication that enters into estimating probabilities. …Mr. Ullian told us that 5 out of 127 rockets that he had looked at had failed—a rate of about 4 percent. He took that 4 percent and divided it by 4, because he assumed a manned flight would be safer than an unmanned one. He came out with about a 1 percent chance of failure, and that was enough to warrant the destruct charges. But NASA \\[the space agency in charge\\] told Mr. Ullian that the probability of failure was more like 1 of 10 5 . I tried to make sense out of that number. “Did you say 1 in 10 5 ?” “That’s right; 1 in 100,000.” “That means you could fly the shuttle every day for an average of 300 years between accidents—every day, one flight, for 300 years—which is obviously crazy!” “Yes, I know,” said Mr. Ullian. “I moved my number up to 1 in 1000 to answer all of NASA’s claims—that they were much more careful with manned flights, that the typical rocket isn’t a valid comparison, etcetera.” But then a new problem came up: the Jupiter probe, Galileo , was going to use a power supply that runs on heat generated by radioactivity. If the shuttle carrying Galileo failed, radioactivity could be spread over a large area. So the argument continued: NASA kept saying 1 in 100,000 and Mr. Ullian kept saying 1 in 1000, at best. Mr. Ullian also told us about the problems he had in trying to talk to the man in charge, Mr. Kingsbury: he could get appointments with underlings, but he never could get through to Kingsbury and find out how NASA got its figure of 1 in 100,000 (Feynman, 1989, pp. 179–180). Feynman tried to ascertain more about the origins of the figure of 1 in 100,000 that entered into NASA’s calculations. He performed an experiment with the engineers: …“Here’s a piece of paper each. Please write on your paper the answer to this question: what do you think is the probability that a flight would be uncompleted due to a failure in this engine?” They write down their answers and hand in their papers. One guy wrote “99-44/100% pure” (copying the Ivory soap slogan), meaning about 1 in 200. Another guy wrote something very technical and highly quantitative in the standard statistical way, carefully defining everything, that I had to translate—which also meant about 1 in \\200. The third guy wrote, simply, “1 in 300.” Mr. Lovingood’s paper, however, said, Cannot quantify. Reliability is judged from: past experience quality control in manufacturing engineering judgment “Well,” I said, “I’ve got four answers, and one of them weaseled.” I turned to Mr. Lovingood: “I think you weaseled.” “I don’t think I weaseled.” “You didn’t tell me what your confidence was, sir; you told me how you determined it. What I want to know is: after you determined it, what was it?” He says, “100 percent”—the engineers’ jaws drop, my jaw drops; I look at him, everybody looks at him—“uh, uh, minus epsilon!” So I say, “Well, yes; that’s fine. Now, the only problem is, WHAT IS EPSILON?” He says, “10 -5 .” It was the same number that Mr. Ullian had told us about: 1 in 100,000. I showed Mr. Lovingood the other answers and said, “You’ll be interested to know that there is a difference between engineers and management here—a factor of more than 300.” He says, “Sir, I’ll be glad to send you the document that contains this estimate, so you can understand it.” * Later, Mr. Lovingood sent me that report. It said things like “The probability of mission success is necessarily very close to 1.0”—does that mean it is close to 1.0, or it ought to be close to 1.0?—and “Historically, this high degree of mission success has given rise to a difference in philosophy between unmanned and manned space flight programs; i.e., numerical probability versus engineering judgment.” As far as I can tell, “engineering judgment” means they’re just going to make up numbers! The probability of an engine-blade failure was given as a universal constant, as if all the blades were exactly the same, under the same conditions. The whole paper was quantifying everything. Just about every nut and bolt was in there: “The chance that a HPHTP pipe will burst is 10 -7 .” You can’t estimate things like that; a probability of 1 in 10,000,000 is almost impossible to estimate. It was clear that the numbers for each part of the engine were chosen so that when you add everything together you get 1 in 100,000. (Feynman, 1989, pp. 182-183). We see in the Challenger shuttle case very mixed kinds of inputs to actual estimates of probabilities. They include frequency series of past flights of other rockets, judgments about the relevance of experience with that different sort of rocket, adjustments for special temperature conditions (cold), and much much more. There also were complex computational processes in arriving at the probabilities that were made the basis for the launch decision. And most impressive of all, of course, are the extraordinary differences in estimates made by various persons (or perhaps we should talk of various statuses and roles) which make a mockery of the notion of objective estimation in this case. Working with different sorts of estimation methods in different sorts of situations is not new; practical statisticians do so all the time. The novelty here lies in making no apologies for doing so, and for raising the practice to the philosophical level of a theoretically-justified procedure—the theory being that of the operational definition. The concept of probability varies from one field of endeavor to another; it is different in the law, in science, and in business. The concept is most straightforward in decision-making situations such as business and gambling; there it is crystal-clear that one’s interest is entirely in making accurate predictions so as to advance the interests of oneself and one’s group. The concept is most difficult in social science, where there is considerable doubt about the aims and values of an investigation. In sum, one should not think of what a probability “is” but rather how best to estimate it. In practice, neither in actual decision-making situations nor in scientific work—nor in classes—do people experience difficulties estimating probabilities because of philosophical confusions. Only philosophers and mathematicians worry—and even they really do not need to worry—about the “meaning” of probability. This topic is continued in the following chapter. "],["basic-concepts-in-probability-and-statistics-part-2.html", "6 Basic Concepts in Probability and Statistics, Part 2 6.1 The relationship of probability to other magnitudes 6.2 The concept of chance 6.3 What Do We Mean by “Chance”? 6.4 The philosophers’ dispute about the concept of probability 6.5 The relationship of probability to the concept of resampling 6.6 Conclusion 6.7 Endnote", " 6 Basic Concepts in Probability and Statistics, Part 2 This chapter may be skipped by those anxious to reach the actual machinery of estimating probabilities. 6.1 The relationship of probability to other magnitudes An important argument in favor of approaching the concept of probability with the concept of the operational definition is that an estimate of a probability often (though not always) is the opposite side of the coin from an estimate of a physical quantity such as time or space. For example, uncertainty about the probability that one will finish a task within 9 minutes is another way of labeling the uncertainty that the time required to finish the task will be less than 9 minutes. Hence, if an operational definition is appropriate for time in this case, it should be equally appropriate for probability. The same is true for the probability that the quantity of radios sold will be between 200 and 250 units. Hence the concept of probability, and its estimation in any particular case, should be no more puzzling than is the “dual” concept of time or distance or quantities of radios. That is, lack of certainty about the probability that an event will occur is not different in nature from lack of certainty about the amount of time or distance in the event. There is no essential difference between whether a part 2 inches in length will be the next to emerge from the machine, or what the length of the next part will be, or the length of the part that just emerged (if it has not yet been measured). The information available for the measurement of (say) the length of a car or the location of a star is exactly the same in- formation that is available with respect to the concept of probability in those situations. That is, one may have ten disparate observations of an auto’s length which then constitute a probability distribution, and the same for the altitude of a star in the heavens. All the more reason to see the parallel between Einstein’s concept of time and length as being what you measure on a clock and on a meter stick, respectively—or better, that time and length are equivalent to the measurements that one makes on a clock or meter stick—and the notion that probability should be defined by the measurements made on a clock or a meter stick. Seen this way, all the discussions of logical and empirical notions of probability may be seen as being made obsolete by the Einsteinian invention of the operational definition, just as discussions of absolute space and time were made obsolete by it. Or: Consider having four different measurements of the length of a model auto. Which number should we call the length? It is standard practice to compute the mean. But the mean could be seen as a weighted average of each observation by its probability. That is: (.25 * 20 inches + .25 * 22 inches...) = mean model length, instead of (20 + 22 + ...) / 4 = mean model length This again makes clear that the decimal weights we call “probabilities” have no extraordinary properties when discussing frequency series; they are just weights we put on some other values. It should be noted that the view outlined above has absolutely no negative implications for the formal mathematical theory of probability. In a book of puzzles about probability (Mosteller, 1965/1987, #42), this problem appears: “If a stick is broken in two at random, what is the average length of the smaller piece?” This particular puzzle does not even mention probability explicitly, and no one would feel the need to write a scholarly treatise on the meaning of the word “length” here, any more than one would one do so if the question were about an astronomer’s average observation of the angle of a star at a given time or place, or the average height of boards cut by a carpenter, or the average size of a basketball team. Nor would one write a treatise about the “meaning” of “time” if a similar puzzle involved the average time between two bird calls. Yet a rephrasing of the problem reveals its tie to the concept of probability, to wit: What is the probability that the smaller piece will be (say) more than half the length of the larger piece? Or, what is the probability distribution of the sizes of the shorter piece? The duality of the concepts of probability and physical entities also emerges in Whitworth’s discussion (1897) of fair betting odds: …What sum ought you fairly give or take now, while the event is undetermined, in exchange for the assurance that you shall receive a stated sum (say $1,000) if the favourable event occur? The chance of receiving $1,000 is worth something. It is not as good as the certainty of receiving $1,000, and therefore it is worth less than $1,000. But the prospect or expectation or chance, however slight, is a commodity which may be bought and sold. It must have its price somewhere between zero and $1,000. (p. xix.) …And the ratio of the expectation to the full sum to be received is what is called the chance of the favourable event. For instance, if we say that the chance is 1/5, it is equivalent to saying that $200 is the fair price of the contingent $1,000. (p. xx.)… The fair price can sometimes be calculated mathematically from a priori considerations: sometimes it can be deduced from statistics, that is, from the recorded results of observation and experiment. Sometimes it can only be estimated generally, the estimate being founded on a limited knowledge or experience. If your expectation depends on the drawing of a ticket in a raffle, the fair price can be calculated from abstract considerations: if it depend upon your outliving another person, the fair price can be inferred from recorded statistics: if it depend upon a benefactor not revoking his will, the fair price depends upon the character of your benefactor, his habit of changing his mind, and other circumstances upon the knowledge of which you base your estimate. But if in any of these cases you determine that $300 is the sum which you ought fairly to accept for your prospect, this is equivalent to saying that your chance, whether calculated or estimated, is 3/10... (p. xx.) It is indubitable that along with frequency data, a wide variety of other information will affect the odds at which a reasonable person will bet. If the two concepts of probability stand on a similar footing here, why should they not be on a similar footing in all discussion of probability? Why should both kinds of information not be employed in an operational definition of probability? I can think of no reason that they should not be so treated. Scholars write about the “discovery” of the concept of probability in one century or another. But is it not likely that even in pre-history, when a fisherperson was asked how long the big fish was, s/he sometimes extended her/his arms and said, “About this long, but I’m not exactly sure,” and when a scout was asked how many of the enemy there were, s/he answered, “I don’t know for sure...probably about fifty.” The uncertainty implicit in these statements is the functional equivalent of probability statements. There simply is no need to make such heavy work of the probability concept as the philosophers and mathematicians and historians have done. 6.2 The concept of chance The study of probability focuses on randomly generated events—that is, events about which there is uncertainty whether or not they will occur. And the uncertainty refers to your knowledge rather than to the event itself. For example, consider this lecture illustration with a salad spoon. I spin the salad spoon like a baton twirler. If I hold it at the handle and attempt to flip it so that it turns only half a revolution, I can be almost sure that I will correctly get the spoon end and not the handle. And if I attempt to flip it a full revolution, again I can almost surely get the handle successfully. It is not a random event whether I catch the handle or the head (here ignoring those throws when I catch neither end) when doing only half a revolution or one revolution. The result is quite predictable in both these simple maneuvers so far. When I say the result is “predictable,” I mean that you would not bet with me about whether this time I’ll get the spoon or the handle end. So we say that the outcome of my flip aiming at half a revolution is not “random.” When I twirl the spoon so little, I control (almost completely) whether it comes down the handle or the spoon end; this is the same as saying that the outcome does not occur by chance. The terms “random” and “chance” implicitly mean that you believe that I cannot control or cannot know in advance what will happen. Whether this twirl will be the rare time I miss, however, should be considered chance. Though you would not bet at even odds on my catching the handle versus the spoon end if there is to be only a half or one full revolution, you might bet—at (say) odds of 50 to 1—whether I’ll make a mistake and get it wrong, or drop it. So the very same flip can be seen as random or determined depending on what aspect of it we are looking at. Of course you would not bet against me about my not making a mistake, because the bet might cause me to make a mistake purposely. This “moral hazard” is a problem that emerges when a person buys life insurance and may commit suicide, or when a boxer may lose a fight purposely. The people who stake money on those events say that such an outcome is “fixed” (a very appropriate word) and not random. Now I attempt more difficult maneuvers with the ladle. I can do 1-1\\2 flips pretty well, and two full revolutions with some success—maybe even 2-1/2 flips on a good day. But when I get much beyond that, I cannot determine very well whether I’ll get handle or spoon. The outcome gradually becomes less and less predictable—that is, more and more random. If I flip the spoon so that it revolves three or more times, I can hardly control the process at all, and hence I cannot predict well whether I’ll get the handle or the head. With 5 revolutions I have absolutely no control over the outcome; I cannot predict the outcome better than 50-50. At that point, getting the handle or the spoon end has become a very random event for our purposes, just like flipping a coin high in the air. So at that point we say that “chance” controls the outcome, though that word is just a synonym for my lack of ability to control and predict the outcome. “Chance” can be thought to stand for the myriad small factors that influence the outcome. We see the same gradual increase in randomness with increasing numbers of shuffles of cards. After one shuffle, a skilled magician can know where every card is, and after two shuffles there is still much order that s/he can work with. But after (say) five shuffles, the magician no longer has any power to predict and control, and the outcome of any draw can then be thought of as random chance. At what point do we say that the outcome is “random” or “pure chance” as to whether my hand will grasp the spoon end, the handle, or at some other spot? There is no sharp boundary to this transition. Rather, the transition is gradual; this is the crucial idea, and one that I have not seen stated before. Whether or not we refer to the outcome as random depends upon the twirler’s skill, which influences how predictable the event is. A baton twirler or juggler might be able to do ten flips with a non-random outcome; if the twirler is an expert and the outcome is highly predictable, we say it is not random but rather is determined. Again, this shows that the randomness is not a property of the physical event, but rather of a person’s knowledge and skill. 6.3 What Do We Mean by “Chance”? We have defined “chance” as the absence of predictive power and/or explanation and/or control. Here we should not confuse the concepts of determinacy-indeterminacy and predictable-unpredictable. What matters for decision purposes is whether you can predict. Whether the process is “really” determinate is largely a matter of definition and labeling, an unnecessary philosophical controversy for our operational purposes (and perhaps for any other purpose). Much more discussion of this general topic may be found in my forthcoming book The Philosophy and Practice of Resampling Statistics. The ladle in the previous demonstration becomes unpredictable—that is, random—even though it still is subject to similar physical processes as when it is predictable. I do not deny in principle that these processes can be “understood,” or that one could produce a machine that would—like a baton twirler—make the course of the ladle predictable for many turns. But in practice we cannot make the predictions—and it is the practical reality, rather than the principle, that matters here. When I flip the ladle half a turn or one turn, I control (almost completely) whether it comes down at the handle end or the spoon end, so we do not say that the outcome is chance. Much the same can be said about what happens to the predictability of drawing a given card as one increases the number of times one shuffles a deck of cards. Consider, too, a set of fake dice that I roll. Before you know they are fake, you assume that the probabilities of various outcomes is a matter of chance. But after you know that the dice are loaded, you no longer assume that the outcome is chance. This illustrates how the probabilities you work with are influenced by your knowledge of the facts of the situation. Admittedly, this way of thinking about probability takes some getting used to. For example, suppose a magician does a simple trick with dice such as this one: The magician turns his back while a spectator throws three dice on the table. He is instructed to add the faces. He then picks up any one die, adding the number on the bottom to the previous total. This same die is rolled again. The number it now shows is also added to the total. The magician turns around. He calls attention to the fact that he has no way of knowing which of the three cubes was used for the second roll. He picks up the dice, shakes them in his hand a moment, then correctly announces the final sum. Method: Before the magician picks up the dice he totals their faces. Seven \\[the opposite sides of the dice always add to seven\\] added to this number gives the total obtained by the spectator. (Gardner, 1956, pp. 42-44). Can the dice’s sum really be random if the magician knows exactly what it is—as you also could, if you knew the trick? Forget about “really,” I suggest, and accept that this simply is a useful way of thinking. Later we will talk about the famous draft lottery where the balls were not well mixed and the outcomes did not all have the same probabilities, but where we still consider the lottery “fair.” Later on we shall also consider the distributions of heights of various groups of living things (including people). When we consider all living things taken together, the shape of the overall distribution—many individuals at the tiny end where the viruses are found, and very few individuals at the tall end where the giraffes are—is determined mostly by the distribution of species that have different mean heights. Hence we can explain the shape of that distribution, and we do not say that is determined by “chance.” But with a homogenous cohort of a single species—say, all 25-year-old human females in the U.S.—our best description of the shape of the distribution is “chance.” With situations in between, the shape is partly due to identifiable factors—e.g. age—and partly due to “chance.” Or consider the case of a basketball shooter: What causes her or him to make (or not make) a basket this shot, after a string of successes? Only chance, because the “hot hand” does not exist. But what causes a given shooter to be very good or very poor relative to other players? For that explanation we can point to such factors as the amount of practice or natural talent. Again, all this has nothing to do with whether the mechanism is “really” chance, unlike the arguments that have been raging in physics for a century. That is the point of the ladle demonstration. Our knowledge and our power to predict the outcome gradually transits from non-chance (that is, “determined”) to chance (“not determined”) in a gradual way even though the same sort of physical mechanism produces each throw of the ladle. Earlier I mentioned that when we say that chance controls the outcome of the spoon flip after (say) five revolutions, we mean that there are many small forces that affect the outcome. The effect of each force is not known, and each is independent of the other. None of these forces is large enough for me (as the spoon twirler) to deal with, or else I would deal with it and be able to improve my control and my ability to predict the outcome. This concept of many small influences—“small” meaning in practice those influences whose effects cannot be identified and allowed for—which affect the outcome and whose effects are not knowable and which are independent of each other is fundamental in statistical inference. This concept is the basis of the Theory of Errors and the Central Limit Theorem, which enable us to predict how the mean of a distribution will behave in repeated sampling from the distribution, as will be discussed later. That is, the assumptions of the Central Limit Theorem and of the Normal distribution are the conditions that produce an event that we say is chance-like. It is interesting to consider the relationship of this concept to the quincunx: Therein, any one ball’s fate seems chance-like, but the overall distribution is determined. 6.4 The philosophers’ dispute about the concept of probability Those who call themselves “objectivists” or “frequentists” and those who call themselves “personalists” or “Bayesians” have been arguing for hundreds or even thousands of years about the “nature” of probability. The objectivists insist (correctly) that any estimation not based on a series of observations is subject to potential bias, from which they conclude (incorrectly) that we should never think of probability that way. They are worried about the perversion of science, the substitution of arbitrary assessments for value-free data-gathering. The personalists argue (correctly) that in many situations it is not possible to obtain sufficient data to avoid considerable judgment. Indeed, if a probability is about the future, some judgment is always required—about which observations will be relevant, and so on. They sometimes conclude (incorrectly) that the objectivists’ worries are unimportant. As is so often the case, the various sides in the argument have different sorts of situations in mind. As we have seen, the arguments disappear if one thinks operationally with respect to the purpose of the work , rather than in terms of properties , as mentioned earlier. (Much more about this in my book, The Philosophy and Practice of Statistics and Resampling). Here is an example of the difficulty of focusing on the supposed properties of the mechanism or situation: The mathematical theorist asserts that the probability of a die falling with the “5” side up is 1/6, on the basis of the physics of equally-weighted sides. But if one rolls a particular die a million times, and it turns up “5” less than 1/6 of the time, one surely would use the observed proportion as the practical estimate. The probabilities of various outcomes with cheap dice may depend upon the number of pips drilled out on a side. In 20,000 throws of a red die and 20,000 throws of a white die, the proportions of 3’s and 4’s were, respectively, .159 and .146, .145 and .142 – all far below the expected proportions of .167. That is, 3’s and 4’s occurred about 11 percent less often that if the dice had been perfectly formed, a difference that could make a big difference in a gambling game (Bulmer, 1979, p. 18). It is reasonable to think of both the engineering method (the theoretical approach) and the empirical method (experimentation and data collection) as two alternative ways to estimate a probability. The two methods use different processes and different proxies for the probability you wish to estimate. One must adduce additional knowledge to decide which method to use in any given situation. It is sensible to use the empirical method when data are available. (But use both together whenever possible.) In view of the inevitably subjective nature of probability estimates, you may prefer to talk about “degrees of belief” instead of probabilities. That’s fine, just as long as it is understood that we operate with degrees of belief in exactly the same way as we operate with probabilities. The two terms are working synonyms. Most important: One cannot sensibly talk about probabilities in the abstract, without reference to some set of facts. The topic then loses its meaning, and invites confusion and argument. This also is a reason why a general formalization of the probability concept does not make sense. 6.5 The relationship of probability to the concept of resampling There is no all-agreed definition of the concept of the resampling method in statistics. Unlike some other writers, I prefer to apply the term to problems in both pure probability and statistics. This set of examples may illustrate: Consider asking about the number of hits one would expect from a .250 (25 percent) batter in a 400 at-bat season. One would call this a problem in “probability.” The sampling distribution of the batter’s results can be calculated by formula or produced by Monte Carlo simulation. Now consider examining the number of hits in a given batter’s season, and asking how likely that number (or fewer) is to occur by chance if the batter’s long-run batting average is .250. One would call this a problem in “statistics.” But just as in example (1) above, the answer can be calculated by formula or produced by Monte Carlo simulation. And the calculation or simulation is exactly the same as used in (1). Here the term “resampling” might be applied to the simulation with considerable agreement among people familiar with the term, but perhaps not by all such persons. Next consider an observed distribution of distances that a batter’s hits travel in a season with 100 hits, with an observed mean of 150 feet per hit. One might ask how likely it is that a sample of 10 hits drawn with replacement from the observed distribution of hit lengths (with a mean of 150 feet) would have a mean greater than 160 feet, and one could easily produce an answer with repeated Monte Carlo samples. Traditionally this would be called a problem in probability. Next consider that a batter gets 10 hits with a mean of 160 feet, and one wishes to estimate the probability that the sample would be produced by a distribution as specified in (3). This is a problem in statistics, and by 1996, it is common statistical practice to treat it with a resampling method. The actual simulation would, however, be identical to the work described in (3). Because the work in (4) and (2) differ only in question (4) involving measured data and question (2) involving counted data, there seems no reason to discriminate between the two cases with respect to the term “resampling.” With respect to the pairs of cases (1) and (2), and (3) and (4), there is no difference in the actual work performed, though there is a difference in the way the question is framed. I would therefore urge that the label “resampling” be applied to (1) and (3) as well as to (2) and (4), to bring out the important fact that the procedure is the same as in resampling questions in statistics. One could easily produce examples like (1) and (2) for cases that are similar except that the drawing is without replacement, as in the sampling version of Fisher’s permutation test— for example, a tea taster. And one could adduce the example of prices in different state liquor control systems (see Chapter 8) which is similar to cases (3) and (4) except that sampling without replacement seems appropriate. Again, the analogs to cases (2) and (4) would generally be called “resampling.” The concept of resampling is defined in a more precise way in Chapter 10. Fuller discussion may be found in my The Philosophy and Practice of Statistics and Resampling Conclusion. 6.6 Conclusion We define “chance” as the absence of predictive power and/ or explanation and/or control. When the spoon rotates more than three or four turns I cannot control the outcome—whether spoon or ladle end—with any accuracy. That is to say, I cannot predict much better than 50-50 with more than four rotations. So we then say that the outcome is determined by “chance.” As to those persons who wish to inquire into what the situation “really” is: I hope they agree that we do not need to do so to proceed with our work. I hope all will agree that the outcome of flipping the spoon gradually becomes unpredictable (random) though still subject to similar physical processes as when predictable. I do not deny in principle that these processes can be “understood,” certainly one can develop a machine (or a baton twirler) that will make the outcome predictable for many turns. But this has nothing to do with whether the mechanism is “really” something one wants to say is influenced by “chance.” This is the point of the cooking-spoon demonstration. The outcome traverses from non-chance (determined) to chance (not determined) in a smooth way even though the physical mechanism that produces the revolutions remains much the same over the traverse. 6.7 Endnote The idea that our aim is to advance our work in improving our knowledge and our decisions, rather than to answer “ultimate” questions about what is “really” true is in the same spirit as some writing about quantum theory. In 1930 Ruarck and Urey wrote: “The reader who feels disappointed that the information sought in solving a dynamic problem on the quantum theory is \\[only\\] statistical…should console himself with the thought that we seldom need any information other than that which is given by the quantum theory” (quoted by Cartright, 1987, p. 420). This does not mean that I think that people should confine their learning to what they need in their daily work. Having a deeper philosophical knowledge than you ordinarily need can help you deal with extraordinary problems when they arise. References "],["probability-theory-part-1.html", "7 Probability Theory, Part 1 7.1 Introduction 7.2 Definitions 7.3 Theoretical and historical methods of estimation 7.4 Samples and universes 7.5 The conventions of probability Mutually exclusive events – the addition rule Joint probabilities The monte carlo simulation method (resampling) 7.6 The deductive formulaic method 7.7 Multiplication rule 7.8 Conditional and unconditional probabilities 7.9 The skins again, plus leaving the game early", " 7 Probability Theory, Part 1 7.1 Introduction Let’s assume we understand the nature of the system or mechanism that produces the uncertain events in which we are interested. That is, the probability of the relevant independent simple events is assumed to be known, the way we assume we know the probability of a single “6” with a given die. The task is to determine the probability of various sequences or combinations of the simple events—say, three “6’s” in a row with the die. These are the sorts of probability problems dealt with in this chapter. The resampling method—or just call it simulation or Monte Carlo method, if you prefer—will be illustrated with classic examples. Typically, a single trial of the system is simulated with cards, dice, random numbers, or a computer program. Then trials are repeated again and again to estimate the frequency of occurrence of the event in which we are interested; this is the probability we seek. We can obtain as accurate an estimate of the probability as we wish by increasing the number of trials. The key task in each situation is designing an experiment that accurately simulates the system in which we are interested . This chapter begins the Monte Carlo simulation work that culminates in the resampling method in statistics proper. The chapter deals with problems in probability theory—that is, situations where one wants to estimate the probability of one or more particular events when the basic structure and parameters of the system are known. In later chapters we move on to inferential statistics, where similar simulation work is known as resampling. 7.2 Definitions A few definitions first: Simple Event : An event such as a single flip of a coin, or one draw of a single card. A simple event cannot be broken down into simpler events of a similar sort. Simple Probability (also called “primitive probability”): The probability that a simple event will occur; for example, that my favorite football team, the Skins, will win on Sunday. During a recent season, the “experts” said that the Skins had a 60 percent chance of winning on Opening Day; that estimate is a simple probability. We can model that probability by putting into a bucket six green balls to stand for wins, and four red balls to stand for losses. (Or we could use 60 and 40 balls, or 600 and 400). For the outcome on any given day, we draw one ball from the bucket, and record a simulated win if the ball is green, a loss if the ball is red. So far the bucket has served only as a physical representation of our thoughts. But as we shall see shortly, this representation can help us think clearly about the process of interest to us. It can also give us information that is not yet in our thoughts. Estimating simple probabilities wisely depends largely upon gathering evidence well. It also helps to skillfully adjust one’s probability estimates to make them internally consistent. Estimating probabilities has much in common with estimating lengths, weights, skills, costs, and other subjects of measurement and judgment. Composite Event : A composite event is the combination of two or more simple events. Examples include all heads in three throws of a single coin; all heads in one throw of three coins at once; Sunday being a nice day and the Skins winning; and the birth of nine females out of the next ten calves born if the chance of a female in a single birth is .48. Compound Probability : The probability that a composite event will occur. The difficulty in estimating simple probabilities such as the chance of the Skins winning Sunday arises from our lack of understanding of the world around us. The difficulty of estimating compound probabilities such as the probability of it being a nice day Sunday and the Skins winning is the weakness in our mathematical intuition interacting with our lack of understanding of the world around us. Our task in the study of probability and statistics is to overcome the weakness of our mathematical intuition by using a systematic process of simulation (or the devices of formulaic deductive theory). Consider now a question about a compound probability: What are the chances of the Skins winning their first two games if we think that each of those games can be modeled by our bucket containing six red and four green balls? If one drawing from the bucket represents one game, a second drawing should represent the second game (assuming we replace the first ball drawn in order to keep the chances of winning the same for the two games). If so, two drawings from the bucket should represent two games. And we can then estimate the compound probability we seek with a series of two-ball trial experiments. More specifically, our procedure in this case—the prototype of all procedures in the resampling simulation approach to probability and statistics—is as follows: Put six green (“Win”) and four red (“Lose”) balls in a bucket. Draw a ball, record its color, and replace it (so that the probability of winning the second simulated game is the same as the first). Draw another ball and record its color. If both balls drawn were green record “Yes”; otherwise record “No.” Repeat steps 2-4 a thousand times. Count the proportion of “Y”’s to the total number of “Y”’s and “N”’s; the result is the probability we seek. Much the same procedure could be used to estimate the probability of the Skins winning (say) 3 of their next 4 games. We will return to this illustration again and we will see how it enables us to estimate many other sorts of probabilities. Experiment or Experimental Trial, or Trial, or Resampling Experiment : A simulation experiment or trial is a randomly-generated composite event which has the same characteristics as the actual composite event in which we are interested (except that in inferential statistics the resampling experiment is generated with the “benchmark” or “null” universe rather than with the “alternative” universe). Parameter : A numerical property of a universe. For example, the “true” mean (don’t worry about the meaning of “true”), and the range between largest and smallest members, are two of its parameters. Please see the glossary at the end of the book for a complete list of terms used in the book. 7.3 Theoretical and historical methods of estimation As introduced in Chapter 3, there are two general ways to tackle any probability problem: theoretical-deductive and empirical , each of which has two sub-types. These concepts have complicated links with the concept of “frequency series” discussed earlier. Empirical Methods . One empirical method is to look at actual cases in nature —for example, examine all (or a sample of) the families in Brazil that have four children and count the proportion that have three girls among them. (This is the most fundamental process in science and in information-getting generally. But in general we do not discuss it in this book and leave it to courses called “research methods.” I regard that as a mistake and a shame, but so be it.) In some cases, of course, we cannot get data in such fashion because it does not exist. Another empirical method is to manipulate the simple elements in such fashion as to produce hypothetical experience with how the simple elements behave. This is the heart of the resampling method, as well as of physical simulations such as wind tunnels. Theoretical Methods . The most fundamental theoretical approach is to resort to first principles, working with the elements in their full deductive simplicity, and examining all possibilities. This is what we do when we use a tree diagram to calculate the probability of three girls in families of four children. The formulaic approach is a theoretical method that aims to avoid the inconvenience of resorting to first principles, and instead uses calculational shortcuts that have been worked out in the past. What the Book Teaches . This book teaches you the empirical method using hypothetical cases. Formulas can be misleading for most people in most situations, and should be used as a shortcut only when a person understands exactly which first principles are embodied in the formulas. But most of the time, students and practitioners resort to the formulaic approach without understanding the first principles that lie behind them—indeed, their own teachers often do not understand these first principles—and therefore they have almost no way to verify that the formula is right. Instead they use canned checklists of qualifying conditions. 7.4 Samples and universes The terms “sample” and “universe” (or “population”) \\[5\\] were used earlier without definition. But now these terms must be defined. The concept of a sample For our purposes, a “sample” is a collection of observations for which you obtain the data to be used in the problem. Almost any set of observations for which you have data constitutes a sample. (You might, or might not, choose to call a complete census a sample.) The concept of a universe or population For every sample there must also be a universe “behind” it. But “universe” is harder to define, partly because it is often an imaginary concept. A universe is the collection of things or people that you want to say that your sample was taken from . A universe can be finite and well defined—“all live holders of the Congressional Medal of Honor,” “all presidents of major universities,” “all billion-dollar corporations in the United States.” Of course, these finite universes may not be easy to pin down; for instance, what is a “major university”? And these universes may contain some elements that are difficult to find; for instance, some Congressional Medal winners may have left the country, and there may not be any public records on some billion-dollar corporations. Universes that are called “infinite” are harder to understand, and it is often difficult to decide which universe is appropriate for a given purpose. For example, if you are studying a sample of schizophrenics, what is the universe from which the sample comes? Depending on your purposes, the appropriate universe might be all schizophrenics now alive, or it might be all schizophrenics who might ever live. The latter concept of the universe of schizophrenics is imaginary because some of the universe does not exist. And it is infinite because it goes on forever. Not everyone likes this definition of “universe.” Others prefer to think of a universe, not as the collection of people or things that you want to say your sample was taken from, but as the collection that the sample was actually taken from. This latter view equates the universe to the “sampling frame” (the actual list or set of elements you sample from) which is always finite and existent. The definition of universe offered here is simply the most practical, in my opinion. 7.5 The conventions of probability Let’s review the basic conventions and rules used in the study of probability: Probabilities are expressed as decimals between 0 and 1, like percentages. The weather forecaster might say that the probability of rain tomorrow is .2, or .97. The probabilities of all the possible alternative outcomes in a single “trial” must add to unity. If you are prepared to say that it must either rain or not rain, with no other outcome being possible—that is, if you consider the outcomes to be mutually exclusive (a term that will be discussed below), then one of those probabilities implies the other. That is, if you estimate that the probability of rain is .2—written P(rain) = .2—that implies that you estimate that P (no rain) = .8. Mutually exclusive events – the addition rule Definition: If there are just two events a and b and they are “mutually exclusive” or “disjoint,” each implies the absence of the other. Green and red coats are mutually exclusive for you if (but only if) you never wear more than one coat at a time. To state this idea formally, If P(a and b) = 0 then outcomes a and b, and hence outcome a and its own absence (written P(^a)), are necessarily mutually exclusive, and hence the two probabilities add to unity: P(A) + P(^A) = 1. The sales of your store in a given year cannot be both above and below $1 million. Therefore if P(sales &gt; $1 mil) = .2, P(sales=&lt; $1 mil) = .8. This “complements” rule is useful as a consistency check on your estimates of probabilities. If you say that the probability of rain is .2, then you should check that you think that the probability of no rain is .8; if not, reconsider both the estimates. The same for the probabilities of your team winning and losing its next game. Joint probabilities Let’s return now to the Skins. We said earlier that our best guess of the probability that the Skins will win the first game is .6. Let’s complicate the matter a bit and say that the probability of the Skins winning depends upon the weather; on a nice day we estimate a .65 chance of winning, on a nasty (rainy or snowy) day a chance of .55. It is obvious that we then want to know the chance of a nice day, and we estimate a probability of .7. Let’s now ask the probability that both will happen— it will be a nice day and the Skins will win . Before getting on with the process of estimation itself, let’s tarry a moment to discuss the probability estimates. Where do we get the notion that the probability of a nice day next Sunday is .7? We might have done so by checking the records of the past 50 years, and finding 35 nice days on that date. If we assume that the weather has not changed over that period (an assumption that some might not think reasonable, and the wisdom of which must be the outcome of some non-objective judgment), our probability estimate of a nice day would then be 35/50 = .7. Two points to notice here: 1) The source of this estimate is an objective “frequency series.” And 2) the data come to us as the records of 50 days, of which 35 were nice. We would do best to stick with exactly those numbers rather than convert them into a single number—70 percent. Percentages have a way of being confusing. (When his point score goes up from 2 to 3, my racquetball partner is fond of saying that he has made a “fifty percent increase”; that’s just one of the confusions with percentages.) And converting to a percent loses information: We no longer know how many observations the percent is based upon, whereas 35/50 keeps that information. Now, what about the estimate that the Skins have a .65 chance of winning on a nice day—where does that come from? Unlike the weather situation, there is no long series of stable data to provide that information about the probability of winning. Instead, we construct an estimate using whatever information or “hunch” we have. The information might include the Skins’ record earlier in this season, injuries that have occurred, what the “experts” in the newspapers say, the gambling odds, and so on. The result certainly is not “objective,” or the result of a stable frequency series. But we treat the .65 probability in quite the same way as we treat the .7 estimate of a nice day. In the case of winning, however, we produce an estimate expressed directly as a percent. If we are shaky about the estimate of winning—as indeed we ought to be, because so much judgment and guesswork inevitably goes into it—we might proceed as follows: Take hold of a bucket and two bags of balls, green and red. Put into the bucket some number of green balls—say 10. Now add enough red balls to express your judgment that the ratio is the ratio of expected wins to losses on a nice day, adding or subtracting green balls as necessary to get the ratio you want. If you end up with 13 green and 7 red balls, then you are “modeling” a probability of .65, as stated above. If you end up with a different ratio of balls, then you have learned from this experiment with your own mind processes that you think that the probability of a win on a nice day is something other than .65. Don’t put away the bucket. We will be using it again shortly. And keep in mind how we have just been using it, because our use later will be somewhat different though directly related. One good way to begin the process of producing a compound estimate is by portraying the available data in a “tree diagram” like Figure 4-1. The tree diagram shows the possible events in the order in which they might occur. A tree diagram is extremely valuable whether you will continue with either simulation or the formulaic method. nice day (P = .7) Skins win (P = .65) = .455 (Probability of nice day and Skins win) Skins lose (P = .35) Skins win (P = .55) Skins lose (P = .45) nasty day (P = .3) Figure 4-1: Tree Diagram The monte carlo simulation method (resampling) The steps we follow to simulate an answer to the compoundprobability question are as follows: Put seven blue balls (for “nice day”) and three yellow balls (“not nice”) into a bucket labeled A. Put 65 green balls (for “win”) and 35 red balls (“lose”) into a bucket labeled B. This bucket represents the chance that the Skins will when it is a nice day. Draw one ball from bucket A. If it is blue, carry on to the next step; otherwise record “no” and stop. If you have drawn a blue ball from bucket A, now draw a ball from bucket B, and if it is green, record “yes” on a score sheet; otherwise write “no.” Repeat steps 3-4 perhaps 1000 times. Count the number of “yes” trials. Compute the probability you seek as (number of “yeses”/ 1000). (This is the same as (number of “yeses”/ (number of “yeses” + number of “noes”) Actually doing the above series of steps by hand is useful to build your intuition about probability and simulation methods. But the procedure can also be simulated with a computer. Using the language RESAMPLING STATS, we produce an estimate as follows: We make the weather ‘urn’: The “Skins win when there is no rain” bucket: Now do the simulation: The above procedure gives us the probability that it will be a nice day and the Skins will win—about 45.5 percent. With the aid of a bucket with a different composition—one made by substituting 55 blue and 45 yellow balls in Step 4—a similar procedure yields the chance that it will be a nasty day and the Skins will win. With a similar substitution and procedure we could also estimate the probabilities that it will be a nasty day and the Skins will lose, and a nice day and the Skins will lose. The sum of these probabilities should come close to unity, because the sum includes all the possible outcomes. But it will not exactly equal unity because of what we call “sampling variation” or “sampling error.” Please notice that each trial of the procedure begins with the same numbers of balls in the buckets as the previous trial. That is, you must replace the balls you draw after each trial in order that the probabilities remain the same from trial to trial. Later we will discuss the general concept of replacement versus non-replacement more fully. 7.6 The deductive formulaic method It also is possible to get an answer with formulaic methods to the question about a nice day and the Skins winning. The following discussion of nice-day-Skins-win handled by formula is a prototype of the formulaic deductive method for handling other problems. Return now to the tree diagram (Figure 4-1) above. We can read from the tree diagram that 70 percent of the time it will be nice, and of that 70 percent of the time, 65 percent of the games will be wins. That is, .65 * .7 = .455 = the probability of a nice day and a win. That is the answer we seek. The method seems easy, but it also is easy to get confused and obtain the wrong answer. 7.7 Multiplication rule We can generalize what we have just done. The foregoing formula exemplifies what is known as the “multiplication rule”: P(nice day and win) = P(nice day)* P(winning|nice day) where the vertical line in P(winning|nice day) means “conditional upon.” That is, the vertical line indicates a “conditional probability,” a concept we must consider in a minute. The multiplication rule is a formula that produces the probability of the combination (juncture) of two or more events . More discussion of it will follow below. 7.8 Conditional and unconditional probabilities Two kinds of probability statements— conditional and unconditional —must now be distinguished. It is the appropriate concept when many factors, all small relative to each other rather than one force having an overwhelming influence, affect the outcome. A conditional probability is formally written P(Skins win|rain) = .65, and it is read “The probability that the Skins will win if (given that) it rains is .65.” It is the appropriate concept when there is one (or more) major event of interest in decision contexts. Let’s use another football example to explain conditional and unconditional probabilities. In the year this is being written, the University of Maryland has an unpromising football team. Someone may nevertheless ask what chance the team has of winning the post season game at the bowl to which only the best team in the University of Maryland’s league is sent. One may say that if by some miracle the University of Maryland does get to the bowl, its chance would be a bit less than 50- 50—say, .40. That is, the probability of its winning, conditional on getting to the bowl is .40. But the chance of its getting to the bowl at all is very low, perhaps .01. If so, the unconditional probability of winning at the bowl is the probability of its getting there multiplied by the probability of winning if it gets there; that is, .01 x .40 = .004. (It would be even better to say that .004 is the probability of winning conditional only on having a team, there being a league, and so on, all of which seem almost sure things.) Every probability is conditional on many things—that war does not break out, that the sun continues to rise, and so on. But if all those unspecified conditions are very sure, and can be taken for granted, we talk of the probability as unconditional. A conditional probability is a statement that the probability of an event is such-and-such if something else is so-and-so; it is the “if” that makes a probability statement conditional. True, in some sense all probability statements are conditional; for example, the probability of an even-numbered spade is 6/52 if the deck is a poker deck and not necessarily if it is a pinochle deck or Tarot deck. But we ignore such conditions for most purposes. Most of the use of the concept of probability in the social sciences is conditional probability. All hypothesis-testing statistics (discussed starting in Chapter 14) are conditional probabilities. Here is the typical conditional-probability question used in social-science statistics: What is the probability of obtaining this sample S (by chance) if the sample were taken from universe A? For example, what is the probability of getting a sample of five children with I.Q.s over 100 by chance in a sample randomly chosen from the universe of children whose average I.Q. is 100? One way to obtain such conditional-probability statements is by examination of the results generated by universes like the conditional universe. For example, assume that we are considering a universe of children where the average I.Q. is 100. Write down “over 100” and “under 100” respectively on many slips of paper, put them into a hat, draw five slips several times, and see how often the first five slips drawn are all over 100. This is the resampling (Monte Carlo simulation) method of estimating probabilities. Another way to obtain such conditional-probability statements is formulaic calculation. For example, if half the slips in the hat have numbers under 100 and half over 100, the probability of getting five in a row above 100 is .03125 —that is, .5 5 , or .5 x .5 x .5 x .5 x .5, using the multiplication rule introduced above. But if you are not absolutely sure you know the proper mathematical formula, you are more likely to come up with a sound answer with the simulation method. Let’s illustrate the concept of conditional probability with four cards—two aces and two 3’s (or two black and two red). What is the probability of an ace? Obviously, .5. If you first draw an ace, what is the probability of an ace now? That is, what is the probability of an ace conditional on having drawn one already? Obviously not .5. This change in the conditional probabilities is the basis of mathematician Edward Thorp’s famous system of card-counting to beat the casinos at blackjack (Twenty One). Casinos can defeat card counting by using many decks at once so that conditional probabilities change more slowly, and are not very different than unconditional probabilities. Looking ahead, we will see that sampling with replacement, and sampling without replacement from a huge universe, are much the same in practice, so we can substitute one for the other at our convenience. Let’s further illustrate the concept of conditional probability with a puzzle (from Gardner, 1983, p. 42). “Shuffle a packet of four cards—two red, two black—and deal them face down in a row. Two cards are picked at random, say by placing a penny on each. What is the probability that those two cards are the same color?” 1. Play the game with the cards 100 times, and estimate the probability sought. OR Put slips with the numbers “1,” “1,” “2,” and “2” in a hat, or in an array on a computer named N. Shuffle the slips or the array Take the first number in the hat or array and store it someplace—perhaps in a location called A. Take the second number and store it in B. Subtract the numbers in A and B. If the result is 0, record “Y,” otherwise “N.” Repeat (1-5) 1000 times, and count the proportion of “Y’s.” That proportion equals the probability we seek to estimate. Now let’s play the game differently, first picking one card and putting it back and shuffling before picking a second card. What are the results now? You can try it with the cards, or with a computer program similar to the above. Why do you get different results in the two cases? Let’s ask the question differently: What is the probability of first picking a black card? Clearly, it is 50-50, or .5. Now, if you first pick a black card, what is the probability in the first game above of getting a second black card? There are two red and one black cards left, so now p = 1/3. But in the second game, what is the probability of picking a second black card if the first one you pick is black? It is still .5 because we are sampling with replacement. The probability of picking a second black card conditional on picking a first black card in the first game is 1/3, and it is different from the unconditional probability of picking a black card first. But in the second game the probability of the second black card conditional on first picking a black card is the same as the probability of the first black card. So the reason you lose money if you play the first game at even odds against a carnival game operator is because the conditional probability is different than the original probability. And an illustrative joke: The best way to avoid there being a live bomb aboard your plane flight is to take an inoperative bomb aboard with you; the probability of one bomb is very low, and by the multiplication rule, the probability of two bombs is very very low . Two hundred years ago the same joke was told about the midshipman who, during a battle, stuck his head through a hole in the ship’s side that had just been made by an enemy cannon ball because he had heard that the probability of two cannonballs striking in the same place was one in a million. What’s wrong with the logic in the joke? The probability of there being a bomb aboard already, conditional on your bringing a bomb aboard, is the same as the conditional probability if you do not bring a bomb aboard. Hence you change nothing by bringing a bomb aboard, and do not reduce the probability of an explosion. 7.9 The skins again, plus leaving the game early Let’s carry exactly the same process one tiny step further. Assume that if the Skins win, there is a .3 chance you will leave the game early. Now let us ask the probability of a nice day, the Skins winning, and you leaving early. You should be able to see that this probability can be estimated with three buckets instead of two. Or it can be computed with the multiplication rule as .65 * .7 * .3 = .1365 (about .14)—the probability of a nice day and a win and you leave early. The book shows you the formal method—the multiplication rule, in this case—for several reasons: 1) Simulation is weak with very low probabilities, e.g. P(50 heads in 50 throws). But— a big but— statistics and probability is seldom concerned with very small probabilities. Even for games like poker, the orders of magnitude of 5 aces in a wild game with joker, or of a royal flush, matter little. 2) The multiplication rule is wonderfully handy and convenient for quick calculations in a variety of circumstances. A back-of-the-envelope calculation can be quicker than a simulation. And it can also be useful in situations where the probability you will calculate will be very small, in which case simulation can require considerable computer time to be accurate. (We will shortly see this point illustrated in the case of estimating the rate of transmission of AIDS by surgeons.) 3) It is useful to know the theory so that you are able to talk to others, or if you go on to other courses in the mathematics of probability and statistics. The multiplication rule also has the drawback of sometimes being confusing, however. If you are in the slightest doubt about whether the circumstances are correct for applying it, you will be safer to perform a simulation as we did earlier with the Skins, though in practice you are likely to simulate with the aid of a computer program, as we shall see shortly. So use the multiplication rule only when there is no possibility of confusion. Usually that means using it only when the events under consideration are independent. Notice that the same multiplication rule gives us the probability of any particular sequence of hits and misses—say, a miss, then a hit, then a hit if the probability of a single miss is 2/3. Among the 2/3 of the trials with misses on the first shot, 1/3 will next have a hit, so 2/3 x 1/3 equals the probability of a miss then a hit. Of those 2/9 of the trials, 1/3 will then have a hit, or 2/3 x 1/3 x 1/3 = 2/27 equals the probability of the sequence miss-hit-hit. The multiplication rule is very useful in everyday life. It fits closely to a great many situations such as “What is the chance that it will rain (.3) and that (if it does rain) the plane will not fly (.8)?” Hence the probability of your not leaving the airport today is .3 x .8 = .24. "],["probability-theory-part-i-continued.html", "8 Probability Theory Part I (continued) 8.1 The special case of independence 8.2 The addition of probabilities 8.3 The addition rule 8.4 Theoretical devices for the study of probability 8.5 The Concept of Sample Space 8.6 Endnotes 8.7 Afternote: Useful hints about simple numbers", " 8 Probability Theory Part I (continued) 8.1 The special case of independence A key concept in probability and statistics is that of the independence of two events in which we are interested. Two events are said to be “independent” when one of them does not have any apparent relationship to the other. If I flip a coin that I know from other evidence is a fair coin, and I get a head, the chance of then getting another head is still 50-50 (one in two, or one to one.) And, if I flip a coin ten times and get heads the first nine times, the probability of getting a head on the tenth flip is still 50-50. Hence the concept of independence is characterized by the phrase “The coin has no memory.” (Actually the matter is a bit more complicated. If you had previously flipped the coin many times and knew it to be a fair coin, then the odds would still be 50-50, even after nine heads. But, if you had never seen the coin before, the run of nine heads might reasonably make you doubt that the coin was a fair one.) In the Skins example above, we needed a different set of buckets to estimate the probability of a nice day plus a win, and of a nasty day plus a win. But what if the Skins’ chances of winning are the same whether the day is nice or nasty? If so, we say that the chance of winning is independent of the kind of day. That is, in this special case, P(win|nice day) = P(win|nasty day) and P(nice day and win) = P(nice day) * P(winning|nice day) = P(nice day) * P(winning) Note: See chapter 4’s section on conditional and unconditional probabilities for an explanation of this notation. In this case we need only one set of two buckets to make all the estimates. Independence means that the elements are drawn from 2 or more separate sets of possibilities . That is, P(A|B) = P(A|^B) = P(A) and vice versa . In other words, if the occurrence of the first event does not change this probability that the second event will occur, then the events are independent. Another way to put the matter: Events A and B are said to be independent of each other if knowing whether A occurs does not change the probability that B will occur, and vice versa. If knowing whether A does occur alters the probability of B occurring, then A and B are dependent. If two events are independent, the multiplication rule simplifies to p(A and B) = p(A) * p(B) . I’ll repeat once more: This rule is simply a mathematical shortcut, and one can make the desired estimate by simulation. Also again, if two events are not independent—that is, if P(A|B) is not equal to P(A) because P(A) is dependent upon the occurrence of B, then the formula to be used now is, p(A and B) = p(A|B) * p(B) , which is sufficiently confusing that you are probably better off with a simulation. What about if each of the probabilities is dependent on the other outcome? There is no easy formulaic method to deal with such a situation. People commonly make the mistake of treating independent events as non-independent, perhaps from superstitious belief. After a long run of blacks, roulette gamblers say that the wheel is “due” to come up red. And sportswriters make a living out of interpreting various sequences of athletic events that occur by chance, and they talk of teams that are “due” to win because of the “Law of Averages.” For example, if Barry Bonds goes to bat four times without a hit, all of us (including trained statisticians who really know better) feel that he is “due” to get a hit and that the probability of his doing so is very high— higher that is, than his season’s average. The so-called “Law of Averages” implies no such thing, of course. Events are often dependent in subtle ways. A boy may telephone one of several girls chosen at random. But, if he calls the same girl again (or if he does not call her again), the second event is not likely to be independent of the first. And the probability of his calling her is different after he has gone out with her once than before he went out with her. As noted in the section above, events A and B are said to be independent of each other if the conditional probabilities of A and B remain the same . And the conditional probabilities remain the same if sampling is conducted with replacement . Let’s now re-consider the multiplication rule with the special but important case of independence. Example 5-1: Four Events in a Row—The Multiplication Rule Assume that we want to know the probability of four successful archery shots in a row, where the probability of a success on a given shot is .25. Instead of simulating the process with resampling trials we can, if we wish, arrive at the answer with the “multiplication rule.” This rule says that the probability that all of a given number of independent events (the successful shots) will occur (four out of four in this case) is the product of their individual probabilities—in this case, 1/4 x 1/4 x 1/4 x 1/4 = 1/256. If in doubt about whether the multiplication rule holds in any given case, however, you may check by resampling simulation. For the case of four daughters in a row, assuming that the probability of a girl is .5, the probability is 1/2 x 1/2 x 1/2 x 1/2 = 1/16. Better yet, we’d use the more exact probability of getting a girl: 100/206, and multiply (100/206) 4 . An important point here, however: We have estimated the probability of a particular family having four daughters as 1 in 16—that is, odds of 15 to 1. But note well: This is a very different idea from stating that the odds are 15 to 1 against some family’s having four daughters in a row. In fact, as many families will have four girls in a row as will have boy-girl-boy-girl in that order or girl-boy-girl-boy or any other series of four children. The chances against any particular series is the same—1 in 16—and one-sixteenth of all four-children families will have each of these series, on average. This means that if your next-door neighbor has four daughters, you cannot say how much “out of the ordinary” the event is. It is easy to slip into unsound thinking about this matter. Why do we multiply the probabilities of the independent simple events to learn the probability that they will occur jointly (the composite event)? Let us consider this in the context of three basketball shots each with 1/3 probability of hitting. 2/3 Miss 2/3 Miss 2/3 Miss 1/3 Hit 1/3 Hit 1/3 Hit 1/3 Hit Figure 5-1 is a tree diagram showing a set of sequential simple events where each event is conditional upon a prior simple event. Hence every probability after the first is a conditional probability. Success = 1/3 x 1/3 x 1/3 = 1/27 2/3 Miss 2/3 Miss 2/3 Miss 2/3 Miss 1/3 Hit 1/3 Hit 1/3 Hit Figure 5-1: Tree Diagram for 3 Basketball Shots, Probability of a Hit is 1 / 3 In Figure 5-1, follow the top path first. On approximately one-third of the occasions, the first shot will hit. Among that third of the first shots, roughly a third will again hit on the second shot, that is, 1/3 of 1/3 or 1/3 x 1/3 = 1/9. The top path makes it clear that in 1/3 x 1/3 = 1/9 of the trials, two hits in a row will occur. Then, of the 1/9 of the total trials in which two hits in a row occur, about 1/3 will go on to a third hit, or 1/3 x 1/3 x 1/3 = 1/27. Remember that we are dealing here with independent events; regardless of whether the player made his first two shots, the probability is still 1 in 3 on the third shot. 8.2 The addition of probabilities Back to the Washington Redskins again. You ponder more deeply the possibility of a nasty day, and you estimate with more discrimination that the probability of snow is .1 and of rain it is .2 (with .7 of a nice day). Now you wonder: What is the probability of a rainy day or a nice day? To find this probability by simulation: Put 7 blue balls (nice day), 1 black ball (snowy day) and 2 gray balls (rainy day) into a bucket. You want to know the probability of a blue or a gray ball. To find this probability: Draw one ball and record “yes” if its color is blue or gray, “no” otherwise. Repeat step 1 perhaps 200 times. Find the proportion of “yes” trials. This procedure certainly will do the job. And simulation may be unavoidable when the situation gets more complex. But in this simple case, you are likely to see that you can compute the probability by adding the .7 probability of a nice day and the .2 probability of a rainy day to get the desired probability. This procedure of formulaic deductive probability theory is called the addition rule . 8.3 The addition rule The addition rule applies to mutually exclusive outcomes—that is, the case where if one outcome occurs, the other(s) cannot occur; one event implies the absence of the other when events are mutually exclusive. Green and red coats are mutually exclusive if you never wear more than one coat at a time. If there are only two possible mutually-exclusive outcomes, the outcomes are complementary . It may be helpful to note that mutual exclusivity equals total dependence; if one outcome occurs, the other cannot. Hence we write formally that If p(A and B) = 0 P(A or B) = P(A) + P((B) An outcome and its absence are mutually exclusive, and add to unity. P(A) + P(^A) = 1. Examples include a) rain and no rain, and b) if P(sales &gt; $1mil) = .2, P(sales =&lt; $1mil) = .8. As with the multiplication rule, the addition rule can be a useful shortcut. The answer can always be obtained by simulation, too. We have so far implicitly assumed that a rainy day and a snowy day are mutually exclusive. But that need not be so; both rain and snow can occur on the same day; if we take this possibility into account, we cannot then use the addition rule. Consider the case in which seven days in ten are nice, one day is rainy, one day is snowy, and one day is both rainy and snowy. What is the chance that it will be either nice or snowy? The procedure is just as before, except that some rainy days are included because they are also snowy. When A and B are not mutually exclusive—when it is possible that the day might be both rainy and snowy, or you might wear both red and green coats on the same day, we write (in the latter case) P(red and green coats) &gt; 0, and the appropriate formula is P(red or green) = P(red) + p(green)—P(red and green) In this case as in much of probability theory, the simulation for the case in which the events are not mutually exclusive is no more complex than when they are mutually exclusive; indeed, if you simulate you never even need to know the concept of mutual exclusivity or inquire whether that is your situation. In contrast, the appropriate formula for non-exclusivity is more complex, and if one uses formulas one must inquire into the characteristics of the situation and decide which formula to apply depending upon the classification; if you classify wrongly and therefore apply the wrong formula, the result is a wrong answer. To repeat, the addition rule only works when the probabilities you are adding are mutually exclusive —that is, when the two cannot occur together. The multiplication and addition rules are as different from each other as mortar and bricks; both, however, are needed to build walls. The multiplication rule pertains to a single outcome composed of two or more elements (e.g. weather, and win-or-lose), whereas the addition rule pertains to two or more possible outcomes for one element. Drawing from a card deck (with replacement) provides an analogy: the addition rule is like one draw with two or more possible cards of interest, whereas the multiplication rule is like two or more cards being drawn with one particular “hand” being of interest. 8.4 Theoretical devices for the study of probability It may help you to understand the simulation approach to estimating composite probabilities demonstrated in this book if you also understand the deductive formulaic approach. So we’ll say a bit about it here. The most fundamental concept in theoretical probability is the list of events that may occur, together with the probability of each one (often arranged so as to be equal probabilities). This is the concept that Galileo employed in his great fundamental work in theoretical probability about four hundred years ago when a gambler asked Galileo about the chances of getting a nine rather than a ten in a game of three dice (though others such as Cardano had tackled the subject earlier).9 Galileo wrote down all the possibilities in a tree form, a refinement for mapping out the sample space. Galileo simply displayed the events themselves—such as “2,” “4,” and “4,” making up a total of 10, a specific event arrived at in a specific way. Several different events can lead to a 10 with three dice. If we now consider each of these events, we arrive at the concept of the ways that a total of 10 can arise. We ask the number of ways that an outcome can and cannot occur. (See the paragraph above). This is equivalent both operationally and linguistically to the paths in (say) the quincunx device or Pascal’s Triangle which we shall discuss shortly. A tree is the most basic display of the paths in a given situation. Each branch of the tree—a unique path from the start on the left-hand side to the endpoint on the right-hand side—contains the sequence of all the elements that make up that event, in the order in which they occur. The right-hand ends of the branches constitute a list of the outcomes. That list includes all possible permutations—that is, it distinguishes among outcomes by the orders in which the particular die outcomes occur. 8.5 The Concept of Sample Space The formulaic approach begins with the idea of sample space , which is the set of all possible outcomes of the “experiment” or other situation that interests us. Here is a formal definition from Goldberg (1960/1986, p. 46): A sample space S associated with a real or conceptual experiment is a set such that (1) each element of S denotes an outcome of the experiment, and (2) any performance of the experiment results in an outcome that corresponds to one and only one element of S. Because the sum of the probabilities for all the possible outcomes in a given experimental trial is unity, the sum of all the events in the sample space (S) = 1. Early on, people came up with the idea of estimating probabilities by arraying the possibilities for, and those against, the event occurring. For example, the coin could fall in three ways—head, tail, or on its side. They then speedily added the qualification that the possibilities in the list must have an equal chance, to distinguish the coin falling on its side from the other possibilities (so ignore it). Or, if it is impossible to make the probabilities equal, make special allowance for inequality. Working directly with the sample space is the method of first principles . The idea of a list was refined to the idea of sample space, and “for” and “against” were refined to the “success” and “failure” elements among the total elements. The concept of sample space raises again the issue of how to estimate the simple probabilities. While we usually can estimate the probabilities accurately in gambling games because we ourselves construct the games and therefore control the probabilities that they produce, we have much less knowledge of the structures that underlie the important problems in life— in science, business, the stock market, medicine, sports, and so on. We therefore must wrestle with the issue of what probabilities we should include in our theoretical sample space, or in our experiments. Often we proceed by choosing as an analogy a physical “model” whose properties we know and which we consider to be appropriate—such as a gambling game with coins, dice, cards. This model becomes our idealized setup. But this step makes crystal-clear that judgment is heavily involved in the process, because choosing the analogy requires judgment. A Venn diagram is another device for displaying the elements that make up an event. But unlike a tree diagram, it does not show the sequence of those elements; rather, it shows the extent of overlap among various classes of elements . A Venn diagram expresses by areas (especially rectangular Venn diagrams) the numbers at the end of the branches in a tree. Pascal’s Triangle is still another device. It aggregates the last permutation branches in the tree into combinations —that is, without distinguishing by order. It shows analytically (by tracing them) the various paths that lead to various combinations. The study of the mathematics of probability is the study of calculational shortcuts to do what tree diagrams do. If you don’t care about the shortcuts, then you don’t need the formal mathematics--though it may improve your mathematical insight (or it may not). The resampling method dispenses not only with the shortcuts but also with the entire counting of points in the sample space. 8.6 Endnotes The material in this chapter is largely drawn from Simon (1969; 3rd edition with Paul Burstein, 1985). A given probability may be expressed in terms of probability, odds, or chances, and I shall use all three terms to help familiarize you with them. If the chances are 1 in 10, the odds are 9 to 1, and the probability is .1. If the odds are 2 to 5, the chances are 5 in 7, and the probability is 5/7. If the odds are 99 to 1, the chances are 1 in 100, and the probability is .01. If the odds are 100 to 1, the chances are 1 in 101, and the probability is 1/101. “Likelihood” is a term related to “probability” but is not a complete synonym for it. I hope you are not offended by the references to gambling games in the discussion of statistics in this and other chapters. Not only was the theory of probability invented to answer questions about gambling games, but gambling games still provide useful examples. At one time, some writers believed there was a difference between “objectively sharply defined” and “objectively vague” probabilities. Raiffa gives a clear example of why this is not so: Suppose you are confronted with two options. In option 1, you must toss coin 1 (which is fair and true), guess heads or tails, and win $1.00 if you match and lose $1.00 if you fail to match. In option 2, you have a 50-50 chance of getting coin 2, which has two heads, or of getting coin 3, which has two tails. Not knowing whether you are tossing coin 2 or 3, you must call, toss, and get the payoffs as in option 1. With option 1, the probability of the toss coming out heads is .5; with option 2, the same probability is either 0 or 1, and since the chance of each in turn is .5, the probability of heads is ultimately .5 once again. Nothing is to be gained by saying that one-one-one .5 is sharply defined and that the other is fuzzy. Of course, if , and this is a big “if,” you could experiment with the coin you will toss before you are obliged to declare, then the two options are manifestly asymmetrical. Barring this privilege, the two options are equivalent (Raiffa, 1968, p. 108). “Universe” and “population” are perfect synonyms in scientific research. I choose to use “universe” because it seems to have fewer confusing associations. 8.7 Afternote: Useful hints about simple numbers A useful piece of shortcut knowledge: You can test whether two numbers are equal by subtracting one from the other; if they are equal, the result is zero. Another useful device: You can check whether two units are paired by giving one a minus and one a plus for the same number, and then adding them; if the pair appear together, the sum is zero. Here is another example of the confusion on such matters, this one written by Charles Cotton (part-author of The Complete Angler ) in \\1674: Now six and eight one would think should admit of no difference in advantage with seven, but if you will rightly consider the case, and be so vain to make trial thereof, you will find a great advantage in seven over six and eight. How can that be you will say, hath not six, seven and eight equal chances? For example, in six, quarter deuce and two treys; in eight, six deuce, cinque trey, and two quarters; and hath not seven three as aforesaid? It is confest; but pray consider the disadvantage in the doublets, two treys and two quarters, and you will find that six deuce is sooner thrown than two quarters, and so consequently, cinque Ace or quarter deuce sooner than two treys: I saw an old rook once take up a young fellow in a tavern, upon this very score: the bargain was made that the rook should have seven always and the young gentleman six, and throw continually; agreed to play they went, the rook got the first day ten pound, the next day the like sum; and so for six days together losing in all threescore pounds; notwithstanding the gentleman, I am confident, had square dice, and threw them always himself. (cited in Bulmer 1979, p. 20) Bulmer, M.G., Principles of Statistics (New York: Dover Publications, Inc., 1979).↩︎ "],["probability-theory-part-2-compound-probability.html", "9 Probability Theory, Part 2: Compound Probability 9.1 Introduction 9.2 Puzzle Problems 9.3 Examples of basic problems in probability 9.4 The concepts of replacement and non-replacement 9.5 Endnotes", " 9 Probability Theory, Part 2: Compound Probability 9.1 Introduction In this chapter we will deal with what are usually called “probability problems” rather than the “statistical inference problems” discussed in later chapters. The difference is that for probability problems we begin with a knowledge of the properties of the universe with which we are working. (See Chapter 4 on the definition of resampling.) Before we get down to the business of complex probabilistic problems in this and the next two chapters, let’s consider a couple of peculiar puzzles which do not fit naturally into any chapter in this book, but which are extremely valuable in showing the power of the Monte Carlo simulation method. 9.2 Puzzle Problems The treasure fleet recovered This problem is: A Spanish treasure fleet of three ships was sunk at sea off Mexico. One ship had a trunk of gold forward and another aft, another ship had a trunk of gold forward and a trunk of silver aft, while a third ship had a trunk of silver forward and another trunk of silver aft. Divers just found one of the ships and a trunk of gold in it. They are now taking bets about whether the other trunk found on the same ship will contain silver or gold. What are fair odds? (This is a restatement of a problem that Joseph Bertrand posed early in the 19th century.) In the Goldberg variation: Three identical boxes each contain two coins. In one box both are pennies, in the second both are nickels, and in the third there is one penny and one nickel. A man chooses a box at random and takes out a coin. If the coin is a penny, what is the probability that the other coin in the box is also a penny? These are the logical steps one may distinguish in arriving at a correct answer with deductive logic (portrayed in Figure 6- 1): Postulate three ships—Ship I with two gold chests (G-G), ship II with one gold and one silver chest (G-S), and ship III with S-S. (Choosing notation might well be considered one or more additional steps.) Assert equal probabilities of each ship being found. Step 2 implies equal probabilities of being found for each of the six chests. Fact: Diver finds a chest of gold. Step 4 implies that S-S ship III was not found; hence remove it from subsequent analysis. Three possibilities: 6a) Diver found chest I-Ga, 6b) diver found I-Gb, 6c) diver found II-Gc. From step 2, the cases a, b, and c in step 6 have equal probabilities. If possibility 6a is the case, then the other trunk is I-Gb; the comparable statements for cases 6b and 6c are I-Ga and II-S. From steps 6 and 7: From equal probabilities of the three cases, and no other possible outcome, p (6a) = 1/3, p (6b) = 1/ 3, p (6c) = 1/3, 9. So p(G) = p(6a) + p(6b) = 1/3 + 1/3 = 2/3. I I I G a G b P= 1 / 3 II G a G a G b G b P = ? II II 2G P(G) = 2 / 3 1S P(S) = 1 / 3 P= 1 / 3 G P(G) = .5 G G c P(S) = .5 G c c P = ? S S S P= 1 / 3 III S S III S S 2 1 3 4 5 6 7 8 9 Figure 6-1: Ships with Gold and Silver The following simulation arrives at the correct answer: Construct three buckets containing the numbers “7,7,” “7,8,” and “8,8” respectively. Choose a bucket at random, and shuffle the numbers in it. Choose the first element in the chosen bucket’s vector (a vector is an array or list of numbers). If “8,” stop trial and make no further record. If “7,” continue. Record the second element in the chosen bucket’s vector on the scoreboard. Repeat steps (2 - 5), and calculate the proportion “7’s” on a scoreboard. (The answer should be about 2/3.) Here is a computer simulation with RESAMPLING STATS: The three-door problem Consider the famous problem of the three doors: The player faces three closed containers, one containing a prize and two empty. After the player chooses, s/he is shown that one of the other two containers is empty. The player is now given the option of switching from her/his original choice to the other closed container. Should s/he do so? Answer: Switching doubles the chances of winning. When this problem was published in the Sunday newspapers across the U.S., the thousands of letters—including a good many from Ph.D.s in mathematics—show that logical mathematical deduction fails badly in this case. Most people—both laypersons and statisticians—arrive at the wrong answer. Simulation, however—and hands-on simulation with physical symbols, rather than computer simulation—is a surefire way of obtaining and displaying the correct solution. Table 6-1 shows such a simple simulation with a random-number table. Column 1 represents the container you choose, column 2 where the prize is. Based on columns 1 and 2, column 3 indicates the container that the “host” would now open and show to be empty. Lastly, column 4 scores whether the “switch” or “remain” strategy would be preferable. A count of the number of winning cases for “switch” and the “remain” gives the result sought. Not only is the best choice obvious with this simulation method, but you are likely to understand quickly why switching is better. No other mode of explanation or solution brings out this intuition so well. And it is much the same with other problems in probability and statistics. Simulation can provide not only answers but also insight into why the process works as it does. In contrast, formulas frequently produce obfuscation and confusion for most non-mathematicians. Table 6-1 Simple Simulation With a Random-Number Table Random Pick Random Equiv to Number Door **Actual Location Random Equiv to Number Door Host Opens **Winning Strategy 1 0 1 0 1 1 2 or 3 Remain 2 2 2 2 5 2 1 or 3 R 2 4 2 2 2 2 1 or 3 R 4 2 1 0 6 3 2 Change 3 7 3 8 1 2 1 C 7 7 1 1 1 1 2 or 3 R 9 9 3 5 6 2 1 C 9 6 3 0 5 2 1 C 8 9 2 6 3 3 1 C 8 5 2 4 3 1 3 C 2 8 2 8 8 2 1 or 3 R 6 3 3 4 8 1 2 C 0 9 3 5 2 2 1 C 1 0 1 8 7 2 3 C 7 4 1 7 1 1 2 or 3 R 5 1 2 5 1 2 1 or 3 R 0 2 2 5 2 2 1 or 3 R 0 1 1 3 3 3 2 C 5 2 2 4 6 1 3 C 0 7 1 3 9 3 2 C 4 8 1 8 5 2 3 C Note: Underlining indicates numbers used. Zeros are omitted; numbers 1, 4, 7 = 1; 2, 5, 8 = 2; 3, 6, 9 = 3 9.3 Examples of basic problems in probability A Poker Problem: One Pair (Two of a Kind) What is the chance that the first five cards chosen from a deck of 52 (bridge/poker) cards will contain two (and only two) cards of the same denomination (two 3’s for example)? (Please forgive the rather sterile unrealistic problems in this and the other chapters on probability. They reflect the literature in the field for 300 years. We’ll get more realistic in the statistics chapters.) We shall estimate the odds the way that gamblers have estimated gambling odds for thousands of years. First, check that the deck is not a pinochle deck and is not missing any cards. (Overlooking such small but crucial matters often leads to errors in science.) Shuffle thoroughly until you are satisfied that the cards are randomly distributed. (It is surprisingly hard to shuffle well.) Then deal five cards, and mark down whether the hand does or does not contain a pair of the same denomination. At this point, we must decide whether three of a kind, four of a kind or two pairs meet our criterion for a pair. Since our criterion is “two and only two,” we decide not to count them. Then replace the five cards in the deck, shuffle, and deal again. Again mark down whether the hand contains one pair of the same denomination. Do this many times. Then count the number of hands with one pair, and figure the proportion (as a percentage) of all hands. In one series of 100 experiments, 44 percent of the hands contained one pair, and therefore .44 is our estimate (for the time being) of the probability that one pair will turn up in a poker hand. But we must notice that this estimate is based on only 100 hands, and therefore might well be fairly far off the mark (as we shall soon see). This experimental “resampling” estimation does not require a deck of cards. For example, one might create a 52-sided die, one side for each card in the deck, and roll it five times to get a “hand.” But note one important part of the procedure: No single “card” is allowed to come up twice in the same set of five spins, just as no single card can turn up twice or more in the same hand. If the same “card” did turn up twice or more in a dice experiment, one could pretend that the roll had never taken place; this procedure is necessary to make the dice experiment analogous to the actual card-dealing situation under investigation. Otherwise, the results will be slightly in error. This type of sampling is known as “sampling without replacement,” because each card is not replaced in the deck prior to dealing the next card (that is, prior to the end of the hand). Table 6-2 Results of 100 Trials for the Problem “OnePair” Trial 1 2 3 4 5 6 7 8 9 10 11 12 13 Results Y Y N N Y Y N N Y N N Y Y Trial 14 15 16 17 18 19 20 21 22 23 24 25 26 Results N Y Y Y Y Y N N Y N Y N Y Trial 27 28 29 30 31 32 33 34 35 36 37 38 39 Results N Y N Y Y N Y N N N N Y N Trial 40 41 42 43 44 45 46 47 48 49 50 Results N N N N Y Y Y N N Y N Subtotal: 23 Yes, 27 No = 46% Trial 51 52 53 54 55 56 57 58 59 60 61 62 63 Results N Y N N Y N Y Y N N N Y Y Trial 64 65 66 67 68 69 70 71 72 73 74 75 76 Results Y N N Y N N N N Y N Y N N Trial 77 78 79 80 81 82 83 84 85 86 87 88 89 Results N N N N Y N N N Y Y N Y N Trial 90 91 92 93 94 95 96 97 98 99 100 Results Y Y N N Y Y Y Y N Y N Subtotal: 21 Yes, 29 No = 42% Total: 44 Yes, 56 No = 44% Still another resampling method uses a random number table , such as that which is shown in Table 6-3. Arbitrarily designate the spades as numbers “01-13,” the diamonds as “14-26,” the hearts as “27-39,” and the clubs as “40-52.” Then proceed across a row (or down a column), writing down each successive pair of digits, excluding pairs outside “01-52” and omitting duplication within sets of five numbers. Then translate them back into cards, and see how many “hands” of five “cards” contain one pair each. Table 6-4 shows six such hands, of which hands numbered 2, 3 and 6 contain pairs. Table 6-3 Table of Random Digits 48 52 78 38 11 90 41 83 43 99 51 55 57 03 83 20 15 11 84 33 09 24 08 52 42 70 37 16 66 73 15 54 25 89 70 11 91 65 41 90 88 04 30 72 15 81 34 46 34 24 66 55 67 79 29 18 36 56 96 95 35 06 05 10 37 27 58 38 23 84 94 39 99 50 74 80 41 85 98 63 12 17 04 68 19 98 53 44 16 32 91 01 71 60 19 12 88 85 44 65 52 01 99 56 72 07 96 39 56 34 86 01 81 92 77 83 10 58 92 33 63 48 62 66 32 61 59 74 08 50 15 18 13 45 65 12 32 92 53 82 07 61 71 80 84 29 90 36 05 95 20 71 17 82 83 38 01 87 74 92 77 76 46 28 47 15 04 21 04 75 51 83 91 37 14 32 01 33 90 94 86 10 03 99 95 98 76 97 97 26 45 62 Table 6-4 Six Simulated Trials for the Problem “OnePair” Aces Deuces 3 4 5 6 7 8 9 10 J Q K What the Random Numbers Mean Spades 01 02 03 04 05 06 07 08 09 10 11 12 13 Diamonds 14 15 16 17 18 19 20 21 22 23 24 25 26 Hear ts 27 28 29 30 31 32 33 34 35 36 37 38 39 Clubs 40 41 42 43 44 45 46 47 48 49 50 51 52 Simulation Results Hand 1: 48 52 38 11 41 no pairs Hand 2: 15 11 33 09 24 one pair Hand 3: 25 11 41 04 30 one pair Hand 4: 34 24 29 18 36 no pairs Hand 5: 37 27 38 23 39 no pairs Hand 6: 12 17 04 19 44 one pair Now let’s do the same job using RESAMPLING STATS on the computer. Let’s name “One Pair” the file which simulates a deck of playing cards and solves the problem. Our first task is to simulate a deck of playing cards analogous to the real cards we used previously. We don’t need to simulate all the features of a deck, but only the features that matter for the problem at hand. We require a deck with four “1”s, four “2”s, etc., up to four “13”s. The suits don’t matter for our present purposes. Therefore, with the URN command we join together in a single array the four sets of thirteen numbers, to represent the 13 denominations. At this point we have a complete deck in location A. But that “deck” is in the same order as a new deck of cards. If we do not shuffle the deck, the results will be predictable. Therefore, we write SHUFFLE a b and then deal a poker hand by taking the first five cards from the shuffled hand, using the TAKE statement. Now we must find out if there is one (and only one) pair; we do this with the MULTIPLES statement—the “2” in that statement indicates that it is a duplicate, rather than a singleton or triplicate or quadruplicate that we are testing for— and we put the result in location D. Next we SCORE in location z how many pairs there are, the number in each trial being either zero, one, or two. (The reason we cannot put the result of the MULTIPLES operation directly into the scorecard vector z is that only the SCORE command accumulates results from trial to trial rather than over-writing the result of the past trial with the current one.) And with that we end a single trial. With the REPEAT 1000 statement and the END statement, we command the program to repeat a thousand times the statements in the “loop” between those two lines. When those 1000 repetitions are over, the computer moves on to COUNT the number of “1’s” in SCOREkeeping vector z, each “1” indicating a hand with a pair. And we then PRINT to the screen the result which is found in location k. If we want the proportion of the trials in which a pair occurs, we simply divide the results of the thousand trials by 1000. Note: The file “onepair” on the Resampling Stats software disk contains this set of commands. In one run of the program, the result in kk was .419, so our estimate would be that the probability of a single pair is .419. How accurate are these resampling estimates? The accuracy depends on the number of hands we deal—the more hands, the greater the accuracy. If we were to examine millions of hands, 42 percent would contain a pair each; that is, the chance of getting a pair in the long run is 42 percent. The estimate of 44 percent based on 100 hands in Table 6-2 is fairly close to the long-run estimate, though whether or not it is close enough depends on one’s needs of course. If you need great accuracy, deal many more hands. A note on the “a”s, “b”s, “c”s in the above program, etc.: These “variables” are called “vectors” in RESAMPLING STATS. A vector is an array of elements that gets filled with numbers as RESAMPLING STATS conducts its operations. When RESAMPLING STATS completes a trial these vectors are generally wiped clean except for the “SCORE” vector (here labeled “z”) which keeps track of the result of each trial. To help keep things straight (though the program does not require it), we usually use “z” to name the vector that collects all the trial results, and “k” to denote our overall summary results. Or you could call it something like “scrbrd.” How many trials (hands) should be made for the estimate? There is no easy answer.10 One useful device is to run several (perhaps ten) equal sized sets of trials, and then examine whether the proportion of pairs found in the entire group of trials is very different from the proportions found in the various subgroup sets. If the proportions of pairs in the various subgroups differ greatly from one another or from the overall proportion, then keep running additional larger subgroups of trials until the variation from one subgroup to another is sufficiently small for your purposes. While such a procedure would be impractical using a deck of cards or any other physical means, it requires little effort with the computer and RESAMPLING STATS. Another Introductory Poker Problem Which is more likely, a poker hand with two pairs, or a hand with three of a kind? This is a comparison problem, rather than a problem in absolute estimation as was the previous example. In a series of 100 “hands” that were “dealt” using random numbers, four hands contained two pairs, and two hands contained three of a kind. Is it safe to say, on the basis of these 100 hands, that hands with two pairs are more frequent than hands with three of a kind? To check, we deal another 300 hands. Among them we see fifteen hands with two pairs (3.75 percent) and eight hands with three of a kind (2 percent), for a total of nineteen to ten. Although the difference is not enormous, it is reasonably clear-cut. Another 400 hands might be advisable, but we shall not bother. Earlier I obtained forty-four hands with one pair each out of 100 hands, which makes it quite plain that one pair is more frequent than either two pairs or three-of-a-kind. Obviously, we need more hands to compare the odds in favor of two pairs with the odds in favor of three-of-a-kind than to compare those for one pair with those for either two pairs or three-of-a-kind. Why? Because the difference in odds between one pair, and either two pairs or three-of-a-kind, is much greater than the difference in odds between two pairs and three-of-a-kind. This observation leads to a general rule: The closer the odds between two events, the more trials are needed to determine which has the higher odds. Again it is interesting to compare the odds with the formulaic mathematical computations, which are 1 in 21 (4.75 percent) for a hand containing two pairs and 1 in 47 (2.1 percent) for a hand containing three-of-a-kind—not too far from the estimates of .0375 and .02 derived from simulation. To handle the problem with the aid of the computer, we simply need to estimate the proportion of hands having triplicates and the proportion of hands with two pairs, and compare those estimates. To estimate the hands with three-of-a-kind, we can use a program just like “One Pair” earlier, except instructing the MUL- TIPLES statement to search for triplicates instead of duplicates. The program, then, is: Note: The file “3kind” on the Resampling Stats software disk contains this set of commands. To estimate the probability of getting a two-pair hand, we revert to the original program (counting pairs), except that we examine all the results in SCOREkeeping vector z for hands in which we had two pairs, instead of one . Note: The file “2pair” on the Resampling Stats software disk contains this set of commands. For efficiency (though efficiency really is not important here because the computer performs its operations so cheaply) we could develop both estimates in a single program by simply generating 1000 hands, and count the number with three-of-a-kind and the number with two pairs. Before we leave the poker problems, we note a difficulty with Monte Carlo simulation. The probability of a royal flush is so low (about one in half a million) that it would take much computer time to compute. On the other hand, considerable inaccuracy is of little matter. Should one care whether the probability of a royal flush is 1/100,000 or 1/500,000? 9.4 The concepts of replacement and non-replacement In the poker example above, we did not replace the first card we drew. If we were to replace the card, it would leave the probability the same before the second pick as before the first pick. That is, the conditional probability remains the same. If we replace, conditions do not change. But if we do not replace the item drawn, the probability changes from one moment to the next. (Perhaps refresh your mind with the examples in the discussion of conditional probability in Chapter 5.) If we sample with replacement, the sample drawings remain independent of each other—a topic addressed in Chapter 5. In many cases, a key decision in modeling the situation in which we are interested is whether to sample with or without replacement. The choice must depend on the characteristics of the situation. There is a close connection between the lack of finiteness of the concept of universe in a given situation, and sampling with replacement. That is, when the universe (population) we have in mind is not small, or has no conceptual bounds at all, then the probability of each successive observation remains the same, and this is modeled by sampling with replacement. (“Not finite” is a less expansive term than “infinite,” though one might regard them as synonymous.) Chapter 7 discusses problems whose appropriate concept of a universe is finite, whereas Chapter 8 discusses problems whose appropriate concept of a universe is not finite. This general procedure will be discussed several times, with examples included. 9.5 Endnotes One simple rule-of-thumb is to quadruple the original number. The reason for quadrupling is that four times as many iterations (trials) of this resampling procedure give twice as much accuracy (as measured by the standard deviation, the most frequent measurement of accuracy). That is, the error decreases with the square root of the number of iterations. If you see that you need much more accuracy, then immediately increase the number of iterations even more than four times—perhaps ten or a hundred times.↩︎ "],["probability-theory-part-3.html", "10 Probability Theory, Part 3 10.1 Example 7-1: The Birthday Problem, Illustrating the Probability of Duplication in a Multi-Outcome Sample from an Infinite Universe(File “Birthday”) 10.2 Example 7-2: Three Daughters Among Four Children, Illustrating A Problem With Two Outcomes (Binomial \\[1\\]) And Sampling With Replacement Among Equally Likely Outcomes. 10.3 Variations of the daughters problem 10.4 A note on clarifying and labeling problems 10.5 Binomial trials 10.6 Example 7-3: Three or More Successful Basketball Shots in Five Attempts (Two-Outcome Sampling with Unequally-Likely Outcomes, with Replacement—A Binomial Experiment) 10.7 Note to the student of analytic probability theory 10.8 Example 7-4: One in the Black, Two in the White, and No Misses in Three Archery Shots(Multiple Outcome \\[Multinomial\\] Sampling With Unequally Likely Outcomes; with Replacement.) 10.9 Example 7-5: Two Groups of Heart Patients 10.10 Example 7-6: Dispersion of a Sum of Random Variables—Hammer Lengths—Heads and Handles 10.11 Example 7-7: The Product of Random Variables—Theft by Employees 10.12 Example 7-8: Flipping Pennies to the End 10.13 Example 7-9: A Drunk’s Random Walk 10.14 Example 7-10 10.15 The general procedure 10.16 Endnotes", " 10 Probability Theory, Part 3 This chapter discusses problems whose appropriate concept of a universe is not finite, whereas Chapter 8 discusses problems whose appropriate concept of a universe is finite. How can a universe be infinite yet known? Consider, for example, the possible flips with a given coin; the number is not limited in any meaningful sense, yet we understand the properties of the coin and the probabilities of a head and a tail. 10.1 Example 7-1: The Birthday Problem, Illustrating the Probability of Duplication in a Multi-Outcome Sample from an Infinite Universe(File “Birthday”) As an indication of the power and simplicity of resampling methods, consider this famous examination question used in probability courses: What is the probability that two or more people among a roomful of (say) twenty-five people will have the same birthday? To obtain an answer we need simply examine the first twenty-five numbers from the random-number table that fall between “001” and “365” (the number of days in the year), record whether or not there is a duplication among the twenty-five, and repeat the process often enough to obtain a reasonably stable probability estimate. Pose the question to a mathematical friend of yours, then watch her or him sweat for a while, and afterwards compare your answer to hers/his. I think you will find the correct answer very surprising. It is not unheard of for people who know how this problem works to take advantage of their knowledge by making and winning big bets on it. (See how a bit of knowledge of probability can immediately be profitable to you by avoiding such unfortunate occurrences?) More specifically, these steps answer the question for the case of twenty-five people in the room: Step 1. Let three-digit random numbers “001-365” stand for the 365 days in the year. (Ignore leap year for simplicity.) Step 2. Examine for duplication among the first twenty-five random numbers chosen “001-365.” (Triplicates or higher-order repeats are counted as duplicates here.) If there is one or more duplicate, record “yes.” Otherwise record “no.” Step 3. Repeat perhaps a thousand times, and calculate the proportion of a duplicate birthday among twenty-five people. Here is the first experiment from a random-number table, starting at the top left of the page of numbers and ignoring numbers &gt;365: 021, 158, 116, 066, 353, 164, 019, 080, 312, 020, 353... Now try the program written as follows. Note: The file “birthday” on the Resampling Stats software disk contains this set of commands. We have dealt with this example in a rather intuitive and unsystematic fashion. From here on, we will work in a more systematic, step-by-step manner. And from here on the problems form an orderly sequence of the classical types of problems in probability theory (Chapters 7 and 8), and inferential statistics (Chapters 14 to 22.) 10.2 Example 7-2: Three Daughters Among Four Children, Illustrating A Problem With Two Outcomes (Binomial \\[1\\]) And Sampling With Replacement Among Equally Likely Outcomes. What is the probability that exactly three of the four children in a four-child family will be daughters? The first step is to state that the approximate probability that a single birth will produce a daughter is 50-50 (1 in 2). This estimate is not strictly correct, because there are roughly 106 male children born to each 100 female children. But the approximation is close enough for most purposes, and the 50-50 split simplifies the job considerably. (Such “false” approximations are part of the everyday work of the scientist. The appropriate question is not whether or not a statement is “only” an approximation, but whether or not it is a good enough approximation for your purposes.) The probability that a fair coin will turn up heads is .50 or 50-50, close to the probability of having a daughter. Therefore, flip a coin in groups of four flips, and count how often three of the flips produce heads . (You must decide in advance whether three heads means three girls or three boys.) It is as simple as that. In resampling estimation it is of the highest importance to work in a careful, step-by-step fashion—to write down the steps in the estimation, and then to do the experiments just as described in the steps. Here are a set of steps that will lead to a correct answer about the probability of getting three daughters among four children: Step 1. Using coins, let “heads” equal “boy” and “tails” equal “girl.” Step 2. Throw four coins. Step 3. Examine whether the four coins fall with exactly three tails up. If so, write “yes” on a record sheet; otherwise write “no.” Step 4. Repeat step 2 perhaps two hundred times. Step 5. Count the proportion “yes.” This proportion is an estimate of the probability of obtaining exactly 3 daughters in 4 children. The first few experimental trials might appear in the record sheet as follows: Number of Tails Yes or No 1 No 0 No 3 Yes 2 No No No The probability of getting three daughters in four births could also be found with a deck of cards, a random number table, a die, or with RESAMPLING STATS. For example, half the cards in a deck are black, so the probability of getting a black card (“daughter”) from a full deck is 1 in 2. Therefore, deal a card, record “daughter” or “son,” replace the card, shuffle, deal again, and so forth for 200 sets of four cards. Then count the proportion of groups of four cards in which you got four daughters. A RESAMPLING STATS computer solution to the “3Girls” problem mimics the above steps: Note: The file “3girls” on the Resampling Stats software disk contains this set of commands. Notice that the procedure outlined in the steps above would have been different (though almost identical) if we asked about the probability of three or more daughters rather than exactly three daughters among four children. For three or more daughters we would have scored “yes” on our scorekeeping pad for either three or four heads, rather than for just three heads. Likewise, in the computer solution we would have used the command “Count a &gt;= 3 k.” It is important that, in this case, in contrast to what we did in Example 6-1 (the introductory poker example), the card is replaced each time so that each card is dealt from a full deck. This method is known as sampling with replacement . One samples with replacement whenever the successive events are independent ; in this case we assume that the chance of having a daughter remains the same (1 girl in 2 births) no matter what sex the previous births were \\[2\\]. But, if the first card dealt is black and would not be replaced, the chance of the second card being black would no longer be 26 in 52 (.50), but rather 25 in 51 (.49), if the first three cards are black and would not be replaced, the chances of the fourth card’s being black would sink to 23 in 49 (.47). To push the illustration further, consider what would happen if we used a deck of only six cards, half (3 of 6) black and half (3 of 6) red, instead of a deck of 52 cards. If the chosen card is replaced each time, the 6-card deck produces the same results as a 52-card deck; in fact, a two-card deck would do as well. But, if the sampling is done without replacement, it is impossible to obtain 4 “daughters” with the 6-card deck because there are only 3 “daughters” in the deck. To repeat, then, whenever you want to estimate the probability of some series of events where each event is independent of the other, you must sample with replacement . 10.3 Variations of the daughters problem In later chapters we will frequently refer to a problem which is identical in basic structure to the problem of three girls in four children—the probability of getting 9 females in ten calf births if the probability of a female birth is (say) .5—when we set this problem in the context of the possibility that a genetic engineering practice is effective in increasing the proportion of females (desirable for the production of milk). Another variation: What if we feel the need to get a bit more precise, and to consider the biological fact that more males are born than females—perhaps 52 to 48. This variation would make a solution using coins more tedious. But with the RESAMPLING STATS program it is not at all more difficult. The only commands in the above program that need to be changed are These commands now become The rest of the program remains unchanged. 10.4 A note on clarifying and labeling problems In conventional analytic texts and courses on inferential statistics, students are taught to distinguish between various classes of problems in order to decide which formula to apply. I doubt the wisdom of categorizing and labeling problems in that fashion, and the practice is unnecessary here. I consider it better that the student think through every new problem in the most fundamental terms. The exercise of this basic thinking avoids the mistakes that come from too-hasty and superficial pigeon-holing of problems into categories. Nevertheless, in order to help readers connect up the resampling material with the conventional curriculum of analytic methods, the examples presented here are given their conventional labels. And the examples given here cover the range of problems encountered in courses in probability and inferential statistics. To repeat, one does not need to classify a problem when one proceeds with the Monte Carlo resampling method; you simply model the features of the situation you wish to analyze. In contrast, with conventional methods you must classify the situation and then apply procedures according to rules that depend upon the classification; often the decision about which rules to follow must be messy because classification is difficult in many cases, which contributes to the difficulty of choosing correct conventional formulaic methods. 10.5 Binomial trials The problem of the three daughters in four births is known in the conventional literature as a “binomial sampling experiment with equally-likely outcomes.” “Binomial” means that the individual simple event (a birth or a coin flip) can have only two outcomes (boy or girl, heads or tails), “binomial” meaning “two names” in Latin \\[1\\]. A fundamental property of binomial processes is that the individual trials are independent , a concept discussed earlier. A binomial sampling process is a series of binomial events about which one may ask many sorts of questions—the probability of exactly X heads (“successes”) in N trials, or the probability of X or more “successes” in N trials, and so on. “Equally likely outcomes” means we assume that the probability of a girl or boy in any one birth is the same (though this assumption is slightly contrary to fact); we represent this assumption with the equal-probability heads and tails of a coin. Shortly we will come to binomial sampling experiments where the probabilities of the individual outcomes are not equal. The term “with replacement” was explained earlier; if we were to use a deck of red and black cards (instead of a coin) for this resampling experiment, we would replace the card each time a card is drawn. Example 6-1, the introductory poker example given earlier, illustrated sampling without replacement, as will other examples to follow. This problem would be done conventionally with the binomial theorem using probabilities of .5, or of .48 and .52, asking about 3 successes in 4 trials. 10.6 Example 7-3: Three or More Successful Basketball Shots in Five Attempts (Two-Outcome Sampling with Unequally-Likely Outcomes, with Replacement—A Binomial Experiment) What is the probability that a basketball player will score three or more baskets in five shots from a spot 30 feet from the basket, if on the average she succeeds with 25 percent of her shots from that spot? In this problem the probabilities of “success” or “failure” are not equal, in contrast to the previous problem of the daughters. Instead of a 50-50 coin, then, an appropriate “model” would be a thumbtack that has a 25 percent chance of landing “up” when it falls, and a 75 percent chance of landing down. If we lack a thumbtack known to have a 25 percent chance of landing “up,” we could use a card deck and let spades equal “success” and the other three suits represent “failure.” Our resampling experiment could then be done as follows: Let “spade” stand for “successful shot,” and the other suits stand for unsuccessful shot. Draw a card, record its suit and replace. Do so five times (for five shots). Record whether the outcome of step 2 was three or more spades. If so indicate “yes,” and otherwise “no.” Repeat steps 2-4 perhaps four hundred times. Count the proportion “yes” out of the four hundred throws. That proportion estimates the probability of getting three or more baskets out of five shots if the probability of a single basket is .25. The first three repetitions on your score sheet might look like this: No S (Spade) N (Non-spade) N N N No N N N N N N . N . Yes S . S . S . Instead of cards, we could have used two-digit random numbers, with (say) “1-25” standing for “success,” and “26-00” (“00” in place of “100”) standing for failure. Then the steps would simply be: Let the random numbers “1-25” stand for “successful shot,” “26-00” for unsuccessful shot. Draw five random numbers; Count how many of the numbers are between “01” and “25.” If three or more, score “yes.” Repeat step 2 four hundred times. If you understand the earlier “girls” program, then the program “bball” should be easy: To create 1000 samples, we start with a REPEAT statement. We then GENERATE 5 numbers between “1” and “4” to simulate the 5 shots, each with a 25 percent—or 1 in 4—chance of scoring. We decide that “1” will stand for a successful shot, and “2” through “4” will stand for a missed shot, and therefore we COUNT the number of “1”’s in a to determine the number of shots resulting in baskets in the current sample. The next step is to transfer the results of each trial to vector z by way of a SCORE statement. We then END the loop. The final step is to search the vector z after the 1000 samples have been generated and COUNT the times that 3 or more baskets were made. We place the results in k, and then PRINT. REPEAT 1000 Note: The file “bball” on the Resampling Stats software disk contains this set of commands. 10.7 Note to the student of analytic probability theory This problem would be done conventionally with the binomial theorem, asking about the chance of getting 3 successes in 5 trials, with the probability of a success = .25. 10.8 Example 7-4: One in the Black, Two in the White, and No Misses in Three Archery Shots(Multiple Outcome \\[Multinomial\\] Sampling With Unequally Likely Outcomes; with Replacement.) Assume from past experience that a given archer puts 10 percent of his shots in the black (“bullseye”) and 60 percent of his shots in the white ring around the bullseye, but misses with 30 percent of his shots. How likely is it that in three shots the shooter will get exactly one bullseye, two in the white, and no misses? Notice that unlike the previous cases, in this example there are more than two outcomes for each trial. This problem may be handled with a deck of three colors (or suits) of cards in proportions varying according to the probabilities of the various outcomes, and sampling with replacement. Using random numbers is simpler, however: Step 1. Let “1” = “bullseye,” “2-7” = “in the white,” and “8-0” = “miss.” Step 2. Choose three random numbers, and examine whether there are one “1” and two numbers “2-7.” If so, record “yes,” otherwise “no.” Step 3. Repeat step 2 perhaps 400 times, and count the proportion of “yeses.” This estimates the probability sought. This problem would be handled in conventional probability theory with what is known as the Multinomial Distribution. This problem may be quickly solved on the computer with RESAMPLING STATS with the program labeled “bullseye” below. Bullseye has a complication not found in previous problems: It tests whether two different sorts of events both happen—a bullseye plus two shots in the white. After GENERATing three randomly-drawn numbers between 1 and 10, we check with the COUNT command to see if there is a bullseye. If there is, the IF statement tells the computer to continue with the operations, checking if there are two shots in the white; if there is no bullseye, the IF command tells the computer to END the trial and start another trial. A thousand repetitions are called for, the number of trials meeting the criteria are counted, and the results are then printed. In addition to showing how this particular problem may be handled with RESAMPLING STATS, the “bullseye” program teaches you some more fundamentals of computer programming. The IF statement and the two loops, one within the other, are basic tools of programming. Note: The file “bullseye” on the Resampling Stats software disk contains this set of commands. This example illustrates the addition rule that was introduced and discussed in Chapter 5. In Example 7-4, a bullseye, an in-the-white shot, and a missed shot are “mutually exclusive” events because a single shot cannot result in more than one of the three possible outcomes. One can calculate the probability of either of two mutually-exclusive outcomes by adding their probabilities. The probability of either a bullseye or a shot in the white is .1 + .6 = .7. The probability of an arrow either in the white or a miss is .6 + .3 = .9. The logic of the addition rule is obvious when we examine the random numbers given to the outcomes. Seven of 10 random numbers belong to “bullseye” or “in the white,” and nine of 10 belong to “in the white” or “miss.” 10.9 Example 7-5: Two Groups of Heart Patients We want to learn how likely it is that, by chance, group A would have as little as two deaths more than group B. Table 7-1 Two Groups of Heart Patients Live Die Group A 79 11 Group B 21 9 This problem, phrased here as a question in probability, is the prototype of a problem in statistics that we will consider later (which the conventional theory would handle with a “chi square distribution”). We can handle it in either of two ways, as follows: 1. Put 120 balls into a bucket, 100 white (for live) and 20 black (for die). 2a. Draw 30 balls randomly and assign them to Group B; the others are assigned to group A. 3a. Count the numbers of black balls in the two groups and determine whether Group A’s excess “deaths” (= black balls), compared to Group B, is two or fewer (or what is equivalent in this case, whether there are 11 or fewer black balls in Group A); if so, write “Yes,” otherwise “No.” 4a. Repeat steps 2a and 3a perhaps 1000 times and compute the proportion “Yes.” A second way we shall think about this sort of problem may be handled as follows: 2b. Draw balls one by one, replacing the drawn ball each time, until you have accumulated 90 balls for Group A and 30 balls for Group B. (You could, of course, just as well use a bucket for 4 white and 1 black balls or 8 white and 2 black in this approach.) 3b. As in approach “a” above, count the numbers of black balls in the two groups and determine whether Group A’s excess deaths is two or fewer; if so, write “Yes,” otherwise “No.” 4b. As above, repeat steps 2a and 3a perhaps 1000 times and compute the proportion “Yes.” We must also take into account the possibility of a similar eye-catching “unbalanced” result of a much larger proportion of deaths in Group B. It will be a tough decision how to do so, but a reasonable option is to simply double the probability computed in step 4a or 4b. Deciding which of these two approaches—the “permutation” (without replacement) and “bootstrap” (with replacement) methods—is the more appropriate is often a thorny matter; it will be discussed latter in Chapter 18. In many cases, however, the two approaches will lead to similar results. Later, we will actually carry out these procedures with the aid of RESAMPLING STATS, and estimate the probabilities we seek. 10.10 Example 7-6: Dispersion of a Sum of Random Variables—Hammer Lengths—Heads and Handles The distribution of lengths for hammer handles is as follows: 20 percent are 10 inches long, 30 percent are 10.1 inches, 30 percent are 10.2 inches, and 20 percent are 10.3 inches long. The distribution of lengths for hammer heads is as follows: 2.0 inches, 20 percent; 2.1 inches, 20 percent; 2.2 inches, 30 per- cent; 2.3 inches, 20 percent; 2.4 inches, 10 percent. If you draw a handle and a head at random, what will be the mean total length? In Chapter 5 we saw that the conventional formulaic method tells you that an answer with a formula that says the sum of the means is the mean of the sums, but it is easy to get the answer with simulation. But now we ask about the dispersion of the sum. There are formulaic rules for such measures as the variance. But consider this other example: What proportion of the hammers made with handles and heads drawn at random will have lengths equal to or greater than inches? No simple formula will provide an answer. And if the number of categories is increased considerably, any formulaic approach will be become burdensome if not undoable. But Monte Carlo simulation produces an answer quickly and easily, as follows: Fill a bucket with 2 balls marked “10 inches,” 3 balls marked “10.1”... 2 marked “10.3,” for the handles. Fill another bucket with 2 balls marked “2.0”... 1 marked “2.4” for the heads. Pick a ball from each bucket, calculate the sum, and replace the balls. Repeat perhaps 200 times (more when you write a computer program), and calculate the proportion greater than 12.4 inches. You may also want to forego learning the standard “rule,” and simply estimate the mean this way, also. As an exercise, compute the interquartile range—the difference between the 25th and the 75th percentiles. 10.11 Example 7-7: The Product of Random Variables—Theft by Employees The distribution of the number of thefts per month you can expect in your business is as follows: Number Probability 0 .5 1 .2 2 .1 3 .1 4 .1 The amounts that may be stolen on any theft are as follows: Amount Probability $50 .4 $75 .4 $100 .1 $125 .1 The same procedure as used above to estimate the mean length of hammers—add the lengths of handles and heads—can be used for this problem except that the results of the drawings from each bucket are multiplied rather than added. In this case there is again a simple rule: The mean of the products equals the product of the means. But this rule holds only when the two urns are indeed independent of each other, as they are in this case. The next two problems are a bit harder than the previous ones; you might skip them for now and come back to them a bit later. However, with the Monte Carlo simulation method they are within the grasp of any introductory student who has had just a bit of experience with the method. In contrast, a standard book whose lead author is Frederick Mosteller, as respected a statistician as there is, says of this type of problem: “Naturally, in this book we cannot expect to study such difficult problems in their full generality \\[that is, show how to solve them, rather than merely state them\\], but we can lay a foundation for their study” (Mosteller, Rourke, and Thomas 1970). 10.12 Example 7-8: Flipping Pennies to the End Two players, each with a stake of ten pennies, engage in the following game: A coin is tossed, and if it is (say) heads, player A gives player B a penny; if it is tails, player B gives player A a penny. What is the probability that one player will lose his or her entire stake of 10 pennies if they play for 200 tosses? This is a classic problem in probability theory; it has many everyday applications in situations such as inventory management. For example, what is the probability of going out of stock of a given item in a given week if customers and deliveries arrive randomly? It also is a model for many processes in modern particle physics. Solution of the penny-matching problem with coins is straightforward. Repeatedly flip a coin and check if one player or the other reaches a zero balance before you reach 200 flips. Or with random numbers: Step 1. Numbers “1-5” = head = “+1”; Numbers “6-0” = tail = “-1.” Step 2. Proceed down a series of 200 numbers, keeping a running tally of the “+1”’s and the “-1”’s. If the tally reaches “+10” or “-10” on or before the two-hundredth digit, record “yes”; otherwise record “no.” Step 3. Repeat step 2 perhaps 400 or 1000 times, and calculate the proportion of “yeses.” This estimates the probability sought. The following RESAMPLING STATS program also solves the problem. The heart of the program starts at the line where the program models a coin flip with the statement “GENERATE 1 1,2 C.” After you study that, go back and notice the REPEAT 200 loop that describes the procedure for flipping a coin 200 times. Finally, note how the REPEAT 1000 loop simulates 1000 games, each game consisting of 200 coin flips. Note: The file “pennies” on the Resampling Stats software disk contains this set of commands. A similar example: Your warehouse starts out with a supply of twelve capacirators. Every three days a new shipment of two capacirators is received. There is a .6 probability that a capacirator will be used each morning, and the same each afternoon. (It is as if a random drawing is made each half-day to see if a capacirator is used; two capacirators may be used in a single day, or one or none). How long will be it, on the average, before the warehouse runs out of stock? 10.13 Example 7-9: A Drunk’s Random Walk If a drunk chooses the direction of each step randomly, will he ever get home? If he can only walk on the road on which he lives, the problem is almost the same as the gambler’s-ruin problem above (“pennies”). But if the drunk can go north-south as well as east-west, the problem becomes a bit different and interesting. Looking now at figure 10.1 — what is the probability of the drunk reaching either his house (at 3 steps east, 2 steps north) or my house (1 west, 4 south) before he finishes taking twelve steps? One way to handle the problem would be to use a four-directional spinner such as is used with a child’s board game, and then keep track of each step on a piece of graph paper. The reader may construct a RESAMPLING STATS program as an exercise. Figure 10.1: Drunk’s random walk 10.14 Example 7-10 Let’s end this chapter with an actual example that will be used again in Chapter 8 when discussing probability in finite universes, and then at great length in the context of statistics in Chapter 18. This example also illustrates the close connection between problems in pure probability and those in statistical inference. As of 1963, there were 26 U.S. states in whose liquor systems the retail liquor stores are privately owned, and 16 “monopoly” states where the state government owns the retail liquor stores. (Some states were omitted for technical reasons.) These were the representative 1961 prices of a fifth of Seagram 7 Crown whiskey in the two sets of states: 16 monopoly states: $4.65, $4.55, $4.11, $4.15, $4.20, $4.55, $3.80, $4.00, $4.19, $4.75, $4.74, $4.50, $4.10, $4.00, $5.05, $4.20. Mean = $4.35 26 private-ownership states: $4.82, $5.29, $4.89, $4.95, $4.55, $4.90, $5.25, $5.30, $4.29, $4.85, $4.54, $4.75, $4.85, $4.85, $4.50, $4.75, $4.79, $4.85, $4.79, $4.95, $4.95, $4.75, $5.20, $5.10, $4.80, $4.29. Mean = $4.84 GOVERNMENT 5 0 350 400 450 500 550 Cents Mean: $4.35 PRIVATE 5 0 350 400 450 500 550 Cents Mean: $4.84 PRIVATE + GOVERNMENT 5 0 350 400 450 500 550 Cents Figure 7-2: Liquor Prices Let us consider that all these states’ prices constitute one single universe (an assumption whose justification will be discussed later). If so, one can ask: If these 42 states constitute a single universe, how likely is it that one would choose two samples at random, containing 16 and 26 observations, that would have prices as different as $.49 (the difference between the means that was actually observed)? This can be thought of as problem in pure probability because we begin with a known universe and ask how it would behave with random drawings from it. We sample with replacement ; the decision to do so, rather than to sample without replacement (which is the way I had first done it, and for which there may be better justification) will be discussed later. We do so to introduce a “bootstrap”-type procedure (defined later) as follows: Write each of the forty-two observed state prices on a separate card. The shuffled deck simulated a situation in which each state has an equal chance for each price. Repeatedly deal groups of 16 and 26 cards, replacing the cards as they are chosen, to simulate hypothetical monopoly-state and private-state samples. For each trial, calculate the difference in mean prices. These are the steps systematically: Step A: Write each of the 42 prices on a card and shuffle. Steps B and C (combined in this case): i) Draw cards randomly with replacement into groups of 16 and 26 cards. Then ii) calculate the mean price difference between the groups, and iii) compare the simulation-trial difference to the observed mean difference of $4.84 - $4.35 = $.49; if it is as great or greater than $.49, write “yes,” otherwise “no.” Step D: Repeat step B-C a hundred or a thousand times. Calculate the proportion “yes,” which estimates the probability we seek. The probability that the postulated universe would produce a difference between groups as large or larger than observed in 1961 is estimated by how frequently the mean of the group of randomly-chosen sixteen prices from the simulated state-ownership universe is less than (or equal to) the mean of the actual sixteen state-ownership prices. The following computer program performs the operations described above. The results shown above—not even one “success” in 1,000 trials—imply that there is only a very small probability that two groups with mean prices as different as were observed would happen by chance if drawn with replacement from the universe of 42 observed prices. Here we think of these states as if they came from a non-finite universe, which is one possible interpretation for one particular context. However, in Chapter 8 we will postulate a finite universe, which is appropriate if it is reasonable to consider that these observations constitute the entire universe (aside from those states excluded from the analysis because of data complexities). 10.15 The general procedure Chapter 19 generalizes what we have done in the probability problems above into a general procedure, which will in turn be a subpart of a general procedure for all of resampling. 10.16 Endnotes Conventional labels such as “binomial” are used here for general background and as guideposts to orient the student of conventional statistics. You do not need to know these labels to understand the resampling approach; one of the advantages of resampling is that it avoids errors resulting from incorrect pigeonholing of problems. This assumption is slightly contrary to scientific fact. A better example would be: What is the probability that four mothers delivering successively in a hospital will all have daughters? But that example has other difficulties—which is the way science always is. References "],["probability-theory-part-4-estimating-probabilities-from-finite-universes.html", "11 Probability Theory, Part 4: Estimating Probabilities from Finite Universes 11.1 Introduction 11.2 Some building-block programs 11.3 Problems in finite universes 11.4 Example 8-1: What is the Probability of Selecting Four Girls and One Boy When Selecting Five Students From Any Twenty-five Girls and Twenty-five Boys?(Sampling Without Replacement When There are Two Outcomes and the Order Does Not Matter) 11.5 Example 8-2: Nine Spades and Four Clubs in a Bridge Hand (Multiple-Outcome Sampling Without Replacement, Order Does not Matter) 11.6 Example 8-3: A Total of Fifteen Points in a Bridge Hand When Ace = 4, King = 3, Queen = 2, and Jack = 1. 11.7 Example 8-4: Four Girls and Then One Boy From Twenty-five Girls and Twenty-five Boys Order Matters, Sampling Without Replacement, Two Outcomes, Several of Each Item 11.8 Example 8-5: Four or More Couples Getting Their Own Partners When Ten Couples are Paired Randomly (Probability of Matching by Chance) (Program “Couples”) 11.9 Example 8-6: Matching Hats: Another famous problem of this sort: The hat-checker at a restaurant mixes up the hats of a party of 6 men. What is the probability that at least one will get his own hat? 11.10 Example 8-7: Twenty executives are to be assigned to two divisions of a firm Example 8-8: Executives Moving 11.11 Example 8-9: State Liquor Systems Again 11.12 Example 8-10: A Compound Problem: Five or More Spades in One Bridge Hand, and Four Girls and a Boy in a Five-Child Family 11.13 Summary", " 11 Probability Theory, Part 4: Estimating Probabilities from Finite Universes 11.1 Introduction The examples in Chapter 7 dealt with infinite universes , in which the probability of a given simple event is unaffected by the outcome of the previous simple event. But now we move on to finite universes, situations in which you begin with a given set of objects whose number is not enormous—say, a total of two, or two hundred, or two thousand. If we liken such a situation to a bucket containing balls of different colors each with a number on it, we are interested in the probability of drawing various sets of numbered and colored balls from the bucket on the condition that we do not replace balls after they are drawn. In the cases addressed in this chapter, it is important to remember that the single events no longer are independent of each other. A typical situation in which sampling without replacement occurs is when items are chosen from a finite universe— for example, when children are selected randomly from a classroom. If the class has five boys and five girls, and if you were to choose three girls in a row, then the chance of selecting a fourth girl on the next choice obviously is lower than the chance that you would pick a girl on the first selection. The key to dealing with this type of problem is the same as with earlier problems: You must choose a simulation procedure that produces simple events having the same probabilities as the simple events in the actual problem involving sampling without replacement. That is, you must make sure that your simulation does not allow duplication of events that have already occurred. The easiest way to sample without replace- ment with resampling techniques is by simply ignoring an outcome if it has already occurred. Examples 8-1 through 8-10 deal with some of the more important sorts of questions one may ask about drawings without replacement from such an urn. To get an overview, I suggest that you read over the titles of Examples 8-1 to 8-10 before beginning to work through the examples themselves. This chapter also revisits the general procedure used in solving problems in probability and statistics with simulation, here in connection with problems involving a finite universe. The steps that one follows in simulating the behavior of a universe of interest are set down in such fashion that one may, by random drawings, deduce the probability of various events. Having had by now the experience of working through the problems in Chapters 5 and 7, the reader should have a solid basis to follow the description of the general procedure which then helps in dealing with specific problems. Let us begin by describing some of the major sorts of problems with the aid of a bucket with six balls. 11.2 Some building-block programs Case 1. Each of six balls is labeled with a number between “1” and “6.” We ask: What is the probability of choosing balls 1, 2, and 3 in that order if we choose three balls without replacement? Figure 8-1 diagrams the events we consider “success.” 5 1 3 1 2 3 pick #: 1 2 3 4 2 6 Figure 8-1: The Event Classified as “Success” for Case 1 Case 2. We begin with the same bucket as in Case 1, but now ask the probability of choosing balls 1, 2, and 3 in any order if we choose three balls without replacement. Figure 8-2 diagrams two of the events we consider success. These possibilities include that which is shown in Figure 8-1 above, plus other possibilities. 1 2 3 3 2 1 2 3 1 pick #: 1 2 3 5 1 3 4 2 6 Figure 8-2: An Incomplete List of the Events Classified as “Success” for Case 2 Case 3. The odd-numbered balls “1,” “3,” and “5,” are painted red and the even-numbered balls “2,” “4,” and “6” are painted black. What is the probability of getting a red ball and then a black ball in that order? Some possibilities are illustrated in Figure 8-3, which includes the possibility shown in Figure 8-1. It also includes some but not all possibilities found in Figure 8-2; for example, Figure 8-2 includes choosing balls 2, 3 and 1 in that order, but Figure 8-3 does not. 1 2 3 5 3 2 1 1 3 4 2 6 pick #: 1 2 3 Figure 8-3: An Incomplete List of the Events Classified as “Success” for Case 3 Case 4. What is the probability of getting two red balls and one black ball in any order? 1 2 3 5 1 3 2 1 3 4 2 6 pick #: 1 2 3 Figure 8-4: An Incomplete List of the Events Classified as “Success” for Case 4 Case 5. Various questions about matching may be asked with respect to the six balls. For example, what is the probability of getting ball 1 on the first draw or ball 2 on the second draw or ball 3 on the third draw? (Figure 8-5) Or, what is the probability of getting all balls on the draws corresponding to their numbers? 5 1 3 4 2 6 1 pick #: 1 2 3 4 5 6 or (and) 2 pick #: 1 2 pick #: 1 2 3 4 5 6 or (and) 3 3 4 5 6 Figure 8-5: An Incomplete List of the Events Classified as “Success” for Case 5 11.3 Problems in finite universes 11.4 Example 8-1: What is the Probability of Selecting Four Girls and One Boy When Selecting Five Students From Any Twenty-five Girls and Twenty-five Boys?(Sampling Without Replacement When There are Two Outcomes and the Order Does Not Matter) The important difference between this example and the infinite-universe examples in the prior chapter is that the probability of obtaining a boy or a girl in a single simple event differs from one event to the next in this example, whereas it stays the same when the sampling is with replacement. To illustrate, the probability of a girl is .5 (25 out of 50) when the first student is chosen, but the probability of a girl is either 25/49 or 24/49 when the second student is chosen, depending on whether a boy or a girl was chosen on the first pick. Or after, say, three girls and one boy are picked, the probability of getting a girl on the next choice is (28-3)/(50-4) = 22/46 which is clearly not equal to .5. As always, we must create a satisfactory analog to the process whose probability we want to learn. In this case, we can use a deck of 50 cards, half red and half black, and deal out five cards without replacing them after each card is dealt; this simulates the choice of five students from among the fifty. We can also work the example with random numbers. But if so, we cannot simply designate “01-25” as boys and “26-50” as girls (ignoring 51-00 when they occur), because if we did so the probability of a girl would be the same on each pick. Instead, to simulate non-replacement, we specify that no number can be taken into the sample twice, just as no student can be chosen twice. That is, if we come across a number more than once, we simply ignore it after the first occurrence. More specifically: Step 1. Let “01-25” = girls, “26-50” = boys Step 2. Select five non-duplicating numbers; if a number comes up more than once, ignore it. Count whether there are four numbers “01-25,” and one “26-50.” If so, write “yes,” otherwise “no.” Step 3. Repeat step 2 perhaps 400 times, and count the proportion “yes,” which estimates the probability sought. The results of a few experimental trials are shown in Table 8-1. Table 8-1 Experimental Trials Experiment Numbers Chosen Success ? 1 18, 22, \\[18 selected but ignored\\] 27, 2, 49 No 2 37, 19, 18, 7, 9 Yes 3 13, 14, 2, 29, 24 Yes It is important to notice that in this problem we do not distinguish among particular girls (or boys). That is, it does not matter which girl (or boy) is selected in a given trial. Nor did we pay attention to the order in which we selected girls or boys. This is an instance of Case 4 discussed above. Subsequent problems will deal with situations where the order of selection, and the particular individuals, do matter. A solution to this problem with RESAMPLING STATS is presented below. Let’s take an overview of the steps in that program: We seek to find the probability of randomly selecting four girls out of five children in a classroom of fifty children with equal numbers of boys and girls. We simulate the class by creating an array A with numbers “1” through “50” in order, designating “1” through “25” as girls and “26” through “50” boys. We then REPEAT the following steps a thousand times. We first SHUFFLE the elements of Vector A into Vector B to randomize the order of the children. We next TAKE the first five children in this randomly-ordered array B and place them in C. This simulates the random selection of five children. Then we COUNT the number of girls—numbers “1” to “25”—in C. Next, we SCORE the number of girls selected for each random sample (D) into z. We need not concern ourselves with whether one boy was chosen, because we know that if four (and only four) girls are chosen, one boy must have been selected as well. After the thousand samples are completed, we determine how many of the random samples of five children contained four girls by COUNTing the number of times four girls were selected. Finally, we PRINT the result. Note: The file “fourgirl” on the Resampling Stats software disk contains this set of commands. We can also find the probabilities of other outcomes from a histogram of trial results obtained with the following command: In the resulting histogram we can see that in 14 percent of the trials, 4 of the 5 selected were girls. It should be noted that for this problem—as for most other problems—there are several other resampling procedures that will also do the job correctly. In analytic probability theory this problem is worked with a formula for “combinations.” 11.5 Example 8-2: Nine Spades and Four Clubs in a Bridge Hand (Multiple-Outcome Sampling Without Replacement, Order Does not Matter) This problem is similar to Example 8-1 except that now there are four equally-likely outcomes instead of only two. The RESAMPLING STATS solution is straightforward in the program “9spades.” Note: The file “9spades” on the Resampling Stats software disk contains this set of commands. 11.6 Example 8-3: A Total of Fifteen Points in a Bridge Hand When Ace = 4, King = 3, Queen = 2, and Jack = 1. This problem bears the same relationship to Example 8-2 that Example 4-5 bears to Example 4-4. From this histogram, we see that in about 5 percent of our trials we obtained a total of exactly 15 points. RESAMPLING STATS will calculate this for us directly if we add the following commands on to the program: Note: The file “bridge” on the Resampling Stats software disk contains this set of commands. 11.7 Example 8-4: Four Girls and Then One Boy From Twenty-five Girls and Twenty-five Boys Order Matters, Sampling Without Replacement, Two Outcomes, Several of Each Item What is the probability of getting an ordered series of four girls and then one boy , from a universe of twenty-five girls and twenty-five boys? This illustrates Case 3 above. Clearly we can use the same sampling mechanism as in Example 8-1, but now we record “yes” for a smaller number of composite events. We record “no” if only one boy is chosen but he is chosen 1st, 2nd, 3rd, or 4th, whereas in Example 8-1 such outcomes are recorded as “yeses.” Step 1. Let “01-25” = girls, “26-50” = boys. Step 2. Select five non-duplicating numbers. Check whether the series includes four numbers “01-25” followed by a number “26-50.” If so, write “yes,” otherwise “no.” Step 3. Repeat step 2 perhaps 1000 times, and count the proportion “yes” which estimates the probability sought. The RESAMPLING STATS program “4girlboy” starts out similarly to the earlier problem “Fourgirls.” As in that problem, an array simulating a class of 50 children is created. But whereas in the previous problem any 4 of the first 5 children had to be girls, in this problem, the first 4 children must be girls, and then the fifth must be a boy for a trial to be considered a success. We thus TAKE the first 4 randomly-chosen children and place them in c. We must then determine the number of these children which are girls by counting those that have a number between “1” and “25,” and placing the result in d. If we find that all 4 of the children selected are girls, then we proceed to pick a fifth child and put it in e. If, however, of the first 4 children selected all were not girls, then we skip the steps through the first END statement, because we would already know that this trial will not be a “success.” To determine whether this fifth child selected is a boy—that is, checking whether it has a number between “26” and “50”—we use a COUNT statement. If it is a boy, then f will equal “1”; if it is not a boy, then f will equal “0.” (Remember that if the first 4 children selected were not all girls, then f will not get any value.) We then SCORE the value in f to e. This ends the conditional statement and the loop. NUMBERS 1,50 a Constitute the set of girls (numbers 1-25) and boys (numbers 26-50), put them in a. REPEAT 1000 Do the following experimental trial 1000 times. SHUFFLE a b Shuffle the numbers, put the shuffled numbers in b. TAKE b 1,4 c Take the first 4 numbers, put them in c. COUNT c between 1 25 d Count the number of girls, put the result in d. IF d = 4 If we have 4 girls... TAKE b 5 e Take the fifth number (not 5 numbers) from b and put it in e. COUNT e between 26 50 f How many boys in e? (It would have to be either 0 or 1) SCORE f z Keep track of each trial result. END END End the IF condition End the experiment, go back and repeat until all 1000 are complete. COUNT z =1 k Count the number of trials in which we got 1 boy (recall that we only counted the boy if we had already gotten the required 4 girls). DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the result. Note: The file “4girlboy” on the Resampling Stats software disk contains this set of commands. This type of problem is conventionally done with a permutation formula. 11.8 Example 8-5: Four or More Couples Getting Their Own Partners When Ten Couples are Paired Randomly (Probability of Matching by Chance) (Program “Couples”) Ten couples of boys and girls come to a party. In order to get them acquainted with each other, the host pairs them at random for the first dance by picking a boy’s name from one hat and a girl’s name from another. What is the chance that four or more couples will get the partners they came with? By now the solution seems obvious to you: Simulate the drawings from the two hats filled with boys’ and girls’ names, and see how often four or more matches occur. Step 1. Let “ace” through “10” of hearts be girls, “ace” through “10” of spades be boys. Step 2. Shuffle the hearts and deal them out in a row; shuffle the spades and deal in a row just below the hearts. Step 3. Count the pairs—a pair is one card from the heart row and one card from the spade row—that contain the same denomination. If 4 or more pairs match, record “yes,” otherwise “no.” Step 4. Repeat steps (2) and (3) perhaps 200 times. Step 5. Count the proportion “yes.” This estimates the probability of 4 or more pairs. Exercise for the student: Write the steps to do this example with random numbers. Now let’s examine the RESAMPLING STATS solution “Couples.” The key step is to fill array A with 10 elements numbered “1” to “10” in order, with each number standing for a male. We fill array B in similar fashion to represent the females. In each of the 1000 samples we SHUFFLE the elements of B, which stands for the females, to create a new vector C. As long as either the males or the females are randomly ordered, the probability of getting a correct match is determined by chance. There are several ways to determine whether a match occurs. Our method is using the SUBTRACT command to compare each element of A with each element of C. A match causes a “0,” this would mean that the third couple was matched correctly because they have the same numbers—“3”s. We then COUNT the number of “0”s in D to determine the number of couples correctly matched in the current sample. The result for the current sample is placed in E, then transferred to Z in order to keep SCORE for each of the 1000 samples. This ends the loop. NUMBERS 1,10 a An array of 10 males. NUMBERS 1,10 b An identical array of 10 females—the pair for each of the males. REPEAT 1000 Do the experiment 1000 times. SHUFFLE b c Shuffle the females. END SUBTRACT a c d This operation pairs each shuffled female with a male and subtracts. If it is an original pairing (1/1, 2/2, etc.), the result will be a 0. The number of 0’s indicates how many of the 10 males got paired up again with their original partner. COUNT d =0 e Count the number of 0’s and put the result in e. SCORE e z Keep track of each trial result. End the trial, go back and repeat until all 1000 are complete. HISTOGRAM z Produce a histogram of the results. From this histogram, we see that in about 3 percent of the trials did 4 or more couples end up being re-paired with their own partners. We can calculate this proportion directly with RESAMPLING STATS: COUNT z &gt;= 4 k Determine how many trials had 4 or more males being matched with their partner after the shuffling. (Note that this is the same as the number of females being matched with their original partner.) DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the result. Note: The file “couples” on the Resampling Stats software disk contains this set of commands. 11.9 Example 8-6: Matching Hats: Another famous problem of this sort: The hat-checker at a restaurant mixes up the hats of a party of 6 men. What is the probability that at least one will get his own hat? Instead of matching men with women, as in the earlier problem, however, we are now matching men with their hats. See the program “Hats” for the solution. First, assign each of the 6 men a number, and place these numbers in A. Next, assign each man’s hat the same number in B, but arrange them in random order by shuffling the numbers from B into C, which represents the group of mixed-up hats. The rest of the problem is the same as in “Couples” except that in the second COUNT statement, we now are interested in any trial where at least one (&gt;= 1) man received the right hat. NUMBERS 1,6 a Constitute the set of six men. NUMBERS 1,6 b Constitute the set of their six hats. REPEAT 1000 Do 1000 trials. SHUFFLE b c Mix up the hats. SUBTRACT a c d Subtract the shuffled hats from the set of men. A “0” will indicate that a man has received his own hat. COUNT d =0 e Count the number of “0’s”—the number of men who received their own hats back. SCORE e z Keep track of each trial result. END End one experiment, go back and repeat until all 1000 are complete. HISTOGRAM z Produce a histogram of the trial results. From the histogram, we see that in roughly 62 percent of the trial results at least one man received his own hat back. RESAMPLING STATS will calculate this for us with the following commands: COUNT z &gt;= 1 k Determine how many trials resulted in at least one man getting his hat back. DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the results Note: The file “hats” on the Resampling Stats software disk contains this set of commands. 11.10 Example 8-7: Twenty executives are to be assigned to two divisions of a firm The top manager wants to spread the talent reasonably evenly, but she does not want to label particular executives with a quality rating and therefore considers distributing them with a random selection. She therefore wonders: What are probabilities of the best ten among the twenty being split among the divisions in the ratios 5 and 5, 4 and 6, 3 and 7, etc. if their names are drawn from a hat? One might imagine much the same sort of problem in choosing two teams for a football or baseball contest. One may proceed as follows: Put 10 balls labeled “W” (for “worst”) and 10 balls labeled “B” (best) in a bucket. Draw 10 balls without replacement and count the W’s. Repeat (say) 400 times. Count the number of times each split—5 W’s and 5 B’s, 4 and 6, etc.—appears in the results. The problem can be done with RESAMPLING STATS as follows: URN 10#1 10#2 a REPEAT 1000 SHUFFLE a b TAKE b 1,10 c COUNT c =1 d SCORE d z END HISTOGRAM z Example 8-8: Executives Moving A major retail chain moves its store managers from city to city every three years in order to calculate individuals’ knowledge and experience. To make the procedure seem fair, the new locations are drawn at random. Nevertheless, the movement is not popular with managers’ families. Therefore, to make the system a bit sporting and to give people some hope of remaining in the same location, the chain allows managers to draw in the lottery the same posts they are now in. What are the probabilities that 1, 2, 3... will get their present posts again if the number of managers is 30? The problem can be solved with the following steps: Number a set of green balls from “1” to “30” and put them into Bucket A randomly. Number a set of red balls from “1” to “30” and then put into Bucket B. For greater concreteness one could use 30 little numbered dolls in Bucket A and 30 little toy houses in Bucket B. Shuffle Bucket A, and array all its green balls into a row (vector A). Array all the red balls from Bucket B into a second row B just below row A. Count how many green balls in row A have the same numbers as the red balls just below them, and record that number on a scoreboard. Repeat steps 2 and 3 perhaps 1000 times. Then count in the scoreboard the numbers of “0,” “1,” “2,” “3.” 11.11 Example 8-9: State Liquor Systems Again Let’s end this chapter with the example of state liquor systems that was examined in the previous chapter and which will be discussed again later in the context of problems in statistics. Remember that as of 1963, there were 26 U.S. states in whose liquor systems the retail liquor stores are privately owned, and 16 “monopoly” states where the state government owns the retail liquor stores. These were the 1961 prices: 16 monopoly states: $4.65, $4.55, $4.11, $4.15, $4.20, $4.55, $3.80, $4.00, $4.19, $4.75, $4.74, $4.50, $4.10, $4.00, $5.05, $4.20 Mean: $4.35 26 private-ownership states: $4.82, $5.29, $4.89, $4.95, $4.55, $4.90, $5.25, $5.30, $4.29, $4.85, $4.54, $4.75, $4.85, $4.85, $4.50, $4.75, $4.79, $4.85, $4.79, $4.95, $4.95, $4.75, $5.20, $5.10, $4.80, $4.29. Mean: $4.84 Let us now consider that all these states’ prices constitute one single finite universe. We ask: If these 42 states constitute a universe, and if they are all shuffled together, how likely is it that if one divides them into two samples at random (sampling without replacement), containing 16 and 26 observations respectively, the difference in mean prices turns out to be as great as $.49 (the difference that was actually observed)? Again we write each of the forty-two observed state prices on a separate card. The shuffled deck simulates a situation in which each state has an equal chance for each price. Repeatedly deal groups of 16 and 26 cards, without replacing the cards as they are chosen, to simulate hypothetical monopoly-state and private-state samples. In each trial calculate the difference in mean prices. The steps more systematically: Step A. Write each of the 42 prices on a card and shuffle. Steps B and C (combined in this case). i) Draw cards randomly without replacement into groups of 16 and 26 cards. Then ii) calculate the mean price difference between the groups, and iii) compare the simulation-trial difference to the observed mean difference of $4.84 – $4.35 = $.49; if it is as great or greater than $.49, write “yes,” otherwise “no.” Step D. Repeat step B-C a hundred or a thousand times. Calculate the proportion “yes,” which estimates the probability we seek. The probability that the postulated universe would produce a difference between groups as large or larger than observed in 1961 is estimated by how frequently the mean of the group of randomly-chosen sixteen prices from the simulated stateownership universe is less than (or equal to) the mean of the actual sixteen state-ownership prices. Please notice how the only difference between this treatment of the problem and the treatment in Chapter 7 is that the drawing in this case is without replacement whereas in Chapter 7 the drawing is with replacement. In Chapter 7 we thought of these states as if they came from a non-finite universe, which is one possible interpretation in one context. But one can also reasonably think about them in another context—as if they constitute the entire universe (aside from those states excluded from the analysis because of data complexities). If so, one can ask: If these 42 states constitute a universe, how likely is it that one would choose two samples at random, containing 16 and 26 observations, that would have prices as different as $.49 (the difference that was actually observed)? 11.12 Example 8-10: A Compound Problem: Five or More Spades in One Bridge Hand, and Four Girls and a Boy in a Five-Child Family “Compound” does not necessarily mean “complicated.” It means that the problem is a compound of two or more simpler problems. A natural way to handle such a compound problem is in stages, as we saw in the archery problem. If a “success” is achieved in the first stage, go on to the second stage; if not, don’t go on. More specifically in this example: Step 1. Use a bridge card deck, and five coins with heads = “girl.” Step 2. Deal a 13-card bridge hand and count the spades. If 5 or more spades, record “no” and end the experimental trial. Otherwise, continue to step 3. Step 3. Throw five coins, and count “heads.” If four heads, record “yes,” otherwise record “no.” Step 4. Repeat steps 2 and 3 a thousand times. Step 5. Compute the proportion of “yes” in step 3. This estimates the probability sought. The RESAMPLING STATS solution to “Compound” is neither long nor difficult. We tackle it almost as if the two parts of the problem were to be dealt with separately. We first determine, in a random bridge hand, whether 5 spades or more are dealt, as was done in the problem “Spades.” Then, IF 5 or more spades are found, we proceed to GENERATE a random family of 5 children. This means that we need not GENERATE families if 5 or more spades were not dealt to the bridge hand, because a “success” is only recorded if both conditions are met. After we SCORE the number of girls in each sample of 5 children, we need only END the loop and COUNT the number of samples that had 4 girls. Since we only drew samples of children for those trials in which a bridge hand of 5 spades had already been dealt, we will have in K the number of trials out of 1000 in which both conditions were met. URN 13#1 39#0 deck Deck with 13 spades (“1”) and 39 other cards (0) REPEAT 1000 Do the following experiment 1000 times. SHUFFLE deck deck$ Shuffle the deck. TAKE b deck$ 1,13 hand Deal out one hand of 13 cards. COUNT hand =1 spades Find out how many of the 13 cards are spades (spades are represented by the number 1). IF spades &gt;= 5 If we have 5 or more spades... GENERATE 5 1,2 a Create a family of 5 children, randomly selecting among girls (1’s) and boys (2’s). COUNT a =1 j Count the number of girls. SCORE j z Keep track of the number of girls in each trial. Recall that we are only counting girls if we have already gotten five spades in the bridge hand. END END End the IF condition. End the experiment, go back and repeat until all 1000 trials are complete. COUNT z =4 k Count the number of times we got 4 girls (and 5 or more spades). DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the results. Result: kk = .01 Note: The file “compound” on the Resampling Stats software disk contains this set of commands. 11.13 Summary This completes the discussion of problems in probability—that is, problems where we assume that the structure is known. Whereas Chapter 7 dealt with samples drawn from universes considered not finite , this chapter deals with problems drawn from finite universes and therefore you sample without replacement . "],["on-variability-in-sampling.html", "12 On Variability in Sampling 12.1 Variability and small samples 12.2 Regression to the mean 12.3 Summary and conclusion", " 12 On Variability in Sampling \\[Debra said\\]: “I’ve had such good luck with Japanese cars and poor luck with American...” The ’65 Ford Mustang: “It was fun, but I had to put two new transmissions in it.” The Ford Torino: “That got two transmissions too. That finished me with Ford.” The Plymouth Horizon: “The disaster of all disasters. That should’ve been painted bright yellow. What a lemon.” ( Washington Post Magazine , May 17, 1992, p. 19) Does the headnote above convince you that Japanese cars are better than American? Has Debra got enough evidence to reach the conclusion she now holds? That sort of question, and the reasoning we use to address it , is the subject of this chapter. More generally, how should one go about using the available data to test the hypothesis that Japanese cars are better? That is an example of the questions that are the subject of statistics. 12.1 Variability and small samples Perhaps the most important idea for sound statistical inference—the section of the book we are now beginning, in contrast to problems in probability, which we have studied in the previous chapters—is recognition of the presence of variability in the results of small samples . The fatal error of relying on too-small samples is all too common among economic forecasters, journalists, and others who deal with trends and public opinion. Athletes, sports coaches, sportswriters, and fans too frequently disregard this principle both in their decisions and in their discussion. Our intuitions often carry us far astray when the results vary from situation to situation—that is, when there is variability in outcomes—and when we have only a small sample of outcomes to look at. To motivate the discussion, I’ll tell you something that almost no American sports fan will believe: There is no such thing as a slump in baseball batting. That is, a batter often goes an alarming number of at-bats without getting a hit, and everyone—the manager, the sportswriters, and the batter himself— assumes that something has changed, and the probability of the batter getting a hit is now lower than it was before the slump. It is common for the manager to replace the player for a while, and for the player and coaches to change the player’s hitting style so as to remedy the defect. But the chance of a given batter getting a hit is just the same after he has gone many at-bats without a hit as when he has been hitting well. A belief in slumps causes managers to play line-ups which may not be their best. By “slump” I mean that a player’s probability of getting a hit in a given at-bat is lower during a period than during average periods. And when I say there is no such thing as a slump, I mean that the chances of getting a hit after any sequence of at-bats without a hit is not different than the long-run average. The “hot hand” in basketball is another illusion. The hot hand does not exist! The chance of a shooter scoring is the same after he has just missed a flock of shots as when he has just sunk a long string. That is, the chance of scoring a basket is no higher after a run of successes than after a run of failures. But even professional teams choose plays on the basis of who supposedly has a hot hand. Managers who substitute for the “slumping” or “cold-handed” players with other players who, in the long run, have lower batting averages, or set up plays for the shooter who supposedly has a hot hand, make a mistake. The supposed hot hand in basketball, and the slump in baseball, are illusions because the observed long runs of outs, or of baskets, are statistical artifacts, due to ordinary random variability. The identification of slumps and hot hands is superstitious behavior, classic cases of the assignment of pattern to a series of events when there really is no pattern. How do statisticians ascertain that slumps and hot hands do not exist? In brief, in baseball we simulate a hitter with a given average—say .250—and compare the results with actual hit- ters of that average, to see whether they have “slumps” longer than the computer. The method of investigation is roughly as follows. You program a computer or other machine to behave the way a player would, given the player’s long-run average, on the assumption that each trial is a random drawing. For example, if a player has a .250 season-long batting average, the machine is programmed like a bucket containing three black balls and one white ball. Then for each simulated at bat, the machine shuffles the “balls” and draws one; it then records whether the result is black or white, after which the ball is replaced in the bucket. To study a season with four hundred at-bats, a simulated ball is drawn four hundred times. The records of the player’s real season and the simulated season are then compared. If there really is such a thing as a non-random slump or streak, there will be fewer but longer “runs” of hits or outs in the real record than in the simulated record. On the other hand, if performance is independent from at-bat trial to at-bat trial, the actual record will change from hit to out and from out to hit as often as does the random simulated record. I suggested this sort of test for the existence of slumps in my 1969 book that first set forth the resampling method, a predecessor of this book. For example, Table 9-1 shows the results of one 400 at-bat season for a simulated .250 hitter. (1 = hit, 0 = out, sequential atbats ordered vertically) Note the “slump”—1 for 24—in columns 7 &amp; 8 (in bold). Table 9-1 A Rookie Season (400 at-bats) 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 Harry Roberts investigated the batting records of a sample of major leaguers. He compared players’ season-long records against the behavior of random-number drawings. If slumps existed rather than being a fiction of the imagination, the real players’ records would shift from a string of hits to a string of outs less frequently than would the random-number sequences. But in fact the number of shifts, and the average lengths of strings of hits and outs, are on average the same for players as for player-simulating random-number devices. Over long periods, averages may vary systematically, as Ty Cobb’s annual batting averages varied non-randomly from season to season, Roberts found. But in the short run, most individual and team performances have shown results similar to the outcomes that a lottery-type random number machine would produce. Thomas Gilovich, Robert Vallone and Amos Twersky performed a similar study of basketball shooting. They examined the records of shots from the floor by the Philadelphia 76’ers, foul shots by the Boston Celtics, and a shooting experiment of Cornell University teams. They found that “basketball players and fans alike tend to believe that a player’s chance of hitting a shot are greater following a hit than following a miss on the previous shot. However, detailed analyses…provided no evidence for a positive correlation between the outcomes of successive shots.” To put their conclusion differently, knowing whether a shooter has scored or not scored on the previous shot—or in any previous sequence of shots—is of absolutely no use in predicting whether the shooter will or will not score on the next shot. Similarly, knowledge of the past series of at-bats in baseball does not improve a prediction of whether a batter will get a hit this time. Of course a batter feels —and intensely—as if she or he has a better chance of getting a hit at some times than at other times. After a series of successful at-bats, both sandlot players and professionals feel confident that this time will be a hit, too. And after you have hit a bunch of baskets from all over the court, you feel as if you can’t miss. But notice that cardplayers get the same poignant feeling of being “hot” or “cold,” too. After a poker player “fills” several straights and flushes in a row, s/he feels s/he will hit the next one too. (Of course there are some players who feel just the opposite, that the “law of averages” is about to catch up with them.) You will agree, I’m sure, that the cards don’t have any memory, and a player’s chance of filling a straight or flush remains the same no matter how he or she has done in the last series of hands. Clearly, then, a person can have a strong feeling that something is about to happen even when that feeling has no foundation. This supports the idea that even though a player in sports “feels” that s/he is in a slump or has a hot hand, this does not imply that the feeling has any basis in reality. Why, when a batter is low in his/her mind because s/he has been making a lot of outs or for personal reasons, does her/ his batting not suffer? And why the opposite? Apparently at any given moment there are many influences operating upon a player’s performance in a variety of directions, with none of them clearly dominant. Hence there is no simple convincing explanation why a player gets a hit or an out, a basket or a miss, on any given attempt. But though science cannot provide an explanation, the sports commentators always are ready to offer their analyses. Listen, for example, to how they tell you that Joe Zilch must have been trying extra hard just because of his slump. There is a sportswriter’s explanation for anything that happens. Why do we believe the nonsense we hear about “momentum,” “comeback,” “she’s due this time,” and so on? The adult of the human species has a powerful propensity to believe that he or she can find a pattern even when there is no pattern to be found. Two decades ago I cooked up series of numbers with a random-number machine that looked as if they were prices on the stock market. Subjects in the experiment were told to buy and sell whichever stocks they chose. Then I gave them “another day’s prices,” and asked them to buy and sell again. The subjects did all kinds of fancy figuring, using an incredible variety of assumptions—even though there was no way for the figuring to help them. That is, people sought patterns even though there was no reason to believe that there were any patterns to be found. When I stopped the game before the ten buy-and-sell sessions the participants expected, people asked that the game continue. Then I would tell them that there was no basis for any patterns in the data. “Winning” or “losing” had no meaning. But the subjects demanded to continue anyway. They continued believing that they could find patterns even after I told them that the numbers were randomly looked up and not real stock prices. The illusions in our thinking about sports have important counterparts in our thinking about such real-world phenomena as the climate, the stock market, and trends in the prices of raw materials such as mercury, copper and wheat. And private and public decisions made on the basis of faulty understanding of these real situations, caused by illusory thinking on the order of belief in slumps and hot hands, are often costly and sometimes disastrous. An example of the belief that there are patterns when there are none: Systems for finding patterns in the stock market are peddled that have about the same reliability as advice from a racetrack tout—and millions buy them. One of the scientific strands leading into research on variability was the body of studies that considers the behavior of stock prices as a “random walk.” That body of work asserts that a stock broker or chartist who claims to be able to find patterns in past price movements of stocks that will predict future movements should be listened to with about the same credulity as a racetrack tout or an astrologer. A second strand was the work in psychology in the last decade or two which has recognized that people’s estimates of uncertain events are systematically biased in a variety of interesting and knowable ways. The U.S. government has made—and continues to make— blunders costing the public scores of billions of dollars, using slump-type fallacious reasoning about resources and energy. Forecasts are issued and policies are adopted based on the belief that a short-term increase in price constitutes a long-term trend. But the “experts” employed by the government to make such forecasts do no better on average than do private forecasters, and often the system of forecasting that they use is much more misleading than would be a random-number generating machine of the sort used in the baseball slump experiments. Please look at the data in Figure 9-2 for the height of the Nile River over about half a century. Is it not natural to think that those data show a decline in the height of the river? One can imagine that if our modern communication technology existed then, the Cairo newspapers would have been calling for research to be done on the fall of the Nile, and the television anchors would have been warning the people to change their ways and use less water. 1250 Height (cm) 1200 1150 1100 1050 810 A.D. 820 830 840 850 Year Figure 9-2: Height of the Nile River Over Half of a Century Let’s look at Figure 9-3 which represents the data over an even longer period. What now would you say about the height of the Nile? Clearly the “threat” was non-existent, and only appeared threatening because the time span represented by the data was too short. The point of this display is that looking at too-short a segment of experience frequently leads us into error. And “too short” may be as long as a century. Figure 9-3: Height of the Nile River for an Extended Period of Time Another example is the price of mercury, which is representative of all metals. Figure 9-4 shows a forecast made in 1976 by natural-scientist Earl Cook. He combined a then-recent upturn in prices with the notion that there is a finite amount of mercury on the earth’s surface, plus the mathematical charm of plotting a second-degree polynomial with the computer. Figures 9-5a and 9-5b show how the forecast was almost immediately falsified, and the price continued its long-run decline. Figure 9-4: The Price of Mercury, Cook, Earl, “Limits to Exploitation of Non-Renewable Resources,” Science , 191, 20 Feb 1976, pp. 677-682 250,000 200,000 Metric tons 150,000 100,000 after 1979 reserve base change 50,000 0 1950 1955 1960 1965 1970 1975 1980 1985 1990 Year 45 40 35 30 Years 25 20 15 10 5 after 1979 reserve base change 0 1950 1955 1960 1965 1970 1975 1980 1985 1990 Year Figure 9-5a: Mercury Reserves, 1950-1990 120 100 Index for wages 80 Divided by wages Divided by CPI 175 150 125 Index for CPI 60 100 75 40 50 20 25 0 0 1850 1870 1890 1910 1930 1950 1970 1990 Figure 9-5b: Mercury Price Indexes, 1950-1990 Lack of sound statistical intuition about variability can lead to manipulation of the public being by unscrupulous persons. Commodity funds sellers use a device of this sort to make their results look good ( The Washington Post , Sep 28, 1987, p. 71). Some individual commodity traders inevitably do well in their private trading, just by chance. A firm then hires one of them, builds a public fund around him, and claims the private record for the fund’s own history. But of course the private record has no predictive power, any more than does the record of someone who happened to get ten heads in a row flipping coins. How can we avoid falling into such traps? It is best to look at the longest possible sweep of history. That is, use the largest possible sample of observations to avoid sampling error. For copper we have data going back to the 18th century B.C. In Babylonia, over a period of 1000 years, the price of iron fell to one fifth of what it was under Hammurabi (almost 4000 years ago), and the price of copper then cost about a thousand times its current price in the U.S., relative to wages. So the inevitable short-run increases in price should be considered in this long-run context to avoid drawing unsound conclusions due to small-sample variability. Proof that it is sound judgment to rely on the longest possible series is given by the accuracy of predictions one would have made in the past. In the context of copper, mercury, and other raw materials, we can refer to a sample of years in the past, and from those years imagine ourselves forecasting the following year. If you had bet every time that prices would go down in consonance with the long-run trend, you would have been a big winner on average. 12.2 Regression to the mean UP, DOWN “The Dodgers demoted last year’s NL rookie of the year, OF Todd Hollandsworth (.237, 1 HR, 18 RBI) to AAA Albuquerque...” (Item in Washington Post , 6/14/97) It is a well-known fact that the Rookie of the Year in a sport such as baseball seldom has as outstanding a season in her/his sophomore year. Why is this so? Let’s use the knowledge we have acquired of probability and simulation to explain this phenomenon. The matter at hand might be thought of as a problem in pure probability—if one simply asks about the chance that a given player (the Rookie of the Year) will repeat. Or it could be considered a problem in statistics, as discussed in coming chapters. Let’s consider the matter in the context of baseball. Imagine 10 mechanical “ball players,” each a machine that has three white balls (hits) and 7 black balls. Every time the machine goes to bat, you take a ball out of the machine, look to see if it is a hit or an out, and put it back. For each “ball player” you do this 100 times. One of them is going to do better than the others, and that one becomes the Rookie of the Year. (See Table 9-2.) Table 9-2 Rookie Seasons (100 at bats) # of Hits Batting Average 32 .320 34 .340 33 .330 30 .300 35 .350 33 .330 30 .300 31 .310 28 .280 25 .250 Would you now expect that the player who happened to be the best among the top ten in the first year to again be the best among the top ten in the next year, also? The sports writers do. But of course this seldom happens. The Rookie of the Year in major-league baseball seldom has as outstanding a season in his sophomore year as in his rookie year. You can expect him to do better than the average of all sophomores, but not necessarily better than all of the rest of the group of talented players who are now sophomores. (Please notice that we are not saying that there is no long-run difference among the top ten rookies. But suppose there is. Table 9-3 shows the season’s performance for ten batters of differing performances). Table 9-3 Season’s Performance “TRUE” ROOKIE .270 .340 .270 .240 .280 .330 .280 .300 .300 .280 .300 .420 .320 .340 .320 .350 .330 .260 .330 .330 We see from Figure 9-3 that we have ten batters whose “true” batting averages range from .270 to .330. Their rookie year performance (400 at bats), simulated on the basis of their “true”average is on the right. Which one is the rookie of the year? It’s #6, who hit .420 during the rookie session. Will he do as well next year? Not likely—his “true” average is only .300. Try generating some rookie “seasons” yourself with the following commands, ranging the batter’s “true” performance by altering what you count as a hit. GENERATE 400 1,100 at-bats COUNT at-bats &lt;= 30 hits DIVIDE hits 400 average Simulate a set of 10 or 20 such rookie seasons, and look at the one who did best. How did his rookie season compare to his “true” average? The explanation is the presence of variability . And lack of recognition of the role of variability is at the heart of much fallacious reasoning. Being alert to the role of variability is crucial. Or consider the example of having a superb meal at a restaurant—the best meal you have ever eaten. That fantastic meal is almost surely the combination of the restaurant being better than average, plus a lucky night for the chef and the dish you ordered. The next time you return you can expect a meal better than average, because the restaurant is better than average in the long run. But the meal probably will be less good than the superb one you had the first time, because there is no reason to believe that the chef will get so lucky again and that the same sort of variability will happen this time. These examples illustrate the concept of “regression to the mean”—a confusingly-titled and very subtle effect caused by variability in results among successive samples drawn from the same population. This phenomenon was given its title more than a century ago by Francis Galton, one of the great founders of modern statistics, when at first he thought that the height of the human species was becoming more uniform, after he noticed that the children of the tallest and shortest parents usually are closer to the average of all people than their parents are. But later he discovered his fallacy—that the variability in heights of children of quite short and quite tall parents also causes some people to be even more exceptionally tall or short than their parents. So the spread in heights among humans remains much the same from generation to generation; there is no “regression to the mean.” The heart of the matter is that any exceptional observed case in a group is likely to be the result of two forces—a) an underlying propensity to differ from the average in one direction or the other, plus b) some chance sampling variability that happens (in the observed case) to push even further in the exceptional direction. A similar phenomenon arises in direct-mail marketing. When a firm tests many small samples of many lists of names and then focuses its mass mailings on the lists that performed best in the tests, the full list “rollouts” usually do not perform as well as the samples did in the initial tests. It took many years before mail-order experts (see especially Burnett, Ed) finally understood that regression to the mean inevitably causes an important part of the dropoff from sample to rollout observed in the set of lists that give the very best results in a multi-list test. The larger the test samples, the less the dropoff, of course, because larger samples reduce variability in results. But larger samples risk more money. So the test-sample-size decision for the marketer inevitably is a trade-off between accuracy and cost. And one last amusing example: After I lectured to the class on this material, the student who had gotten the best grade on the first mid-term exam came up after class and said: “Does that mean that on the second mid-term I should expect to do well but not the best in the class?” And that’s exactly what happened: He had the second-best score in the class on the next midterm. A related problem arises when one conducts multiple tests, as when testing thousands of drugs for therapeutic value. Some of the drugs may appear to have a therapeutic effect just by chance. We will discuss this problem later when discussing hypothesis testing. 12.3 Summary and conclusion The heart of statistics is clear thinking. One of the key elements in being a clear thinker is to have a sound gut understanding of statistical processes and variability. This chapter amplifies this point. A great benefit to using simulations rather than formulas to deal with problems in probability and statistics is that the presence and importance of variability becomes manifest in the course of the simulation work. "],["the-procedures-of-monte-carlo-simulation-and-resampling.html", "13 The Procedures of Monte Carlo Simulation (and Resampling) 13.1 A definition and general procedure for Monte Carlo simulation 13.2 Summary", " 13 The Procedures of Monte Carlo Simulation (and Resampling) Until now, the steps to follow in solving particular problems have been chosen to fit the specific facts of that problem. And so they always must. Now let’s generalize what we have done in the previous chapters on probability into a general procedure for such problems, which will in turn become the basis for a detailed procedure for resampling simulation in statistics. The generalized procedure describes what we are doing when we estimate a probability using Monte Carlo simulation problem-solving operations. 13.1 A definition and general procedure for Monte Carlo simulation This is what we shall mean by the term Monte Carlo simulation when discussing problems in probability: Using the given data-generating mechanism (such as a coin or die) that is a model of the process you wish to understand, produce new samples of simulated data, and examine the results of those samples . That’s it in a nutshell. In some cases, it may also be appropriate to amplify this procedure with additional assumptions. This definition fits both problems in pure probability as well as problems in statistics, but in the latter case the process is called resampling . The reason that the same definition fits is that at the core of every problem in inferential statistics lies a problem in probability ; that is, the procedure for handling every statistics problem is the procedure for handling a problem in probability. (There is related discussion of definitions in Chapters 4 and 14.) The following series of steps should apply to all problems in probability. I’ll first state the procedure straight through without examples, and then show how it applies to individual examples. Step A. Construct a simulated “universe” of cards or dice or some other randomizing mechanism whose composition is similar to the universe whose behavior we wish to describe and investigate. The term “universe” refers to the system that is relevant for a single simple event. Step B. Specify the procedure that produces a pseudo-sample which simulates the real-life sample in which we are interested. That is, specify the procedural rules by which the sample is drawn from the simulated universe. These rules must correspond to the behavior of the real universe in which you are interested. To put it another way, the simulation procedure must produce simple experimental events with the same probabilities that the simple events have in the real world. Step C. If several simple events must be combined into a composite event, and if the composite event was not described in the procedure in step B, describe it now. Step D. Calculate the probability of interest from the tabulation of outcomes of the resampling trials. Now let us apply the general procedure to some examples to make it more concrete. Here are four problems to be used as illustrations: If on average 3 percent of the gizmos sent out are defective, what is the chance that there will be more than 10 defectives in a shipment of 200? What are the chances of getting three or more girls in the first four children, if the probability of a female birth is 106/ 206? What are the chances of Joe Hothand scoring 20 or fewer baskets in 57 shots if his long-run average is 47 percent? What is the probability of two or more people in a group of 25 persons having the same birthday—i. e., the same month and same day of the month? Step A. Construct a simulated “universe” of cards or dice or some other randomizing mechanism whose composition is similar to the universe whose behavior we wish to describe and investigate. The term “universe” refers to the system that is relevant for a single simple event. For example: A random drawing with replacement from the set of numbers 1...100 with 1...3 designated as defective simulates the system that produces 3 defective gizmos among 100. A coin with two sides, or a random drawing from two sets of random numbers “1-105” and “106-205,” simulates the system that produces a single male or female birth, when we are estimating the probability of three girls in the first four children. Notice that in this universe the probability of a girl remains the same from trial event to trial event—that is, the trials are independent—demonstrating a universe from which we sample with replacement. A random drawing with replacement from a bucket containing a hundred balls, 47 red and 53 black, simulates the system that produces 47 percent baskets for Joe Hothand. A random drawing with replacement from the numbers 1...365 simulates the system that produces a birthday. This step A includes two operations: Decide which symbols will stand for the elements of the universe you will simulate. Determine whether the sampling will be with or without replacement. (This can be ambiguous in a complex modeling situation.) Hard thinking is required in order to determine the appropriate “real” universe whose properties interest you. Step B. Specify the procedure that produces a pseudo-sample that simulates the real-life sample in which we are interested. That is, specify the procedural rules by which the sample is drawn from the simulated universe. These rules must correspond to the behavior of the real universe in which you are interested. To put it another way, the simulation procedure must produce simple experimental events with the same probabilities that the simple events have in the real world. For example: For a single gizmo, you can draw a single number from an infinite universe. Or one can use a finite set with replacement and shuffling, In the case of three or more daughters among four children, you can draw a card and then replace it if you are using a deck of red and black cards and you are assuming a female birth is 50-50. Or if you are using a random-numbers table, the random numbers automatically simulate replacement. Just as the chances of having a boy or a girl do not change depending on the sex of the preceding child, so we want to ensure through sampling with replacement that the chances do not change each time we choose from the deck of cards. In the case of Joe Hothand’s shooting, the procedure is to consider the numbers “1-47” as “baskets,” and “48-100” as “misses,” with the same other considerations as the gizmos. In the case of the birthday problem, the drawing obviously must be with replacement. Recording the outcome of the sampling must be indicated as part of this step, e.g., “record ‘yes’ if girl or basket, ‘no’ if a boy or a miss.” Step C. If several simple events must be combined into a composite event, and if the composite event was not described in the procedure in step B, describe it now. For example: For the gizmos, draw a sample of 200. For the three or more girls among four children, the procedure for each simple event of a single birth was described in step B. Now we must specify repeating the simple event four times, and counting whether the outcome is or is not three girls. In the case of Joe Hothand’s shots, we must draw 57 numbers to make up a sample of shots, and examine whether there are 20 or more misses. Recording the results as “ten or more defectives,” “three or more girls” or “two or less girls,” and “20 or more misses” or “19 or fewer,” is part of this step. This record indicates the results of all the trials and is the basis for a tabulation of the final result. Step D. Calculate the probability of interest from the tabulation of outcomes of the resampling trials. For example: the proportions of “yes” and “no,” and “20 or more” and “19 or fewer” estimate the probability we seek in step C. The above procedure is similar to the procedure followed with the analytic formulaic method except that the latter method constructs notation and manipulates it. 13.2 Summary This chapter describes more generally the specific steps used in prior chapters to solve problems in probability. "],["the-basic-ideas-in-statistical-inference.html", "14 The Basic Ideas in Statistical Inference 14.1 Knowledge without probabilistic statistical inference 14.2 The treatment of uncertainty 14.3 Where statistical inference becomes crucial 14.4 Conclusions 14.5 Endnotes", " 14 The Basic Ideas in Statistical Inference Probabilistic statistical inference is a crucial part of the process of informing ourselves about the world around us. Statistics and statistical inference help us understand our world and make sound decisions about how to act. More specifically, statistical inference is the process of drawing conclusions about populations or other collections of objects about which we have only partial knowledge from samples. Technically, inference may be defined as the selection of a probabilistic model to resemble the process you wish to investigate, investigation of that model’s behavior, and interpretion of the results. Fuller understanding of the nature of statistical inference comes with practice in handling a variety of problems. Until the 18th century, humanity’s extensive knowledge of nature and technology was not based on formal probabilistic statistical inference. But now that we have already dealt with many of the big questions that are easy to answer without probabilistic statistics, and now that we live in a more ramified world than in earlier centuries, the methods of inferential statistics become ever more important. Furthermore, statistical inference will surely become ever more important in the future as we voyage into realms that are increasingly difficult to comprehend. The development of an accurate chronometer to tell time on sea voyages became a crucial need when Europeans sought to travel to the New World. Similarly, probability and statistical inference become crucial as we voyage out into space and down into the depths of the ocean and the earth, as well as probe into the secrets of the microcosm and of the human mind and soul. Where probabilistic statistical inference is employed, the inferential procedures may well not be the crucial element. For example, the wording of the questions asked in a public-opinion poll may be more critical than the statistical-inferential procedures used to discern the reliability of the poll results. Yet we dare not disregard the role of the statistical procedures. 14.1 Knowledge without probabilistic statistical inference Let us distinguish two kinds of knowledge with which inference at large (that is, not just probabilistic statistical inference) is mainly concerned: a) one or more absolute measurements on one or more dimensions of a collection of one or more items— for example, your income, or the mean income of the people in your country; and b) comparative measurements and evaluations of two or more collections of items (especially whether they are equal or unequal)—for example, the mean income in Brazil compared to the mean income in Argentina. Types (a) and (b) both include asking whether there has been a change between one observation and another. What is the conceptual basis for gathering these types of knowledge about the world? I believe that our rock bottom conceptual tool is the assumption of what we may call sameness , or continuity , or constancy , or repetition , or equality , or persistence ; “constancy” and “continuity” will be the terms used most frequently here, and I shall use them interchangeably. Continuity is a non-statistical concept. It is a best guess about the next point beyond the known observations, without any idea of the accuracy of the estimate. It is like testing the ground ahead when walking in a marsh. It is local rather than global. We’ll talk a bit later about why continuity seems to be present in much of the world that we encounter. The other great concept in statistical inference, and perhaps in all inference taken together, is representative (usually random) sampling , to be discussed in Chapter 12. Representative sampling—which depends upon the assumption of sameness (homogeneity) throughout the universe to be investigated—is quite different than continuity; representative sampling assumes that there is no greater chance of a connection between any two elements that might be drawn into the sample than between any other two elements; the order of drawing is immaterial. In contrast, continuity assumes that there is a greater chance of connection between two contiguous elements than between either one of the elements and any of the many other elements that are not contiguous to either. Indeed, the process of randomizing is a device for doing away with continuity and autocorrelation within some bounded closed system—the sample “frame.” It is an attempt to map (describe) the entire area ahead using the device of the systematic survey. Random representative sampling enables us to make probabilistic inferences about a population based on the evidence of a sample. To return now to the concept of sameness: Examples of the principle are that we assume: a) our house will be in the same place tomorrow as today; b) a hammer will break an egg every time you hit the latter with the former (or even the former with the latter); c) if you observe that the first fifteen persons you see walking out of a door at the airport are male, the sixteenth probably will be male also; d) paths in the village stay much the same through a person’s life; e) religious ritual changes little through the decades; f) your best guess about tomorrow’s temperature or stock price is that will be the same as today’s. This principle of constancy is related to David Hume’s concept of constant conjunction . When my children were young, I would point to a tree on our lawn and ask: “Do you think that tree will be there tomorrow?” And when they would answer “Yes,” I’d ask, “Why doesn’t the tree fall?” That’s a tough question to answer. There are two reasonable bases for predicting that the tree will be standing tomorrow. First and most compelling for most of us is that almost all trees continue standing from day to day, and this particular one has never fallen; hence, what has been in the past is likely to continue. This assessment requires no scientific knowledge of trees, yet it is a very functional way to approach most questions concerning the trees—such as whether to hang a clothesline from it, or whether to worry that it will fall on the house tonight. That is, we can predict the outcome in this case with very high likelihood of being correct even though we do not utilize anything that would be called either science or statistical inference. (But what do you reply when your child says: “Why should I wear a seat belt? I’ve never been in an accident”?) A second possible basis for prediction that the tree will be standing is scientific analysis of the tree’s roots—how the tree’s weight is distributed, its sickness or health, and so on. Let’s put aside this sort of scientific-engineering analysis for now. The first basis for predicting that the tree will be standing tomorrow—sameness—is the most important heuristic device in all of knowledge-gathering. It is often a weak heuristic; certainly the prediction about the tree would be better grounded (!) after a skilled forester examines the tree. But persistence alone might be a better heuristic in a particular case than an engineering-scientific analysis alone. This heuristic appears more obvious if the child—or the adult— were to respond to the question about the tree with another question: Why should I expect it to fall ? In the absence of some reason to expect change, it is quite reasonable to expect no change. And the child’s new question does not duck the central question we have asked about the tree, any more than one ducks a probability estimate by estimating the complementary probability (that is, unity minus the probability sought); indeed, this is a very sound strategy in many situations. Constancy can refer to location, time, relationship to another variable, or yet another dimension. Constancy may also be cyclical. Some cyclical changes can be charted or mapped with relative certainty—for example the life-cycles of persons, plants, and animals; the diurnal cycle of dark and light; and the yearly cycle of seasons. The courses of some diseases can also be charted. Hence these kinds of knowledge have long been well known. Consider driving along a road. One can predict that the price of the next gasoline station will be within a few cents of the gasoline station that you just passed. But as you drive further and further, the dispersion increases as you cross state lines and taxes differ. This illustrates continuity. The attention to constancy can focus on a single event, such as leaves of similar shape appearing on the same plant. Or attention can focus on single sequences of “production,” as in the process by which a seed produces a tree. For example, let’s say you see two puppies—one that looks like a low-slung dachshund, and the other a huge mastiff. You also see two grown male dogs, also apparently dachshund and mastiff. If asked about the parentage of the small ones, you are likely— using the principle of sameness—to point—quickly and with surety—to the adult dogs of the same breed. (Here it is important to notice that this answer implicitly assumes that the fathers of the puppies are among these dogs. But the fathers might be somewhere else entirely; it is in these ways that the principle of sameness can lead you astray.) When applying the concept of sameness, the object of interest may be collections of data, as in Semmelweiss’s data on the consistent differences in rates of maternal deaths from childbed fever in two clinics with different conditions (see Table 11-1), or the similarities in sex ratios from year to year in Graunt’s data on London births (Table 11-2), or the stark effect in John Snow’s data on the numbers of cholera cases associated with two London wells (Table 11-3), or the reduction in beriberi among Japanese sailors as a result of a change in diet (Table 11-4). These data seem so overwhelmingly clear cut that our naive statistical sense makes the relationships seem deterministic, and the conclusions seems straightforward. (But the same statistical sense frequently misleads us when considering sports and stock market data.) Table 11-1 Deaths of Mothers First Clinic Second Clinic Births Deaths Rate Births Deaths Rate 1841 3,036 237 7.8 2,442 86 3.5 1842 3,287 518 15.8 2,659 202 7.6 1843 3,060 274 8.9 2,739 164 6.0 1844 3,157 260 8.2 2,956 68 2.3 1845 3,492 241 6.9 3,241 66 2.0 1845 4,010 459 11.4 3,754 105 2.8 Tota l Avg . 20,042 1,989 9.9 17,791 691 3.9 Source: Semmelweis, Ignaz, The Etiology, Concept, and Prophylaxis of Childbed Fever , Translated and edited by K. Codell Carter (Madison, Wisconsin: Univ. of Wisconsin Press, 1983), p. 64. Table 11-2 Ratio of Number of Males to Number of Females Period London Christenings 1629-1636 1,072 1637-1640 1,073 1641-1648 1,063 1649-1656 1,095 1657-1660 1,069 Source: Graunt, John, Natural and Political Observations Mentioned in a Following Index and Made Upon the Bills of Mortality (Reprint Edition) (New York; Arno Press, 1662/1975). Table 11-3 John Snow’s Data on Cholera Rates for Three Wells Southwark and Vauxhall Supply Lambeth Supply Rest of London 71 deaths per 10,000 houses 5 deaths per 10,000 houses 9 deaths per 10,000 houses Source: Winslow, Charles-Edward Amory, The Conquest of Epidemic Disease (Madison, Wisconsin: Univ. of Wisconsin Press, 1980), p. 276. Table 11-4 Takaki’s Japanese Naval Records of Deaths from Beriberi Year Diet Total Navy Personnel Deaths from Beriberi 1880 Rice diet 4,956 1,725 1881 Rice diet 4,641 1,165 1882 Rice diet 4,769 1,929 1883 Rice Diet 5,346 1,236 1884 Change to new diet 5,638 718 1885 New diet 6,918 41 1886 New diet 8,475 3 1887 New diet 9,106 0 1888 New diet 9,184 0 Source: K. Takaki, in Kornberg, 1989, p. 9 Constancy and sameness can be seen in macro structures; consider, for example, the constant location of your house. Constancy can also be seen in micro aggregations—for example, the raindrops and rain that account for the predictably fluctuating height of the Nile, or the ratio of boys to girls born in London, cases in which we can average to see the “statistical” sameness. The total sum of the raindrops produces the level of a reservoir or a river from year to year, and the sum of the behaviors of collections of persons causes the birth rates in the various years. Statistical inference is only needed when a person thinks that s/he might have found a pattern but the pattern is not completely obvious to all. Probabilistic inference works to test— either to confirm or discount—the belief in the pattern’s existence. We will see such cases in the following chapter. People have always been forced to think about and act in situations that have not been constant—that is, situations where the amount of variability in the phenomenon makes it impossible to draw clear cut, sensible conclusions. For example, the appearance of game animals in given places and at given times has always been uncertain to hunters, and therefore it has always been difficult to know which target to hunt in which place at what time. And of course variability of the weather has always made it a very uncertain element. The behavior of one’s enemies and friends has always been uncertain, too, though uncertain in a manner different from the behavior of wild animals; there often is a gaming element in interactions with other humans. But in earlier times, data and techniques did not exist to enable us to bring statistical inference to bear. 14.2 The treatment of uncertainty The purpose of statistical inference is to help us peer through the veil of variability when it obscures the main thrust of the data, so as to improve the decisions we make. Statistical inference (or in most cases, simply probabilistic estimation) can help a gambler deciding on the appropriate odds in a betting game when there seems to be little or no difference between two or more outcomes; b) an astronomer deciding upon one or another value as the central estimate for the location of a star when there is considerable variation in the observations s/he has made of the star; c) a basketball coach pondering whether to remove from the game her best shooter who has heretofore done poorly tonight; d) an oil-drilling firm debating whether to follow up a test-well drilling with a full-bore drilling when the probability of success is not overwhelming but the payoff to a gusher could be large. Returning to the tree near the Simon house: Let’s change the facts. Assume now that one major part of the tree is mostly dead, and we expect a big winter storm tonight. What is the danger that the tree will fall on the house? Should we spend $1500 to have the mostly-dead third of it cut down? We know that last year a good many trees fell on houses in the neighborhood during such a storm. We can gather some data on the proportion of old trees this size that fell on houses—about 5 in 100, so far as we can tell. Now it is no longer an open-and-shut case about whether the tree will be standing tomorrow, and we are using statistical inference to help us with our thinking. We proceed to find a set of trees that we consider similar to this one , and study the variation in the outcomes of such trees. So far we have estimated that the average for this group of trees—the mean (proportion) that fell in the last big storm—is 5 percent. Averages are much more “stable”—that is, more similar to each other— than are individual cases. Notice how we use the crucial concept of sameness: We assume that our tree is like the others we observed, or at least that it is not systematically different from most of them and it is more-or-less average. How would our thinking be different if our data were that one tree in 10 had fallen instead of 5 in 100? This is a question in statistical inference. How about if we investigate further and find that 4 of 40 elms fell, but only one of 60 oaks , and ours is an oak tree. Should we consider that oaks and elms have different chances of falling? Proceeding a bit further, we can think of the question as: Should we or should we not consider oaks and elms as different? This is the type of statistical inference called “hypothesis testing”: We apply statistical procedures to help us decide whether to treat the two classes of trees as the same or different. If we should consider them the same, our worries about the tree falling are greater than if we consider them different with respect to the chance of damage. Notice that statistical inference was not necessary for accurate prediction when I asked the kids about the likelihood of a live tree falling on a day when there would be no storm. So it is with most situations we encounter. But when the assumption of constancy becomes shaky for one reason or another, as with the sick tree falling in a storm, we need a more refined form of thinking. We collect data on a large number of instances, inquire into whether the instances in which we are interested (our tree and the chance of it falling) are representative—that is, whether it resembles what we would get if we drew a sample randomly—and we then investigate the behavior of this large class of instances to see what light it throws on the instances(s) in which we are interested. The procedure in this case—which we shall discuss in greater detail later on—is to ask: If oaks and elms are not different, how likely is it that only one of 60 oaks would fall whereas 4 of 40 elms would fall? Again, notice the assumption that our tree is “representative” of the other trees about which we have information—that it is not systematically different from most of them, but rather that it is more-or-less average. Our tree cer- tainly was not chosen randomly from the set of trees we are considering. But for purposes of our analysis, we proceed as if it had been chosen randomly—because we deem it “representative.” This is the first of two roles that the concept of randomness plays in statistical thinking. Here is an example of the second use of the concept of randomness: We conduct an experiment— plant elm and oak trees at randomly-selected locations on a plot of land, and then try to blow them down with a wind-making machine. (The random selection of planting spots is important because some locations on a plot of ground have different growing characteristics than do others.) Some purists object that only this sort of experimental sampling is a valid subject of statistical inference; it can never be appropriate, they say, to simply assume on the basis of other knowledge that the tree is representative. I regard that purist view as a helpful discipline on our thinking. But accepting its conclusion—that one should not apply statistical inference except to randomly-drawn or randomly-constituted samples—would take from us a tool that has proven useful in a variety of activities. As discussed earlier in this chapter, the data in some (probably most) scientific situations are so overwhelming that one can proceed without probabilistic inference. Historical examples include those shown above of Semmelweiss and puerperal fever, and John Snow and cholera. But where there was lack of overwhelming evidence, the causation of many diseases long remained unclear for lack of statistical procedures. This led to superstitious beliefs and counter-productive behavior, such as quarantines against plague often were. Some effective practices also arose despite the lack of sound theory, however—the waxed costumes of doctors, and the burning of mattresses, despite the wrong theory about the causation of plague; see Cipolla, 1981) So far I have spoken only of predictability and not of other elements of statistical knowledge such as understanding and control . This is simply because statistical correlation is the bed rock of most scientific understanding, and predictability. Later we will expand the discussion beyond predictability; it holds no sacred place here. 1 It is because hypothesis testing focuses on this most basic of inferential processes—deciding “same” or “different”—that I believe it to be a more basic technique than estimating confidence intervals, which focus on the accuracy of estimates. 14.3 Where statistical inference becomes crucial There was little role for statistical inference until about three centuries ago because there existed very few scientific data. When scientific data began to appear, the need emerged for statistical inference to improve the interpretation of the data. As we saw, statistical inference is not needed when the evidence is overwhelming. A thousand cholera cases at one well and zero at another obviously does not require a statistical test. Neither would 999 cases to one, or even 700 cases to 300, because our inbred and learned statistical senses can detect that the two situations are different. But probabilistic inference is needed when the number of cases is relatively small or where for other reasons the data are somewhat ambiguous. For example, when working with the 17th century data on births and deaths, John Graunt—great statistician though he was—drew wrong conclusions about some matters because he lacked modern knowledge of statistical inference. For example, he found that in the rural parish of Romsey “there were born 15 Females for 16 Males, whereas in London there were 13 for 14, which shows, that London is somewhat more apt to produce Males, then the country” (p. 71). He suggests that the “curious” inquire into the causes of this phenomenon, apparently not recognizing—and at that time he had no way to test—that the difference might be due solely to chance. He also notices (p. 94) that the variations in deaths among years in Romsey were greater than in London, and he attempted to explain this apparent fact (which is just a statistical artifact) rather than understanding that this is almost inevitable because Romsey is so much smaller than London. Because we have available to us the modern understanding of variability, we can now reach sound conclusions on these matters. Summary statistics—such as the simple mean—are devices for reducing a large mass of data (inevitably confusing unless they are absolutely clear cut) to something one can manage to understand. And probabilistic inference is a device for determining whether patterns should be considered as facts or artifacts. Here is another example that illustrates the state of early quantitative research in medicine: Exploring the effect of a common medicinal substance, Boecker examined the effect of sasparilla on the nitrogenous and other constituents of the urine. An individual receiving a controlled diet was given a decoction of sasparilla for a period of twelve days, and the volume of urine passed daily was carefully measured. For a further twelve days that same individual, on the same diet, was given only distilled water, and the daily quantity of urine was again determined. The first series of researches gave the following figures (in cubic centimeters): 1,467, 1,744, 1,665, 1,220, 1,161, 1,369, 1,675, 2,199, 887, 1,634, 943, and 2,093 (mean = 1,499); the second series: 1,263, 1,740, 1,538, 1,526, 1,387, 1,422, 1,754, 1,320, 1,809, 2,139, 1,574, and 1,114 (mean = 1,549). Much uncertainty surrounded the exactitude of these measurements, but this played little role in the ensuing discussion. The fundamental issue was not the quality of the experimental data but how inferences were drawn from those data (Coleman in Kruger, 1987, p. 207). The experimenter Boecker had no reliable way of judging whether the data for the two groups were or were not meaningfully different, and therefore he arrived at the unsound conclusion that there was indeed a difference. (Gustav Radicke used this example as the basis for early work on statistical significance.) Another example: Joseph Lister convinced the scientific world of the germ theory of infection, and the possibility of preventing death with a disinfectant, with these data: Prior to the use of antiseptics—16 post-operative deaths in 35 amputations; subsequent to the use of antiseptics—6 deaths in 40 amputations (Winslow, 1943, p. 303). But how sure could one be that a difference of that size might not occur just by chance? No one then could say, nor did anyone inquire, apparently. Here’s another example of great scientists falling into error because of a too-primitive approach to data (Feller, 3rd ed, 1968, pp. 69-70): Charles Darwin wanted to compare two sets of measured data, each containing 16 observations. At Darwin’s request, Francis Galton compared the two sets of data by ranking each, and then comparing them pairwise. The a’s were ahead 13 times. Without knowledge of the actual probabilities Galton concluded that the treatment was effective. But, assuming perfect randomness, the probability that the a’s beat \\[the others\\] 13 times or more equals 3/16. This means that in three out of sixteen cases a perfectly ineffectual treatment would appear as good or better than the treatment classified as effective by Galton. That is, Galton and Darwin reached an unsound conclusion. As Feller says, “This shows that a quantitative analysis may be a valuable supplement to our rather shaky intuition” (p. 70). Looking ahead, the key tool in situations like Graunt’s and Boecker’s and Lister’s is creating ceteris paribus —making “everything else the same”—with random selection in experiments, or at least with statistical controls in non-experimental situations. 14.4 Conclusions In all knowledge-seeking and decision-making, our aim is to peer into the unknown and reduce our uncertainty a bit. The two main concepts that we use—the two great concepts in all of scientific knowledge-seeking, and perhaps in all practical thinking and decision-making—are a) continuity (or non-randomness) and the extent to which it applies in given situation, and b) random sampling, and the extent to which we can assume that our observations are indeed chosen by a random process. 14.5 Endnotes These are cases of David Hume’s “constant conjunction.” I benefited from the discussion of this matter by Hald, 1990, p. 93ff. A peculiar perverseness associated with the new knowledge of statistical inference is that very strong findings, which require little or no formal inference to demonstrate and which are so powerful that they can be shown with a simple graph or table, are very hard to publish in social science literature because they do not meet the tests of “rigor,” and “elegance.” Editors view them as detracting from the “technical level” of their journals. A good many of the greatest discoveries of the past would nowadays fall in this category of being difficult or impossible to publish. "],["introduction-to-statistical-inference.html", "15 Introduction to Statistical Inference 15.1 Statistical inference and random sampling 15.2 Summary and conclusions 15.3 Endnotes", " 15 Introduction to Statistical Inference The usual goal of a statistical inference is a decision about which of two or more hypotheses a person will thereafter choose to believe and act upon. The strategy of such inference is to consider the behavior of a given universe in terms of the samples it is likely to produce, and if the observed sample is not a likely outcome of sampling from that universe, we then proceed as if the sample did not in fact come from that universe. (The previous sentence is a restatement in somewhat different form of the core of statistical analysis.) 15.1 Statistical inference and random sampling Continuity and sameness is the fundamental concept in inference in general, as discussed in Chapter 11. Random sampling is the second great concept in inference, and it distinguishes probabilistic statistical inference from non-statistical inference as well as from non-probabilistic inference based on statistical data. Let’s begin the discussion with a simple though unrealistic situation. Your friend Arista a) looks into a cardboard carton, b) reaches in, c) pulls out her hand, and d) shows you a green ball. What might you reasonably infer? You might at least be fairly sure that the green ball came from the carton, though you recognize that Arista might have had it concealed in her hand when she reached into the carton. But there is not much more you might reasonably conclude at this point except that there was at least one green ball in the carton to start with. There could be no more balls; there could be many green balls and no others; there could be a thousand red balls and just one green ball; and there could be one green ball, a hundred balls of different colors, and two pounds of mud—given that she looked in first, it is not improbable that she picked out the only green ball among other material of different sorts. There is not much you could say with confidence about the probability of yourself reaching into the same carton with your eyes closed and pulling out a single green ball. To use other language (which some philosophers might say is not appropriate here as the situation is too specific), there is little basis for induction about the contents of the box. Nor is the situation very different if your friend reaches in three times in a row and hands you a green ball each time. So far we have put our question rather vaguely. Let us frame a more precise inquiry: What do we predict about the next item(s) we might draw from the carton? If we assume—based on who-knows-what information or notions—that another ball will emerge, we could simply use the principle of sameness and (until we see a ball of another color) predict that the next ball will be green, whether one or three or 100 balls is (are) drawn. But now what about if Arista pulls out nine green balls and one red ball? The principle of sameness cannot be applied as simply as before. Based on the last previous ball, the next one will be red. But taking into account all the balls we have seen, the next will “probably” be green. We have no solid basis on which to go further. There cannot be any “solution” to the “problem” of reaching a general conclusion on the basis of these specific pieces of evidence. Now consider what you might conclude if you were told that a single green ball had been drawn with a random sampling procedure from a box containing nothing but balls. Knowledge that the sample was drawn randomly from a given universe is grounds for belief that one knows much more than if a sample were not drawn randomly. First, you would be sure—if you had reasonable basis to believe that the sampling really was random, which is not easy to guarantee—that the ball came from the box. Second, you would guess that the proportion of green balls is not very small, because if there are only a few green balls and many other-colored balls, it would be unusual—that is, the event would have a low probability—to draw a green ball. Not impossible, but unlikely. And we can compute the probability of drawing a green ball —or any other combination of colors— for different assumed compositions within the box . So the knowledge that the sampling process is random greatly increases our ability—or our confidence in our ability—to infer the contents of the box. Let us note well the strategy of the previous paragraph: Ask about the probability that one or more various possible contents of the box (the “universe”) will produce the observed sample , on the assumption that the sample was drawn randomly. This is the central strategy of all statistical inference , though I do not find it so stated elsewhere. We shall come back to this idea shortly. There are several kinds of questions one might ask about the contents of the box. One general category includes questions about our best guesses of the box’s contents—that is, questions of estimation . Another category includes questions about our surety of that description, and our surety that the contents are similar or different from the contents of other boxes; the consideration of surety follows after estimates are made. The estimation questions can be subtle and unexpected (Savage, 1972, Chapter 15), but do not cause major controversy about the foundations of statistics. So we can quickly move on to questions about the extent of surety in our estimations. Consider your reaction if the sampling produces 10 green balls in a row, or 9 out of 10. If you had no other information (a very important assumption that we will leave aside for now), your best guess would be that the box contains all green balls, or a proportion of 9 of 10, in the two cases respectively. This estimation process seems natural enough. You would be surprised if someone told you that instead of the box containing the proportion in the sample, it contained just half green balls. How surprised? Intuitively, the extent of your surprise would depend on the probability that a half-green “universe” would produce 10 or 9 green balls out of 10. This surprise is a key element in the logic of the hypothesis-testing branch of statistical inference. We learn more about the likely contents of the box by asking about the probability that various specific populations of balls within the box would produce the particular sample that we received. That is, we can ask how likely a collection of 25 percent green balls is to produce (say) 9 of 10 green ones, and how likely collections of 50 percent, 75 percent, 90 percent (and any other collections of interest) are to produce the observed sample. That is, we ask about the consistency between any particular hypothesized collection within the box and the sample we observe. And it is reasonable to believe that those universes which have greater consistency with the observed sample— that is, those universes that are more likely to produce the observed sample—are more likely to be in the box than other universes. This (to repeat, as I shall repeat many times) is the basic strategy of statistical investigation. If we observe 9 of 10 green balls, we then determine that universes with (say) 9/10 and 10/10 green balls are more consistent with the observed evidence than are universes of 0/10 and 1/10 green balls. So by this process of considering specific universes that the box might contain, we make possible more specific inferences about the box’s probable contents based on the sample evidence than we could without this process. Please notice the role of the assessment of probabilities here: By one technical means or another (either simulation or formulas), we assess the probabilities that a particular universe will produce the observed sample, and other samples as well. It is of the highest importance to recognize that without additional knowledge (or assumption) one cannot make any statements about the probability of the sample having come from any particular universe , on the basis of the sample evidence. (Better read that last sentence again.) We can only speak about the probability that a particular universe will produce the observed sample, a very different matter. This issue will arise again very sharply in the context of confidence intervals. Let us generalize the steps in statistical inference: Frame the original question as: What is the chance of getting the observed sample x from population X? That is, what is probability of (If x then X)? Proceed to this question: What kinds of samples does X produce, with which probability? That is, what is the probability of this particular x coming from X? That is, what is p(x|X)? Actually investigate the behavior of X with respect to x and other samples. One can do this in two ways: Use the formulaic calculus of probability, perhaps resorting to Monte Carlo methods if an appropriate formula does not exist. Or, Use resampling (in the larger sense), the domain of which equals (all Monte Carlo experimentation) minus (the use of Monte Carlo methods for approximations, investigation of complex functions in statistics and other theoretical mathematics, and uses elsewhere in science). Resampling in its more restricted sense includes the bootstrap, permutation tests, and other non-parametric methods. Interpretation of the probabilities that result from step 3 in terms of i) acceptance or rejection of hypotheses, ii) surety of conclusions, or iii) inputs to decision theory. Here is a short definition of statistical inference: The selection of a probabilistic model that might resemble the process you wish to investigate, the investigation of that model’s behavior, and the interpretation of the results. We will get even more specific about the procedure when we discuss the canonical procedures for hypothesis testing and for the finding of confidence intervals in the chapters on those subjects. The discussion so far has been in the spirit of what is known as hypothesis testing . The result of a hypothesis test is a decision about whether or not one believes that the sample is likely to have been drawn randomly from the “benchmark universe” X. The logic is that if the probability of such a sample coming from that universe is low, we will then choose to believe the alternative—to wit, that the sample came from the universe that resembles the sample. The underlying idea is that if an event would be very surprising if it really happened—as it would be very surprising if the dog had really eaten the homework (see Chapter 15)—we are inclined not to believe in that possibility. (This logic will be explored further in later chapters on hypothesis testing.) We have so far assumed that our only relevant knowledge is the sample. And though we almost never lack some additional information, this can be a sensible way to proceed when we wish to suppress any other information or speculation. This suppression is controversial; those known as Bayesians or subjectivists want us to take into account all the information we have. But even they would not dispute suppressing information in certain cases—such as a teacher who does not want to know students’ IQ scores because s/he might want avoid the possibility of unconsciously being affected by that score, or an employer who wants not to know the potential employee’s ethnic or racial background even though it might improve the hiring process, or a sports coach who refuses to pick the starting team each year until the players have competed for the positions. If the Bayesians will admit the reasonability of suppressing information in at least some situations, it will be a major step in accommodation and in bringing all views into greater harmony. (More about this topic in the appendix). Now consider a variant on the green-ball situation discussed above. Assume now that you are told that samples of balls are alternately drawn from one of two specified universes—two buckets of balls, one with 50 percent green balls and the other with 80 percent green balls. Now you are shown a sample of nine green and one red balls drawn from one of those buckets. On the basis of your sample you can then say how probable it is that the sample came from one or the other universe . You proceed by computing the probabilities (often called the likelihoods in this situation) that each of those two universes would individually produce the observed samples—probabilities that you could arrive at with resampling, with Pascal’s Triangle, or with a table of binomial probabilities, or with the Normal approximation and the Z distribution, or with yet other devices. Those probabilities are .01 and .27, and the ratio of the two (0.1/.27) is a bit less than .04. That is, fair betting odds are about 1 to 27. Let us consider a genetics problem on this model. Plant A produces 3/4 black seeds and 1/4 reds; plant B produces all reds. You get a red seed. Which plant would you guess produced it? You surely would guess plant B. Now, how about 9 reds and a black, from Plants A and C, the latter producing 50 percent reds on average? To put the question more precisely: What betting odds would you give that the one red seed came from plant B? Let us reason this way: If you do this again and again, 4 of 5 of the red seeds you see will come from plant B. Therefore, reasonable (or “fair”) odds are 4 to 1, because this is in accord with the ratios with which red seeds are produced by the two plants— 4/4 to 1/4. How about the sample of 9 reds and a black, and plants A and C? It would make sense that the appropriate odds would be derived from the probabilities of the two plants producing that particular sample, probabilities which we computed above. Now let us move to a bit more complex problem: Consider two buckets—urn G with 2 red and 1 black balls, and bucket H with 100 red and 100 black balls. Someone flips a coin to decide which bucket will be drawn from, reaches into that bucket, and chooses two balls without replacing the first one before drawing the second. Both are red. What are the odds that the sample came from bucket G? Clearly, the answer should derive from the probabilities that the two buckets would produce the observed sample. (Now just for fun, how about if the first ball drawn is thrown back after examining? What now are the appropriate odds?) Let’s restate the central issue. One can state the probability that a particular plant which produces on average 1 red and 3 black seeds will produce one red seed, or 5 reds among a sample of 10. But without further assumptions—such as the assumption above that the possibilities are limited to two specific universes—one cannot say how likely a given red seed is to have come from a given plant, even if we know that that plant produces only reds. (For example, it may have come from other plants producing only red seeds.) When we limit the possibilities to two universes (or to a larger set of specified universes) we are able to put a probability on one hypothesis or another. But to repeat, in many or most cases, one cannot reasonably assume it is only one or the other. And then we cannot state any odds that the sample came from a particular universe. This is a very difficult point to grasp, experience shows, but a crucial one. (It is the sort of subtle issue that makes statistics so difficult.) The additional assumptions necessary to talk about the probability that the red seed came from a given plant are the stuff of statistical inference. And they must be combined with such “objective” probabilistic assessments as the probability that a 1-red-3-black plant will produce one red, or 5 reds among 10 seeds. Now let us move one step further. Instead of stating as a fact under our control that there is a .5 chance of the sample being drawn from each of the two buckets in the problem above, let us assume that we do not know the probability of each bucket being picked, but instead we estimate a probability of .5 for each bucket, based on a variety of other information that all is uncertain. But though the facts are now different, the most reasonable estimate of the odds that the observed sample was drawn from one or the other bucket will not be different than before—because in both situations we were working with a “prior probability” of .5. (The term “prior probability” is the language of the Bayesian approach to statistics.) And when we view the situation this way, the Neyman-Pearson model may be seen perfectly well in a Bayesian framework. Now let us go a step further by allowing the universes from which the sample may have come to have different assumed probabilities as well as different compositions. That is, we now consider prior probabilities other than .5. How do we decide which universe(s) to investigate for the probability of producing the observed sample, and of producing samples that are even less likely, in the sense of being more surprising? That judgment depends upon the purpose of your analysis, upon your point of view of how statistics ought to be done, and upon some other factors. It should be noted that the logic described so far applies in exactly the same fashion whether we do our work estimating probabilities with the resampling method or with conventional methods. We can figure the probability of nine or more green chips from a universe of (say) p = .7 with either approach. So far we have discussed the comparison of various hypotheses and possible universes. We must also consider where the consideration of the reliability of estimates comes in. This leads to the concept of confidence limits , which will be discussed in Chapters 20 and 21. Samples Whose Observations May Have More Than Two Values So far we have discussed samples and universes that we can characterize as proportions of elements which can have only one of two characteristics—green or other, in this case, which is equivalent to “1” or “0.” This expositional choice has been solely for clarity. All the ideas discussed above pertain just as well to samples whose observations may have more than two values, and which may be either discrete or continuous. 15.2 Summary and conclusions A statistical question asks about the probabilities of a sample having arisen from various source universes in light of the evidence of a sample. In every case, the statistical answer comes from considering the behavior of particular specified universes in relation to the sample evidence and to the behavior of other possible universes. That is, a statistical problem is an exercise in postulating universes of interest and interpreting the probabilistic distributions of results of those universes. The preceding sentence is the key operational idea in statistical inference. Different sorts of realistic contexts call for different ways of framing the inquiry. For each of the established models there are types of problems which fit that model better than other models, and other types of problems for which the model is quite inappropriate. Fundamental wisdom in statistics, as in all other contexts, is to employ a large tool kit rather than just applying only a hammer, screwdriver, or wrench no matter what the problem is at hand. (Philosopher Abraham Kaplan once stated Kaplan’s Law of scientific method: Give a small boy a hammer and there is nothing that he will encounter that does not require pounding.) Studying the text of a poem statistically to infer whether Shakespeare or Bacon was the more likely author is quite different than inferring whether bioengineer Smythe can produce an increase in the proportion of calves, and both are different from decisions about whether to remove a basketball player from the game or to produce a new product. Some key points: 1) In statistical inference as in all sound thinking, one’s purpose is central . All judgments should be made relative to that purpose, and in light of costs and benefits. (This is the spirit of the Neyman-Pearson approach). 2) One cannot avoid making judgments; the process of statistical inference cannot ever be perfectly routinized or objectified. Even in science, fitting a model to experience requires judgment. 3) The best ways to infer are different in different situations—economics, psychology, history, business, medicine, engineering, physics, and so on. 4) Different tools must be used when the situations call for them—sequential vs. fixed sampling, Neyman-Pearson vs. Fisher, and so on. 5) In statistical inference it is wise not to argue about the proper conclusion when the data and procedures are ambiguous. Instead, whenever possible, one should go back and get more data, hence lessening the importance of the efficiency of statistical tests. In some cases one cannot easily get more data, or even conduct an experiment, as in biostatistics with cancer patients. And with respect to the past one cannot produce more historical data. But one can gather more and different kinds of data, e.g. the history of research on smoking and lung cancer. 15.3 Endnotes 1. Hence I shall merely mention that the method of moments and the method of maximum likelihood serve most of our needs, and often agree in their conclusions; furthermore, we often know when the former may be inappropriate. "],["point-estimation.html", "16 Point Estimation 16.1 Ways to estimate the mean 16.2 Criteria of estimates 16.3 Estimation of accuracy of the point estimate 16.4 Uses of the mean 16.5 Conclusion 16.6 Endnotes", " 16 Point Estimation One of the great questions in statistical inference is: How big is it? This can mean—How long? How deep? How much time? At what angle? This question about size may pertain to a single object, of which there are many measurements; an example is the location of a star in the heavens. Or the question may pertain to a varied set of elements and their measurements; examples include the effect of treatment with a given drug, and the incomes of the people of the United States in 1994. From where the observer stands, having only the evidence of a sample in hand, it often is impossible to determine whether the data represent multiple observations of a single object, or single (or multiple) observations of multiple objects. For example, from crude measurements of weight you could not know whether one person is being weighed repeatedly, or several people have been weighed once. Hence all the following discussion of point estimation is the same for both of these situations. The word “big” in the first sentence above is purposely vague, because there are many possible kinds of estimates that one might wish to make concerning a given object or collection. For a single object like a star, one surely will wish to make a best guess about its location. But about the effects of a drug treatment, or the incomes of a nation, there are many questions that one may wish to answer. The average effect or income is a frequent and important object of our interest. But one may also wish to know about the amount of dispersion in the distribution of treatment effects, or of incomes, or the sym- metry of the distribution. And there are still other questions one may wish to answer. Even if we focus on the average, the issue often is less clear cut than we may think at first. If we are to choose a single number to characterize the population (universe) from which a given set of data has been drawn, what should that representative number be for the case at hand? The answer must depend on the purpose with which we ask the question, of course. There are several main possibilities such as the mean, the median, and the mode. Even if we confine our attention to the mean as our measure of the central tendency of a distribution, there are various ways of estimating it, each of them having a different rationale. The various methods of estimation often lead to the same estimate, especially if the distribution is symmetric (such as the distribution of errors you make in throwing darts at a dart board). But in an asymmetric case such as a distribution of incomes, the results may differ among the contending modes of estimation. So the entire topic is more messy than appears at first look. Though we will not inquire into the complexities, it is important that you understand that the matter is not as simple as it may seem. (See Savage (1972), Chapter 15, for more discussion of this topic.) 16.1 Ways to estimate the mean The Method of Moments Since elementary school you have been taught to estimate the mean of a universe (or calculate the mean of a sample) by taking a simple arithmetic average. A fancy name for that process is “the method of moments.” It is the equivalent of estimating the center of gravity of a pole by finding the place where it will balance on your finger. If the pole has the same size and density all along its length, that balance point will be halfway between the endpoints, and the point may be thought of as the arithmetic average of the distances from the balance point of all the one-centimeter segments of the pole. Consider this example: Example 13-1: Twenty-nine Out of Fifty People Polled Say They Will Vote For The Democrat. Who Will Win The Election? The Relationship Between The Sample Proportion and The Population Proportion in a Two-Outcome Universe. You take a random sample of 50 people in Maryland and ask which party’s candidate for governor they will vote for. Twenty-nine say they will vote for the Democrat. Let’s say it is reasonable to assume in this case that people will vote exactly as they say they will. The statistical question then facing you is: What proportion of the voters in Maryland will vote for the Democrat in the general election? Your intuitive best guess is that the proportion of the “universe”—which is composed of voters in the general election, in this case—will be the same as the proportion of the sample. That is, 58 percent = 29/50 is likely to be your guess about the proportion that will vote Democratic. Of course, your estimate may be too high or too low in this particular case, but in the long run—that is, if you take many samples like this one—on the average the sample mean will equal the universe (population) proportion, for reasons to be discussed later. The sample mean seems to be the “natural” estimator of the population mean in this and many other cases. That is, it seems quite natural to say that the best estimate is the sample mean, and indeed it probably is best. But why? This is the problem of inverse probability that has bedeviled statisticians for two centuries. If the only information that you have (or that seems relevant) is the evidence of the sample, then there would seem to be no basis for judging that the shape and location of the population differs to the “left” or “right” from that of the sample. That is often a strong argument. Another way of saying much the same thing: If a sample has been drawn randomly, each single observation is a representative estimator of the mean; if you only have one observation, that observation is your best guess about the center of the distribution (if you have no reason to believe that the distribution of the population is peculiar—such as not being symmetrical). And therefore the sum of 2, 3…n of such observations (divided by their number) should have that same property, based on basic principles. But if you are on a ship at sea and a leaf comes raining down from the sky, your best guess about the location of the tree from which it comes is not directly above you, and if two leaves fall, the midpoint of them is not the best location guess, either; you know that trees don’t grow at sea, and birds sometimes carry leaves out to sea. We’ll return to this subject when we discuss criteria of methods. Expected Value and the Method of Moments Consider this gamble: You and another person roll a die. If it falls with the “6” upwards you get $4, and otherwise you pay $1. If you play 120 times, at the end of the day you would expect to have (20*$4 - 100*$1 =) -$20 dollars. We say that -$20 is your “expected value,” and your expected value per roll is (-$20/120 =) $.166 or the loss of 1/6 of a dollar. If you get $5 instead of $4, your expected value is $0. This is exactly the same idea as the method of moments, and we even use the same term—“expected value,” or “expectation”—for the outcome of a calculation of the mean of a distribution. We say that the expected value for the success of rolling a “6” with a single cast of a die is 1/6, and that the expected value of rolling a “6” or a “5” is (1/6 + 1/6 = ) 2/6. The Maximum Likelihood Principle Another way of thinking about estimation of the population mean asks: Which population(s) would, among the possible populations, have the highest probability of producing the observed sample? This criterion frequently produces the same answer as the method of moments, but in some situations the estimates differ. Furthermore, the logic of the maximum-likelihood principle is important. Consider that you draw without replacement six balls—2 black and 4 white—from a bucket that contains twenty balls. What would you guess is the composition of the bucket from which they were drawn? Is it likely that those balls came from a bucket with 4 white and 16 black balls? Rather obviously not, because it would be most unusual to get all the 4 white balls in your draw. Indeed, we can estimate the probability of that happening with simulation or formula to be about .003. How about a bucket with 2 black and 18 whites? The probability is much higher than with the previous bucket, but it still is low—about .075. Let us now estimate the probabilities for all buckets across the range of probabilities. In Figure 13-1 we see that the bucket with the highest probability of producing the observed sample has the same proportions of black and white balls as does the sample. This is called the “maximum likelihood universe.” Nor should this be very surprising, because that universe obviously has an equal chance of producing samples with proportions below and above that observed proportion—as was discussed in connection with the method of moments. We should note, however, that the probability that even such a maximum-likelihood universe would produce exactly the observed sample is very low (though it has an even lower probability of producing any other sample). .14 .12 .10 Probability .08 .06 .04 .02 0 4 White and 2 Black Balls in the Sample 2 4 6 8 10 12 14 16 18 20 Number of White Balls in the Universe (N=20) Figure 13-1 Choice of Estimation Method When should you base your estimate on the method of moments, or of maximum likelihood, or still some other principle? There is no general answer. Sound estimation requires that you think long and hard about the purpose of your estimation, and fit the method to the purpose. I am well aware that this is a very vague statement. But though it may be an uncomfortable idea to live with, guidance to sound statistical method must be vague because it requires sound judgment and deep knowledge of the particular set of facts about the situation at hand. 16.2 Criteria of estimates How should one judge the soundness of the process that produces an estimate? General criteria include representativeness and accuracy . But these are pretty vague; we’ll have to get more specific. Unbiasedness Concerning representativeness: We want a procedure that will not be systematically in error in one direction or another. In technical terms, we want an “unbiased estimate,” if possible. “Unbiased” in this case does not mean “friendly” or “unprejudiced,” but rather implies that on the average—that is, in the long run, after taking repeated samples—estimates that are too high will about balance (in percentage terms) those that are too low. The mean of the universe (or the proportion, if we are speaking of two-valued “binomial situations”) is a frequent object of our interest. And the sample mean is (in most cases) an unbiased estimate of the population mean. Let’s now see an informal proof that the mean of a randomlydrawn sample is an “unbiased” estimator of the population mean. That is, the errors of the sample means will cancel out after repeated samples because the mean of a large number of sample means approaches the population mean. A second “law” to be informally proven is that the size of the inaccuracy of a sample proportion is largest when the population proportion is near 50 percent, and smallest when it approaches zero percent or 100 percent. The statement that the sample mean is an unbiased estimate of the population mean holds for many but not all kinds of samples—proportions of two-outcome (Democrat-Republican) events (as in this case) and also the means of many measured-data universes (heights, speeds, and so on) that we will come to later. But, you object, I have only said that this is so; I haven’t proven it. Quite right. Now we will go beyond this simple assertion, though we won’t reach the level of formal proof. This discussion applies to conventional analytic statistical theory as well as to the resampling approach. We want to know why the mean of a repeated sample—or the proportion, in the case of a binomial universe—tends to equal the mean of the universe (or the proportion of a binomial sample). Consider a population of one thousand voters. Split the population into random sub-populations of 500 voters each; let’s call these sub-populations by the name “samples.” Almost inevitably, the proportions voting Democratic in the samples will not exactly equal the “true” proportions in the population. (Why not? Well, why should they split evenly? There is no general reason why they should.) But if the sample proportions do not equal the population proportion, we can say that the extent of the difference between the two sample proportions and the population proportion will be identical but in the opposite direction . If the population proportion is 600/1000 = 60 percent, and one sample’s proportion is 340/500 = 68 percent, then the other sample’s proportion must be (600-340 = 260)/500 = 52 percent. So if in the very long run you would choose each of these two samples about half the time (as you would if you selected between the two samples randomly) the average of the sample proportions would be (68 percent + 52 percent)/2 = 60 percent. This shows that on the average the sample proportion is a fair and unbiased estimate of the population proportion—if the sample is half the size of the population. If we now sub-divide each of our two samples of 500 (each of which was half the population size) into equal-size subsamples of 250 each, the same argument will hold for the proportions of the samples of 250 with respect to the sample of 500: The proportion of a 250-voter sample is an unbiased estimate of the proportion of the 500-voter sample from which it is drawn. It seems inductively reasonable, then, that if the proportion of a 250-voter sample is an unbiased estimate of the 500-voter sample from which it is drawn, and the proportion of a 500-voter sample is an unbiased estimate of the 1000-voter population, then the proportion of a 250-voter sample should be an unbiased estimate of the population proportion. And if so, this argument should hold for samples of 1/2 x 250 = 125, and so on—in fact for any size sample. The argument given above is not a rigorous formal proof. But I doubt that the non-mathematician needs, or will benefit from, a more formal proof of this proposition. You are more likely to be persuaded if you demonstrate this proposition to yourself experimentally in the following manner: Step 1. Let “1-6” = Democrat, “7-10” = Republican Step 2. Choose a sample of, say, ten random numbers, and record the proportion Democrat (the sample proportion). Step 3. Repeat step 2 a thousand times. Step 4. Compute the mean of the sample proportions, and compare it to the population proportion of 60 percent. This result should be close enough to reassure you that on the average the sample proportion is an “unbiased” estimate of the population proportion, though in any particular sample it may be substantially off in either direction. Efficiency We want an estimate to be accurate, in the sense that it is as close to the “actual” value of the parameter as possible. Sometimes it is possible to get more accuracy at the cost of biasing the estimate. More than that does not need to be said here. Maximum Likelihood Knowing that a particular value is the most likely of all values may be of importance in itself. For example, a person betting on one horse in a horse race is interested in his/her estimate of the winner having the highest possible probability, and is not the slightest bit interested in getting nearly the right horse. Maximum likelihood estimates are of particular interest in such situations. Criteria of the Criteria What should we look for in choosing criteria? Logically, this question should precede the above list of criteria. Savage (1954, Chapter 15) has urged that we should always think in terms of the consequences of choosing criteria, in light of our purposes in making the estimate. I believe that he is making an important point. But it often is very hard work to think the matter through all the way to the consequences of the criteria chosen. And in most cases, such fine inquiry is not needed, in the sense that the estimating procedure chosen will be the same no matter what consequences are considered. 16.3 Estimation of accuracy of the point estimate So far we have discussed how to make a point estimate, and criteria of good estimators. We also are interested in estimating the accuracy of that estimate. That subject—which is harder to grapple with—is discussed in Chapters 20 and 21 on confidence intervals. Most important: One cannot sensibly talk about the accuracy of probabilities in the abstract, without reference to some set of facts. In the abstract, the notion of accuracy loses any meaning, and invites confusion and argument. 16.4 Uses of the mean Let’s consider when the use of a device such as the mean is valuable, in the context of the data on marksmen in Table 13-1. If we wish to compare marksman A versus marksman B, we can immediately see that marksman A hit the bullseye (80 shots for 3 points each time) as many times as marksman B hit either the bullseye or simply got in the black (30 shots for 3 points and 50 shots for 2 points), and A hit the black (2 points) as many times as B just got in the white (1 point). From these two comparisons covering all the shots, in both of which comparisons A does better, it is immediately obvious that marksman A is better than marksman B. We can say that A’s score dominates B’s score. Table 13-1 Score No. of Occurrences Percent / Probability Marksman A 1 0 0 2 20 .2 3 80 .8 Marksman B 1 20 .2 2 50 .5 3 30 .3 Marksman C 1 40 .4 2 10 .1 3 50 .5 Marksman D 1 10 .1 2 60 .6 3 30 .3 When we turn to comparing marksman C to marksman D, however, we cannot say that one “dominates” the other as we could with the comparison of marksmen A and B. Therefore, we turn to a summarizing device. One such device that is useful here is the mean. For marksman C the mean score is (40*1) + (10*2) + (50*3) = 210, while for marksman D the mean score is (10*1) + (60*2) + (30*3) = 220. Hence we can say that D is better than C even though D’s score does not dominate C’s score in the bullseye category. Another use of the mean (Gnedenko, 1962, p. 68) is shown in the estimation of the number of matches that we need to start fires for an operation carried out 20 times in a day (Table 13- 2). Let’s say that the number of cases where s/he needs 1, 2 ... 5 matches to start a fire are as follows (along with their probabilities) based on the last 100 fires started: Table 13-2 Number of Matches Number of Cases Probabilities 1 7 .16 2 16 .16 3 55 .55 4 21 .21 5 1 .01 If you know that the operator will be lighting twenty fires, you can estimate the number of matches that s/he will need by multiplying the mean number of matches (which turns out be 1*.07 + 2*0.16 + 3* 0.55 + 4*0.21 + 5*0.01 = 2.93 ) in the ob- served experience by 20. Here you are using the mean as an indication of a representative case. It is common for writers to immediately produce the data in the forms of percentages or probabilities. But I think it is important to include in our discussion the absolute numbers, because this is what one must begin with in practice. And keeping the absolute numbers in mind is likely to avoid some confusions that arise if one immediately goes to percentages or to probabilities. Still another use for the mean is when you have a set of observations with error in them. The mean of the observations probably is your best guess about which is the “right” one. Furthermore, the distance you are likely to be off the mark is less if you select the mean of the observations. An example might be a series of witnesses giving the police their guesses about the height of a man who overturned an outhouse. The mean probably is the best estimate to give to police officers as a description of the perpetrator (though it would be helpful to give the range of the observations as well). We use the mean so often, in so many different circumstances, that we become used to it and never think about its nature. So let’s do so a bit now. Different statistical ideas are appropriate for business and engineering decisions, biometrics, econometrics, scientific explanation (the philosophers’ case), and other fields. So nothing said here holds everywhere and always. One might ask: What is the “meaning” of a mean? But that is not a helpful question. Rather, we should ask about the uses of a mean. Usually a mean is used to summarize a set of data. As we saw with marksmen C and D, it often is difficult to look at a table of data and obtain an overall idea of how big or how small the observations are; the mean (or other measurements) can help. Or if you wish to compare two sets of data where the distributions of observations overlap each other, comparing the means of the two distributions can often help you better understand the matter. Another complication is the confusion between description and estimation , which makes it difficult to decide where to place the topic of descriptive statistics in a textbook. For example, compare the mean income of all men in the U. S., as measured by the decennial census. This mean of the universe can have a very different meaning from the mean of a sample of men with respect to the same characteristic. The sample mean is a point estimate, a statistical device, whereas the mean of the universe is a description. The use of the mean as an estimator is fraught with complications. Still, maybe it is no more complicated than deciding what describer to use for a population. This entire matter is much more complex than it appears at first glance. When the sample size approaches in size the entire population—when the sample becomes closer and closer to being the same as the population—the two issues blend. What does that tell us? Anything? What is the relationship between a baseball player’s average for two weeks, and his/her lifetime average? This is subtle stuff—rivaling the subtleness of arguments about inference versus probability, and about the nature of confidence limits (see Chapters 20 and 21). Maybe the only solid answer is to try to stay super-clear on what you are doing for what purpose, and to ask continually what job you want the statistic (or describer) to do for you. The issue of the relationship of sample size to population size arises here. If the sample size equals or approaches the population size, the very notion of estimation loses its meaning. The notion of “best estimator” makes no sense in some situations, including the following: a) You draw one black ball from a bucket. You cannot put confidence intervals around your estimate of the proportion of black balls, except to say that the proportion is somewhere between 1 and 0. No one would proceed without bringing in more information. That is, when there is almost no information, you simply cannot make much of an estimate—and the resampling method breaks down, too. It does not help much to shift the discussion to the models of the buckets, because then the issue is the unknown population of the buckets, in which case we need to bring in our general knowledge. b) When the sample size equals or is close to the population size, as discussed in Chapter 13, the data are a description rather than an estimate, because the sample is getting to be much the same as the universe; that is, if there are twelve people in your family, and you randomly take a sample of the amount of sugar used by eight members of the family, the results of the sample cannot be very different than if you com- pute the amount for all twelve family members. In such a case, the interpretation of the mean becomes complex. Underlying all estimation is the assumption of continuation, which follows from random sampling—that there is no reason to expect the next sample to be different from the present one in any particular fashion, mean or variation. But we do expect it to be different in some fashion because of sampling variability. 16.5 Conclusion A Newsweek article says, “According to a recent reader’s survey in Bride’s magazine, the average blowout \\[wedding\\] will set you back about $16,000” (Feb 15, 1993, p. 67). That use of the mean (I assume) for the average, rather than the median, could cost the parents of some brides a pretty penny. It could be that the cost for the average person —that is, the median expenditure—might be a lot less than $16,000. (A few million dollar weddings could have a huge effect on a survey mean.) An inappropriate standard of comparison might enter into some family discussions as a result of this article, and cause higher outlays than otherwise. This chapter helps one understand the nature of such estimates. 16.6 Endnotes See Savage, 1954, Chapter 15, for many other criteria of estimators. This discussion follows Genedenko and Khinchtin, 1962, Chapter 8. References "],["framing-statistical-questions.html", "17 Framing Statistical Questions 17.1 Introduction 17.2 Translating scientific questions into probabilistic and statistical questions 17.3 The three types of questions Illustrative translations The steps in statistical inference 17.4 Summary 17.5 Endnotes", " 17 Framing Statistical Questions 17.1 Introduction Chapters 3-10 discussed problems in probability theory. That is, we have been estimating the probability of a composite event resulting from a system in which we know the probabilities of the simple events —the “parameters” of the situation. Then Chapters 11-13 discussed the underlying philosophy of statistical inference. Now we turn to inferential-statistical problems. Up until now, we have been estimating the complex probabilities of known universes—the topic of probability . Now as we turn to problems in statistics , we seek to learn the characteristics of an unknown system—the basic probabilities of its simple events and parameters. (Here we note again, however, that in the process of dealing with them, all statistical-inferential problems eventually are converted into problems of pure probability). To assess the characteristics of the system in such problems, we employ the characteristics of the sample(s) that have been drawn from it. For further discussion on the distinction between inferential statistics and probability theory, see Chapters 1-3. This chapter begins the topic of hypothesis testing . The issue is: whether to adjudge that a particular sample (or samples) come(s) from a particular universe. A two-outcome yes-no universe is discussed first. Then we move on to “measured-data” universes, which are more complex than yes-no outcomes be- cause the variables can take on many values, and because we ask somewhat more complex questions about the relationships of the samples to the universes. This topic is continued in subsequent chapters. In a typical hypothesis-testing problem presented in this chapter, one sample of hospital patients is treated with a new drug and a second sample is not treated but rather given a “placebo.” After obtaining results from the samples, the “null” or “test” or “benchmark” hypothesis would be that the resulting drug and placebo samples are drawn from the same universe. This device of the null hypothesis is the equivalent of stating that the drug had no effect on the patients. It is a special intellectual strategy developed to handle such statistical questions. We start with the scientific question: Does the medicine have an effect? We then translate it into a testable statistical question: How likely is it that the sample means come from the same universe? This process of question-translation is the crucial step in hypothesis-testing and inferential statistics. The chapter then explains how to solve these problems using resampling methods after you have formulated the proper statistical question. Though the examples in the chapter mostly focus on tests of hypotheses, the procedures also apply to confidence intervals, which will be discussed later. 17.2 Translating scientific questions into probabilistic and statistical questions The first step in using probability and statistics is to translate the scientific question into a statistical question. Once you know exactly which prob-stats question you want to ask—that is, exactly which probability you want to determine—the rest of the work is relatively easy (though subtle). The stage at which you are most likely to make mistakes is in stating the question you want to answer in probabilistic terms. Though this translation is difficult, it involves no mathematics. Rather, this step requires only hard thought. You cannot beg off by saying, “I have no brain for math!” The need is for a brain that will do clear thinking, rather than a brain especially talented in mathematics. A person who uses conventional methods can avoid this hard thinking by simply grabbing the formula for some test without understanding why s/he chooses that test. But resampling pushes you to do this thinking explicitly. This crucial process of translating from a pre-statistical question to a statistical question takes place in all statistical inference. But its nature comes out most sharply with respect to testing hypotheses, so most of what will be said about it will be in that context. 17.3 The three types of questions Let’s consider the natures of conceptual, operational, and statistical questions. The Scientific Question A study for either scientific or decision-making purposes properly begins with a general question about the nature of the world—that is, a conceptual or theoretical question. One must then transform this question into an operational-empirical form that one can study scientifically. Thence comes the translation into a technical-statistical question. The scientific-conceptual-theoretical question can be an issue of theory, or a policy choice, or the result of curiosity at large. Examples include: Can a bioengineer increase the chance of female calves being born? Is copper becoming less scarce? Are the prices of liquor systematically different in states where the liquor stores are publicly owned compared to states where they are privately owned? Does a new formulation of pig rations lead to faster hog growth? Was the rate of unemployment higher last month than the long-run average, or was the higher figure likely to be the result of sampling error? What are the margins of probable error for an unemployment survey? The Operational-Empirical Question The operational-empirical question is framed in measurable quantities in a meaningful design. Examples include: How likely is this state of affairs (say, the new pig-food formulation) to cause an event such as was observed (say, the observed increase in hog growth)? How likely is it that the mean unemployment rate of a sample taken from the universe of interest (say, the labor force, with an unemployment rate of 10 percent) will be between 11 percent and 12 percent? What is the probability of getting three girls in the first four children if the probability of a girl is .48? How unlikely is it to get nine females out of ten calves in an experiment on your farm? Did the price of copper fall between 1800 and the present? These questions are in the form of empirical questions, which have already been transformed by operationalizing from scientific-conceptual questions. The Statistical Question At this point one must decide whether the conceptual-scientific question is of the form of either a) or b): A test about whether some sample will frequently happen by chance rather than being very surprising—a test of the “significance” of a hypothesis. Such hypothesis testing takes the following form: How likely is a given “universe” to produce some sample like x? This leads to interpretation about: How likely is a given universe to be the cause of this observed sample? A question about the accuracy of the estimate of a parameter of the population based upon sample evidence (an inquiry about “confidence intervals”). This sort of question is considered by some (but not by me) to be a question in estimation— that is, one’s best guess about (say) the magnitude and probable error of the mean or median of a population. This is the form of a question about confidence limits—how likely is the mean to be between x and y? Notice that the statistical question is framed as a question in probability. Illustrative translations The best way to explain how to translate a scientific question into a statistical question is to illustrate the process. Illustration A Were doctors’ beliefs as of 1964 about the harmfulness of cigarette smoking (and doctors’ own smoking behavior) affected by the social groups among whom the doctors live (Simon, 1967-1968)? That was the theoretical question. We decided to define the doctors’ reference groups as the states in which they live, because data about doctors and smoking were available state by state ( Modern Medicine , 1964). We could then translate this question into an operational and testable scientific hypothesis by asking this question: Do doctors in tobacco-economy states differ from doctors in other states in their smoking, and in their beliefs about smoking? Which numbers would help us answer this question, and how do we interpret those numbers? We now were ready to ask the statistical question: Do doctors in tobacco-economy states “belong to the same universe” (with respect to smoking) as do other doctors? That is, do doctors in tobacco-economy states have the same characteristics—at least, those characteristics we are interested in, smoking in this case—as do other doctors? Later we shall see that the way to proceed is to consider the statistical hypothesis that these doctors do indeed belong to that same universe; that hypothesis and the universe will be called “benchmark hypothesis” and “benchmark universe” respectively—or in more conventional usage, the “null hypothesis.” If the tobacco-economy doctors do indeed belong to the benchmark universe—that is, if the benchmark hypothesis is correct—then there is a 49/50 chance that doctors in some state other than the state in which tobacco is most important will have the highest rate of cigarette smoking. But in fact we observe that the state in which tobacco accounts for the largest proportion of the state’s income—North Carolina—had (as of 1964) a higher proportion of doctors who smoked than any other state. (Furthermore, a lower proportion of doctors in North Carolina than in any other state said that they believed that smoking is a health hazard.) Of course, it is possible that it was just chance that North Carolina doctors smoked most, but the chance is only 1 in 50 if the benchmark hypothesis is correct. Obviously, some state had to have the highest rate, and the chance for any other state was also 1 in 50. But, because our original scientific hypothesis was that North Carolina doctors’ smoking rate would be highest, and we then observed that it was highest even though the chance was only 1 in 50, the observation became interesting and meaningful to us. It means that the chances are strong that there was a connection between the importance of tobacco in the economy of a state and the rate of cigarette smoking among doctors living there (as of 1964). To consider this problem from another direction, it would be rare for North Carolina to have the highest smoking rate for doctors if there were no special reason for it; in fact, it would occur only once in fifty times. But, if there were a special rea- son—and we hypothesize that the tobacco economy provides the reason—then it would not seem unusual or rare for North Carolina to have the highest rate; therefore we choose to believe in the not-so-unusual phenomenon, that the tobacco economy caused doctors to smoke cigarettes. Like many (most? all?) actual situations, the cigarettes and doctors’ smoking issue is a rather messy business. Did I have a clear-cut, theoretically-derived prediction before I began? Maybe I did a bit of “data dredging”—that is, maybe I started with a vague expectation, and only arrived at my sharp hypothesis after I saw the data. This would weaken the probabilistic interpretation of the test of significance—but this is something that a scientific investigator does not like to do because it weakens his/her claim for attention and chance of publication. On the other hand, if one were a Bayesian, one could claim that one had a prior probability that the observed effect would occur, and the observed data strengthens that prior; but this procedure would not seem proper to many other investigators. The only wholly satisfactory conclusion is to obtain more data—but as of 1993, there does not seem to have been another data set collected since 1964, and collecting a set by myself is not feasible. This clearly is a case of statistical inference that one could argue about, though perhaps it is true that all cases where the data are sufficiently ambiguous as to require a test of significance are also sufficiently ambiguous that they are properly subject to argument. For some decades the hypothetico-deductive framework was the leading point of view in empirical science. It insisted that the empirical and statistical investigation should be preceded by theory, and only propositions suggested by the theory should be tested. Investigators were not supposed to go back and forth from data to theory to testing. It is now clear that this is an ivory-tower irrelevance, and no one lived by the hypothetico-deductive strictures anyway—just pretended to. Furthermore, there is no sound reason to feel constrained by it, though it strengthens your conclusions if you had theoretical reason in advance to expect the finding you obtained. Illustration B Does medicine CCC cure a cancer? That’s the scientific question. So you give the medicine to six patients who have the cancer and you do not give it to six similar patients who have the cancer. Your sample contains only twelve people because it is not feasible for you to obtain a larger sample. Five of six “medicine” patients get well, two of six “no medicine” patients get well. Does the medicine cure the cancer? That is, if future cancer patients take the medicine, will their rate of recovery be higher than if they did not take the medicine? One way to translate the scientific question into a statistical question is to ask: Do the “medicine” patients belong to the same universe as the “no medicine” patients? That is, we ask whether “medicine” patients still have the same chances of getting well from the cancer as do the “no medicine” patients, or whether the medicine has bettered the chances of those who took it and thus removed them from the original universe, with its original chances of getting well. The original universe, to which the “no medicine” patients must still belong, is the benchmark universe. Shortly we shall see that we proceed by comparing the observed results against the benchmark hypothesis that the “medicine” patients still belong to the benchmark universe — that is, they still have the same chance of getting well as the “no medicine” patients. We want to know whether or not the medicine does any good. This question is the same as asking whether patients who take medicine are still in the same population (universe) as “no medicine” patients, or whether they now belong to a different population in which patients have higher chances of getting well. To recapitulate our translations, we move from asking: Does the medicine cure the cancer? to, Do “medicine” patients have the same chance of getting well as “no medicine” patients?; and finally, to: Do “medicine” patients belong to the same universe (population) as “no medicine” patients? Remember that “population” in this sense does not refer to the population at large, but rather to a group of cancer sufferers (perhaps an infinitely large group) who have given chances of getting well, on the average. Groups with different chances of getting well are called “different populations” (universes). Shortly we shall see how to answer this statistical question. We must keep in mind that our ultimate concern in cases like this one is to predict future results of the medicine, that is, to predict whether use of the medicine will lead to a higher recovery rate than would be observed without the medicine. Illustration C Is method Alpha a better method of teaching reading than method Beta? That is, will method Alpha produce a higher average reading score in the future than will method Beta? Twenty children taught to read with method Alpha have an average reading score of 79, whereas children taught with method Beta have an average score of 84. To translate this scientific question into a statistical question we ask: Do children taught with method Alpha come from the same universe (population) as children taught with method Beta? Again, “universe” (population) does not mean the town or social group the children come from, and indeed the experiment will make sense only if the children do come from the same population, in that sense of “population.” What we want to know is whether or not the children belong to the same statistical population (universe), defined according to their reading ability, after they have studied with method Alpha or method Beta. Illustration D If one plot of ground is treated with fertilizer, and another similar plot is not treated, the benchmark (null) hypothesis is that the corn raised on the treated plot is no different than the corn raised on the untreated lot—that is, that the corn from the treated plot comes from (“belongs to”) the same universe as the corn from the untreated plot. If our statistical test makes it seem very unlikely that a universe like that from which the untreated-plot corn comes would also produce corn such as came from the treated plot, then we are willing to believe that the fertilizer has an effect. For a psychological example, substitute the words “group of children” for “plot,” “special training” for “fertilizer,” and “I.Q. score” for “corn.” There is nothing sacred about the benchmark (null) hypothesis of “no difference.” You could just as well test the benchmark hypothesis that the corn comes from a universe that averages 110 bushels per acre, if you have reason to be especially interested in knowing whether or not the fertilizer produces more than 110 bushels per acre. But in many cases it is reasonable to test the probability that a sample comes from the population that does not receive the special treatment of medicine, fertilizer, or training. So far we have discussed the scientific question and the statistical question. Remember that there is always a generalization question, too: Do the statistical results from this particular sample of, say, rats apply to a universe of humans? This question can be answered only with wisdom, common sense, and general knowledge, and not with probability statistics. Translating from a scientific question into a statistical question is mostly a matter of asking the probability that some given benchmark universe (population) will produce one or more observed samples. Notice that we must (at least for general scientific testing purposes) ask about a given universe whose composition we assume to be known , rather than about a range of universes, or about a universe whose properties are unknown. In fact, there is really only one question that probability statistics can answer: Given some particular benchmark universe of some stated composition, what is the probability that an observed sample would come from it? (Please notice the subtle but all-important difference between the words “would come” in the previous sentence, and the word “came.”) A variation of this question is: Given two (or more) samples, what is the probability that they would come from the same universe—that is, that the same universe would produce both of them? In this latter case, the relevant benchmark universe is implicitly the universe whose composition is the two samples combined. The necessity for stating the characteristics of the universe in question becomes obvious when you think about it for a moment. Probability-statistical testing adds up to comparing a sample with a particular benchmark universe, and asking whether there probably is a difference between the sample and the universe. To carry out this comparison, we ask how likely it is that the benchmark universe would produce a sample like the observed sample. But in order to find out whether or not a universe could produce a given sample, we must ask whether or not some particular universe—with stated characteristics— could produce the sample. There is no doubt that some universe could produce the sample by a random process; in fact, some universe did. The only sensible question, then, is whether or not a particular universe, with stated (or known) characteristics, is likely to produce such a sample. In the case of the medicine, the universe with which we compare the sample who took the medicine is the benchmark universe to which that sample would belong if the medicine had had no effect. This comparison leads to the benchmark (null) hypothesis that the sample comes from a population in which the medicine (or other experimental treatment) seems to have no effect . It is to avoid confusion inherent in the term “null hypothesis” that I replace it with the term “benchmark hypothesis.” The concept of the benchmark (null) hypothesis is not easy to grasp. The best way to learn its meaning is to see how it is used in practice. For example, we say we are willing to be- lieve that the medicine has an effect if it seems very unlikely from the number who get well that the patients given the medicine still belong to the same benchmark universe as the patients given no medicine at all—that is, if the benchmark hypothesis is unlikely. The steps in statistical inference These are the steps in conducting statistical inference Step 1. Frame a question in the form of: What is the chance of getting the observed sample x from some specified population X? For example, what is the probability of getting a sample of 9 females and one male from a population where the probability of getting a single female is .48? Step 2. Reframe the question in the form of: What kinds of samples does population X produce, with which probabilities? That is, what is the probability of the observed sample x (9 females in 10 calves), given that a population is X (composed of 48 percent females)? Or in notation, what is p(x|X)? Step 3. Actually investigate the behavior of S with respect to S and other samples. This can be done in two ways: Use the calculus of probability (the formulaic method), perhaps resorting to the Monte Carlo method if an appropriate formula does not exist. Or Resampling (in the larger sense), which equals the Monte Carlo method minus its use for approximations, investigation of complex functions in statistics and other theoretical mathematics, and non-resampling uses elsewhere in science. Resampling in the more restricted sense includes bootstrap, permutation, and other non-parametric methods. More about the resampling procedure follows in the paragraphs to come, and then in later chapters in the book. Step 4. Interpret the probabilities that result from step 3 in terms of acceptance or rejection of hypotheses, surety of conclusions, and as inputs to decision theory. The following short definition of statistical inference summarizes the previous four steps: Statistical inference equals the selection of a probabilistic model to resemble the process you wish to investigate, the investigation of that model’s behavior, and the interpretation of the results. Stating the steps to be followed in a procedure is an operational definition of the procedure. My belief in the clarifying power of this device (the operational definition) is embodied in the set of steps given in Chapter 10 for the various aspects of statistical inference. A canonical question-and-answer procedure for testing hypotheses will be found in Chapter 19, and one for confidence intervals will be found in Chapter 20. 17.4 Summary We define resampling to include problems in inferential statistics as well as problems in probability as follows: Using the entire set of data you have in hand, or using the given data-generating mechanism (such as a die) that is a model of the process you wish to understand, produce new samples of simulated data, and examine the results of those samples . That’s it in a nutshell. In some cases, it may also be appropriate to amplify this procedure with additional assumptions. Problems in pure probability may at first seem different in nature than problems in statistical inference. But the same logic as stated in this definition applies to both varieties of problems. The difference is that in probability problems the “model” is known in advance—say, the model implicit in a deck of poker cards plus a game’s rules for dealing and counting the results—rather than the model being assumed to be best estimated by the observed data, as in resampling statistics. The hardest job in using probability statistics, and the most important, is to translate the scientific question into a form to which statistics can give a sensible answer. You must translate scientific questions into the appropriate form for statistical operations , so that you know which operations to perform. This is the part of the job that requires hard, clear thinking— though it is non-mathematical thinking—and it is the part that someone else usually cannot easily do for you. Once you know exactly which probability-statistical question you want to ask—that is, exactly which probability you want to determine—the rest of the work is relatively easy. The stage at which you are most likely to make mistakes is in stating the question you want to answer in probabilistic terms. Though this step is hard, it involves no mathematics . This step requires only hard, clear thinking . You cannot beg off by saying “I have no brain for math!” To flub this step is to admit that you have no brain for clear thinking, rather than no brain for mathematics. 17.5 Endnotes 1. These steps are discussed in more philosophic depth in my forthcoming book on the philosophy of statistics and resampling. "],["hypothesis-testing-with-counted-data-part-1.html", "18 Hypothesis-Testing with Counted Data, Part 1 18.1 Introduction 18.2 Should a single sample of counted data be considered different from a benchmark universe?", " 18 Hypothesis-Testing with Counted Data, Part 1 18.1 Introduction The first task in inferential statistics is to make one or more point estimates —that is, to make one or more statements about how much there is of something we are interested in—including especially the mean and the dispersion. (That work goes under the label “estimation” and is discussed in chapter 13.) Frequently the next step, after making such quantitative estimation of the universe from which a sample has been drawn, is to consider whether two or more samples are different from each other, or whether the single sample is different from a specified value; this work goes under the label “hypothesis testing.” We ask: Did something happen? Or: Is there a difference between two universes? These are yes-no questions. In other cases, the next step is to inquire into the reliability of the estimates; this goes under the label “confidence intervals.” (Some writers include assessing reliability under the rubric of estimation, but I judge it better not to do so). So: Having reviewed how to convert hypothesis-testing problems into statistically testable questions in Chapter 14, we now must ask: How does one employ resampling methods to make the statistical test? As is always the case when using resampling techniques, there is no unique series of steps by which to proceed. The crucial criterion in assessing the model is whether it accurately simulates the actual event. With hypothesis-testing problems, any number of models may be correct. Generally speaking, though, the model that makes fullest use of the quantitative information available from the data is the best model. When attempting to deduce the characteristics of a universe from sample data, or when asking whether a sample was drawn from a particular universe, a crucial issue is whether a “one-tailed test” or a “two-tailed test” should be applied. That is, in examining the results of our resampling experiment based on the benchmark universe, do we examine both ends of the frequency distribution, or just one? If there is strong reason to believe a priori that the difference between the benchmark (null) universe and the sample will be in a given direction—for example if you hypothesize that the sample mean will be smaller than the mean of the benchmark universe—you should then employ a one-tailed test . If you do not have strong basis for such a prediction, use the two-tailed test. As an example, when a scientist tests a new medication, his/her hypothesis would be that the number of patients who get well will be higher in the treated group than in the control group. Thus, s/he applies the one-tailed test. Some language first: Hypothesis: In inferential statistics, a statement or claim about a universe that can be tested and that you wish to investigate. Testing: The process of investigating the validity of a hypothesis. Benchmark (or null) hypothesis: A particular hypothesis chosen for convenience when testing hypotheses in inferential statistics. For example, we could test the hypothesis that there is no difference between a sample and a given universe, or between two samples, or that a parameter is less than or greater than a certain value. The benchmark universe refers to this hypothesis. (The concept of the benchmark or null hypothesis was discussed in Chapters 5 and 14.) Now let us begin the actual statistical testing of various sorts of hypotheses about samples and populations. 18.2 Should a single sample of counted data be considered different from a benchmark universe? Example 15-1: Does Irradiation Affect the Sex Ratio in Fruit Flies? (Where the Benchmark Universe Mean (in this case, the Propor tion) is Known, is the Mean (Propor tion) of the Population Affected by the Treatment?) (Program “Fruitfly”) You think you have developed a technique for irradiating the genes of fruit flies so that the sex ratio of the offspring will not be half males and half females. In the first twenty cases you treat, there are fourteen males and six females. Does this experimental result confirm that the irradiation does work? First convert the scientific question—whether or not the treatment affects the sex distribution—into a probability-statistical question: Is the observed sample likely to have come from a benchmark universe in which the sex ratio is one male to one female? The benchmark (null) hypothesis, then, is that the treatment makes no difference and the sample comes from the one-male-to-one-female universe. Therefore, we investigate how likely a one-to-one universe is to produce a distribution of fourteen or more of just one sex . A coin has a one-to-one (one out of two) chance of coming up tails. Therefore, we might flip a coin in groups of twenty flips, and count the number of heads in each twenty flips. Or we can use a random number table. The following steps will produce a sound estimate: Step 1. Let heads = male, tails = female. Step 2. Flip twenty coins and count the number of males. If 14 or more males occur, record “yes.” Also, if 6 or fewer males occur, record “yes” because this means we have gotten 14 or more females. Otherwise, record “no.” Step 3. Repeat step 2 perhaps 100 times. Step 4. Calculate the proportion “yes” in the 100 trials. This proportion estimates the probability that a fruit-fly population with a propensity to produce 50 percent males will by chance produce as many as 14 or as few as 6 males in a sample of 20 flies. Table 15-1 shows the results obtained in twenty-five trials of twenty flips each. In two of the twenty-five trials (8 percent) there were fourteen or more heads, which we call “males,” and in one of the twenty-five trials (4 percent) there were only six heads, meaning there were fourteen tails (“females”). We can therefore estimate that, even if the treatment does not affect the sex and the births over a long period really are one to one, three out of twenty-five times (12 percent) we would get fourteen or more of one sex or the other. Therefore, finding fourteen males out of twenty births is not overwhelming evidence that the treatment has any effect, even though the result is suggestive. How accurate is the estimate? Seventy-five more trials were made, and of the 100 trials ten contained fourteen or more “males” (10 percent), and seven trials contained fourteen or more “females” (7 percent), a total of 17 percent. So the first twenty-five trials gave a fairly reliable indication. As a matter of fact, analytically-based computation (not explained here) shows that the probability of getting fourteen or more females out of twenty births is .057 and, of course, the same for fourteen or more males from a one-to-one universe, implying a total probability of .114 of getting fourteen or more males or females. Table 15-1 Results From 25 Random Trials for “Fruitfly” Problem Trial of # &gt;=14 Trial of # &gt;=14 Trial of # &gt;=14 20 Coin of or 20 Coin of or 20 Coin of or Flips Heads &lt;=6 Flips Heads &lt;=6 Flips Heads &lt;=6 1 11 no 10 10 no 19 13 no 2 12 no 11 10 no 20 10 no 3 8 no 12 10 no 21 11 no 4 12 no 13 9 no 22 14 yes 5 12 no 14 9 no 23 9 no 6 7 no 15 12 no 24 7 no 7 9 no 16 7 no 25 10 no 8 8 no 17 14 yes 9 6 yes 18 12 no Now let us obtain larger and more accurate simulation samples with the computer. The key step in the RESAMPLING STATS program “Fruitfly” is to GENERATE 20 numbers with two equally-likely outcomes: “1” to stand for a male, and “2” to stand for a female. This simulates randomly choosing 20 fruit flies on the benchmark assumption—the “null hypothesis”— that each fruit fly has an equal chance of being a male or female. Now we want to discover the chances of getting more than 13 (i.e., 14 or more) males or more than 13 females under these conditions. So we COUNT the number of males in each random sample and then keep SCORE of this number for each sample. After one thousand samples have been drawn, we COUNT how often there were more than 13 males and then count the number of times there were fewer than 7 males (because if there were fewer than 7 males there must have been more than 13 females). When we ADD the two results together we have the probability that the results obtained from the sample of irradiated fruit flies would be obtained from a random sample of fruit flies. REPEAT 1000 Do 1000 experiments GENERATE 20 1,2 a Generate randomly 20 “1’s” and “2’s,” put them in a. COUNT a =1 b Count the number of “1”s (males), put that result in b. SCORE b z Keep track of each trial result in z. END End one trial, go back and repeat until all 1000 trials are complete. HISTOGRAM z Produce a histogram of the trial results. 20 Fruitflies # males In the histogram above, we see that in 16 percent of the trials, the number of males was 14 or more, or 6 or fewer. Or instead of reading the results from the histogram, we can calculate the result by tacking on the following commands to the above program: COUNT z &gt;=14 j Determine the number of trials in which we had 14 or more males. COUNT z &lt;=6 k Determine the number of trials in which we had 6 or fewer males. ADD j k m Add the two results together. DIVIDE m 1000 mm Convert to a proportion. PRINT mm Print the results. Note: The file “fruitfly” on the Resampling Stats software disk contains this set of commands. Notice that the strength of the evidence for the effectiveness of the radiation treatment depends upon the original question: whether or not the treatment had any effect on the sex of the fruit fly, which is a two-tailed question. If there were reason to believe at the start that the treatment could increase only the number of males , then we would focus our attention on the result that in only two of our first twenty-five trials were fourteen or more males. There would then be only a 2/25 = .08 probability of getting the observed results by chance if the treatment really has no effect, rather than the weaker odds against obtaining fourteen or more of either males or females. Therefore, whether you decide to figure the odds of just fourteen or more males (what is called a “one-tail test”) or the odds for fourteen or more males plus fourteen or more females (a “two-tail test”), depends upon your advance knowledge of the subject. If you have no reason to believe that the treatment will have an effect only in the direction of creating more males and if you figure the odds for the one-tail test anyway, then you will be kidding yourself. Theory comes to bear here. If you have a strong hypothesis, deduced from a strong theory, that there will be more males, then you should figure one-tail odds, but if you have no such theory you should figure the weaker two-tail odds. In the case of the next problem concerning calves, we shall see that a one-tail test is appropriate because we have no interest in producing more male calves. Before leaving this example, let us review our intellectual strategy in handling the problem. First we observe a result (14 males in 20 flies) which differs from the proportion of the benchmark population (50 percent males). Because we have treated this sample with irradiation and observed a result that differs from the untreated benchmark-population’s mean, we speculate that the irradiation caused the sample to differ from the untreated population. We wish to check on whether this speculation is correct. When asking whether this speculation is correct, we are implicitly asking whether future irradiation would also produce a proportion of males higher than 50 percent. That is, we are implicitly asking whether irradiated flies would produce more samples with male proportions as high as 14/20 than would occur by chance in the absence of irradiation. If samples as far away as 14/20 from the benchmark population mean of 10/20 would occur frequently by chance, then we would not be impressed with that experimental evidence as proof that irradiation does affect the sex ratio. Hence we set up a model that will tell us the frequency with which samples of 14 or more males out of 20 births would be observed by chance. Carrying out the resampling procedure tells us that perhaps a tenth of the time such samples would be observed by chance. That is not extremely frequent, but it is not infrequent either. Hence we would probably conclude that the evidence is provocative enough to justify further experimentation, but not so strong that we should immediately believe in the truth of this speculation. The logic of attaching meaning to the probabilistic outcome of a test of a hypothesis is discussed in Chapter 16. There also is more about the concept of the level of significance in Chapter 16. Because of the great importance of this sort of case, which brings out the basic principles particularly clearly, let us consider another example: Example 15-2: Does the Bio-Engineer’s Treatment Increase the Female Calf Rate? What is the probability that among 10 calves born, 9 or more will be female? Let’s consider this question in the context of a set of queries for performing statistical inference that will be discussed further in Chapter 19. The question (From Hodges and Lehman): Female calves are more valuable than males. A bio-engineer claims to be able to cause more females to be born than the expected 50 percent rate. He conducts his procedure, and nine females are born out of the next 10 pregnancies among the treated cows. Should you believe his claim? That is, what is the probability of a result this (or more) surprising occurring by chance if his procedure has no effect? In this problem, we assume that on average 100 of 206 births are female, in contrast to the 50-50 benchmark universe in the previous problem. What is the purpose of the work? Female calves are more valuable than male calves. Statistical inference? Yes. Confidence interval or Test of hypothesis? Test of hypothesis. Will you state the costs and benefits of various outcomes, or a loss function? Yes. One need only say that the benefits are very large, and if the results are promising, it is worth gathering more data to confirm results. How many samples of data are part of the hypothesis test? One. What is the size of the first sample about which you wish to make significance statements? Ten. What comparison(s) to make? Compare the sample to the benchmark universe. What is the benchmark universe that embodies the null hypothesis? 100/206 female. Which symbols for the observed entities? Balls in bucket, or numbers. What values or ranges of values ? “1-100” and “101-206.” Finite or infinite universe ? Infinite. Which sample(s) do you wish to compare to which, or to the null universe (and perhaps to the alternative universe)? Ten calves. What procedure to produce the sample entities? Sampling with replacement. Simple (single step) or complex (multiple “if” drawings)? Can think of it either way. What to record as the outcome of each resample trial? The proportion (or number) of females. What is the criterion to be used in the test? The probability that in a sample of ten calves, nine (or more) females would be drawn by chance from the benchmark universe of 100/206 females. “One tail” or “two tail” test? One tail, because the farmer is only interested in females. Finding a large proportion of males would not be of interest; it would not cause rejecting the null hypothesis. The actual computation of probability may be done in several ways, as discussed earlier for four children and for ten cows. Conventional methods are discussed for comparison in Chapter 19. Here is the resampling solution in RESAMPLING STATS: REPEAT 15000 GENERATE 10 1,206 a COUNT a BETWEEN 101 206 b SCORE b z END HISTOGRAM z COUNT z &gt;=9 k DIVIDE k 15000 kk PRINT kk 10 Calves # females Result: kk = 0.013867 We read from the result in vector kk in the “calves” program that the probability of 9 or 10 females occurring by chance is a bit more than one percent. Example 15-3: A Public-Opinion Poll (Is the Propor tion of a Population Greater Than a Given Value?) (Program “CABLEPOL”) A municipal official wants to determine whether a majority of the town’s residents are for or against the awarding of a cable-television franchise, and he asks you to take a poll. You judge that the telephone book is a fair representation of the universe in which the politician was interested, and you therefore decided to interview by telephone. Of a sample of fifty people who expressed opinions, thirty said “yes” they were for the plan and twenty said “no,” they were against it. How conclusively do the results show that the people in town want cable television? Now comes some necessary subtle thinking in the interpretation of what seems like a simple problem. Notice that our aim in the analysis is to avoid the mistake of saying that the town favors the plan when in fact it does not favor the plan. Our chance of making this mistake is greatest when the voters are evenly split, so we choose as the benchmark (null) hypothesis that 50 percent of the town does not want the plan. This statement really means that “50 percent or more do not want the plan.” We could assess the probability of obtaining our result from a population that is split (say) 52-48 against, but such a probability would necessarily be even smaller, and we are primarily interested in assessing the maximum probability of being wrong. If the maximum probability of error turns out to be inconsequential, then we need not worry about less likely errors. This problem is very much like the one-group fruit flyirradiation problem in Example 15-1. The only difference is that now we are comparing the observed sample against an arbitrary value of 50 percent (because that is the break-point in a situation where the majority decides) whereas in the fruitfly example we compared the observed sample against the normal population proportion (also 50 percent, because that is the normal proportion of males). But it really does not matter why we are comparing the observed sample to the figure of 50 percent; the procedure is the same in both cases. (Please notice that there is nothing special about the 50 percent figure; the same procedure would be followed for 20 percent or 85 percent.) In brief, we a) designate “1-5” as “no” in the random-number table, “6-0” as “yes,” b) count the number of “yeses” and “noes” in the first fifty numbers, c) repeat for perhaps a hundred trials, then d) count the proportion of the trials in which a 50-50 universe would produce thirty or more “yes” answers. In operational steps, the procedure is as follows: Step 1. “1-5” = no, “6-0” = yes. Step 2. In 50 random numbers, count the “yeses,” and record “false positive” if 30 or more “yeses.” Step 3. Repeat step 2 perhaps 100 times. Step 4. Calculate the proportion of experimental trials showing “false positive.” This estimates the probability that as many as 30 “yeses” would be observed by chance in a sample of 50 people if half (or more) are really against the plan. In Table 15-2, we see the results of twenty trials; 4 of 20 times (20 percent) 30 or more “yeses” were observed by chance. So our “significance level” or “prob value” is 20 percent, which is normally too high to feel confident that our poll results are reliable. This is the probability that as many as thirty of fifty people would say “yes” by chance if the population were “really” split evenly. (If the population were split so that more than 50 percent were against the plan, the probability would be even less that the observed results would occur by chance. In this sense, the benchmark hypothesis is conservative). On the other hand, if we had been counting “1-5” as “yes” instead of “no,” there would only have been one “false positive.” This indicates how samples can vary just by chance. Table 15-2 Results of Twenty Random Trials for Problem “Cablepol” Trials With Trial Number of “Noes” Number of “Yeses” &gt;=30 “Yeses” 1 23 27 2 25 25 3 26 24 4 22 28 5 22 28 6 20 30  + 7 25 25 8 21 29 9 28 22 10 19 31  + 11 28 22 12 19 31  + 13 18 32  + 14 23 27 15 34 16 16 27 23 17 22 28 18 26 24 19 28 22 20 27 23 Taken together, the evidence suggests that the mayor would be wise not to place very much confidence in the poll results, but rather ought to act with caution or else take a larger sample of voters. The RESAMPLING STATS program “Cablepol” GENERATEs samples of 50 simulated voters on the assumption that only 50 percent are in favor of the franchise. Then it COUNTs the number of samples where over 29 (30 or more) of the 50 respondents said they were in favor of the franchise. (That is, we use a “one-tailed test.”) The result in KK is the chance of a “false positive,” that is, 30 or more people saying they favor a franchise when support for the proposal is actually split evenly down the middle. REPEAT 1000 Do 1000 trials. GENERATE 50 1,2 a Generate randomly 50 “1”s and “2”s, put them in a. Let “1” = “yes” and “2” = “no.” END COUNT a =1 b Count the number of yeses, put the result in b. SCORE b z Keep track of each trial result in z. End the trial, go back and repeat until all 1000 trials are complete, then proceed. HISTOGRAM z Produce a histogram of the trial results. In the histogram below, we see that 11 percent of our trials had 30 or more voters in favor, despite the fact that they were drawn from a population that was split 50-50. RESAMPLING STATS will calculate this proportion directly if we add the following commands to the above: COUNT z &gt;= 30 k Determine how many trials had 30 or more in favor. DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the result. Note: The file “cablepol” on the Resampling Stats software disk contains this set of commands. Samples of 50 Voters # voters in favor The section above discusses testing hypotheses about a single sample of counted data relative to a benchmark universe. This section discusses the issue of whether two samples with counted data should be considered the same or different. Example 15-4: Did the Bush-Dukakis Poll Indicate that Bush Would Win? What is the probability that a sample outcome such as actually observed (840 Bush, 660 Dukakis) would occur by chance if Dukakis is “really” ahead—that is, if Dukakis has 50 percent (or more) of the support? To restate in sharper statistical language: What is the probability that the observed sample or one even more favorable to Bush would occur if the universe has a mean of 50 percent or below? Here is a procedure that responds to that question: Create a benchmark universe with one ball marked “B” and another marked “D.” Draw a ball, record its marking, and replace. (We sample with replacement to simulate the practically-infinite population of U. S. voters.) Repeat step 2 1500 times and count the number of “B”s. If 840 or greater, record “Y”; otherwise, record “N.” Repeat steps 3 and 4 perhaps 1000 or 10,000 times, and count the number of “Y”s. The outcome estimates the probability that 840 or more Bush choices would occur if the universe is “really” half or more in favor of Dukakis. This procedure may be done as follows with a computer and Resampling Stats (program “bush-hyp”): REPEAT 1000 GENERATE 1500 1,2 a Generate 1500 “1’s” and “2’s,” selected randomly, letting “1” = Bush COUNT a =1 b Count Bush votes SCORE b z Keep score END HISTOGRAM z COUNT z &gt;=840 k How often &gt;= 840 Bush votes in random draw? DIVIDE k 1000 kk As a proportion of simulated resamples PRINT kk The result was kk = 0. This implies shows a probability so small as not to occur once in a thousand times that Bush’s “victory” in the sample would occur by chance if he really were behind. The results of the various runs may be seen in the histogram and printout following. MAXSIZE a 1500 Enlarge vector a’s capacity to 1500 (default is 1000 in some versions of Resampling Stats) REPEAT 1000 GENERATE 1500 1,2 a Generate 50 “1’s” and “2’s,” selected randomly, letting “1” = Bush COUNT a =1 b Count Bush votes DIVIDE b 1500 c Find proportion pro-Bush SCORE b z Keep score END HISTOGRAM z COUNT z &gt;=.56 k How often &gt;= .56 Bush votes in random draw? DIVIDE k 1000 kk As a proportion of simulated resamples PRINT kk Samples of 1500 Voters propor tion voters for Bush Bin Center Freq Pct Cum Pct 0.465 4 0.4 0.4 0.47 14 1.4 1.8 0.475 28 2.8 4.6 0.48 46 4.6 9.2 0.485 97 9.7 18.9 0.49 106 10.6 29.5 0.495 159 15.9 45.4 0.5 121 12.1 57.5 0.505 157 15.7 73.2 0.51 102 10.2 83.4 0.515 75 7.5 90.9 0.52 39 3.9 94.8 0.525 35 3.5 98.3 0.53 11 1.1 99.4 0.535 5 0.5 99.9 0.54 1 0.1 100 Example 15-5: Comparison of Possible Cancer Cure to Placebo Effect (Do Two Binomial Populations Differ in Their Propor tions) (Program “CANCER”). Example 15-1 used an observed sample of male and female fruitflies to test the benchmark (null) hypothesis that the flies came from a universe with a one-to-one sex ratio, and the poll data problem also compared results to a 50-50 hypothesis. The calves problem also compared the results to a single benchmark universe—a proportion of 100/206 females. Now we want to compare two samples with each other , rather than comparing one sample with a hypothesized universe. That is, in this example we are not comparing one sample to a benchmark universe, but rather asking whether both samples come from the same universe. The universe from which both samples come, if both belong to the same universe, may be thought of as the benchmark universe, in this case. The scientific question is whether pill P cures a rare cancer. A researcher gave pill P to six patients selected randomly from a group of twelve cancer patients; of the six, five got well. He gave an inactive placebo to the other six patients, and two of them got well. Does the evidence justify a conclusion that the pill has a curative effect? (An identical statistical example would serve for an experiment on methods of teaching reading to children. In such a situation the researcher would respond to inconclusive results by running the experiment on more subjects, but in cases like the cancer-pill example the researcher often cannot obtain more subjects.) We can answer the stated question by combining the two samples and testing both samples against the resulting combined universe. In this case, the universe is twelve subjects, seven (5 + 2) of whom got well. How likely would such a universe produce two samples as far apart as five of six, and two of six, patients who get well? In other words, how often will two samples of six subjects, each drawn from a universe in which 7/12 of the patients get well, be as far apart as 5 – 2 = 3 patients in favor of the sample designated “pill”? This is obviously a one-tail test, for there is no reason to believe that the pill group might do less well than the placebo group. We might construct a twelve-sided die, seven of whose sides are marked “get well.” Or we would use pairs of numbers from the random-number table, with numbers “01-07” corresponding to get well, numbers “08-12” corresponding to “not get well,” and all other numbers omitted. (If you wish to save time, you can work out a system that uses more numbers and skips fewer, but that is up to you.) Designate the first six subjects “pill” and the next six subjects “placebo.” The specific procedure might be as follows: Step 1. “01-07” = get well, “08-12” = do not get well Step 2. Select two groups, A and B, each with six random numbers from “01” to “12.” Step 3. Record how many “get well” in each group. Step 4. Subtract the result in group A from that in group B (the difference may be negative). Step 5. Repeat steps 1-4 perhaps 100 times. Step 6. Compute the proportion of trials in which the pill does better by three or more cases. In the trials shown in Table 15-3, in three cases (12 percent) the difference between the randomly-drawn groups is three cases or greater. Apparently it is somewhat unusual—it happens 12 percent of the time—for this universe to generate “pill” samples in which the number of recoveries exceeds the number in the “placebo” samples by three or more. Therefore the answer to the scientific question, based on these samples, is that there is some reason to think that the medicine does have a favorable effect. But the investigator might sensibly await more data before reaching a firm conclusion about the pill’s efficiency, given the 7 to 1 odds (12 percent probability). Table 15-3 Results of 25 Random Trials for Probability “Cancer” Trial Pill Cures Placebo Cures Difference 1 4 4 0 2 3 5 -2 3 4 3 1 4 * 5 2 3 5 4 3 1 6 2 5 -3 7 4 4 0 8 4 5 -1 9 4 4 0 10 * 5 2 3 11 4 5 -1 12 5 3 2 13 3 5 -2 14 3 2 1 15 3 4 -1 16 5 4 1 17 * 6 3 3 18 4 5 -1 19 3 4 -1 20 2 3 -1 21 4 4 0 22 4 4 0 23 3 5 -2 24 3 3 0 25 3 3 0 Now for a RESAMPLING STATS solution. Again, the benchmark hypothesis is that pill P has no effect, and we ask how often, on this assumption, the results that were obtained from the actual test of the pill would occur by chance. Given that in the test 7 of 12 patients overall got well, the benchmark hypothesis assumes 7/12 to be the chances of any random patient being cured. We GENERATE two similar samples of 6 patients, both taken from the same universe composed of the combined samples—the bootstrap procedure. Letting the numbers “1” through “7” denote patients who got well and “8-12” denote persons not getting well, we COUNT the number who got well in each sample. Then we SUBTRACT the number who got well in the “pill” sample from the number who got well in the “no-pill” sample. We SCORE the resulting difference for each trial in Z. In the actual test, 3 more patients got well in the sample given the pill than in the sample given the placebo. We therefore count how many of the trials yield results where the difference between the sample given the pill and the sample not given the pill was greater than 2 (equal to or greater than 3). This result is the probability that the results derived from the actual test would be obtained from random samples drawn from a population which has a constant cure rate, pill or no pill. REPEAT 1000 Do 1000 experiments. GENERATE 6 1,12 a Randomly generate 6 numbers between “1” and “12.” Let “1-7” = cure, “8-12” = no cure. This will be the “medicine” group. GENERATE 6 1,12 b Similarly for the “placebo” group. COUNT a between 1 7 aa Count the number of cures in the trial “medicine” group. (“Medicine” is in quotes because the vector a is an arbitrary random selection of our experiment—one we know has no medicinal value because the cure rate—7/12—is the same as for the “placebo” experimental group.) COUNT b between 1 7 bb Count the number of cures in the trial “placebo” group. SUBTRACT aa bb c Subtract trial “placebo” cures from trial “medicine” cures. SCORE c z Keep track of each trial result in z. END End one experiment, go back and repeat until 1000 are complete, then proceed. HISTOGRAM z Produce a histogram of the trial results. Cancer 12 Treatments (6 Medicine, 6 Placebo) excess cures (“medicine”—“placebo”) Recall our actual observed results: In the medicine group, three more patients were cured than in the placebo group. From the histogram, we see that in only 8 percent of the simulated trials did the “medicine” group do as well or better. The results seem to suggest—but by no means conclusively—that the medicine’s performance is not due to chance. Further study would probably be warranted. The following commands added to the above program will calculate this proportion directly: COUNT z &gt;=3 k DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the result. Note: The file “cancer” on the Resampling Stats software disk contains this set of commands. As I wrote when I first proposed this bootstrap method in 1969, this method is not the standard way of handling the problem; it is not even analogous to the standard analytic difference-of-proportions method (though since then it has become widely accepted). Though the method shown is quite direct and satisfactory, there are also many other resampling methods that one might construct to solve the same problem. By all means, invent your own statistics rather than simply trying to copy the methods described here; the examples given here only illustrate the process of inventing statistics rather than offering solutions for all classes of problems. Example 15-6: Did Attitudes About Marijuana Change? Consider two polls, each asking 1500 Americans about marijuana legalization. One poll, taken in 1980, found 52 percent of respondents in favor of decriminalization; the other, taken in 1985, found 46 percent in favor of decriminalization (Wonnacott and Wonnacott 1990) (p. 275). Our null (benchmark) hypothesis is that both samples came from the same universe (the universe made up of the total of the two sets of observations). If so, let us then ask how likely would be two polls to produce results as different as were observed? Hence we construct a universe with a mean of 49 percent (the mean of the two polls of 52 percent and 46 percent), and repeatedly draw pairs of samples of size 1500 from it. To see how the construction of the appropriate question is much more challenging intellectually than is the actual mathematics, let us consider another possibility suggested by a student: What about considering the universe to be the earlier poll with a mean of 52 percent, and then asking the probability that the later poll of 1500 people with a mean of 46 percent would come from it? Indeed, on first thought that procedure seems reasonable. Upon reflection—and it takes considerable thought on these matters to get them right—that would not be an appropriate procedure. The student’s suggested procedure would be the same as assuming that we had long-run solid knowledge of the universe, as if based on millions of observations, and then asking about the probability of a particular sample drawn from it. That does not correspond to the facts. The only way to find the approach you eventually consider best—and there is no guarantee that it is indeed correct —is by close reference to the particular facts of the case. Example 15-7: Infarction and Cholesterol: Framingham Study It is so important to understand the logic of hypothesis tests, and of the resampling method of doing them, that we will now tackle another problem similar to the preceding one. This will be the first of several problems that use data from the famous Framingham study (drawn from Kahn and Sempos, 1989), concerning the development of myocardial infarction 16 years after the Framingham study began, for men ages 35- 44 with serum cholesterol above 250, compared to those with serum cholesterol below 250. The raw data are shown in Table 15-4 (divided into “high” and “low” cholesterol by Kahn and Sempos). Table 15-4 Development of Mycardial Infarction in Framingham After 16 Years Men Age 35-44, by Level of Serum Cholesterol Serum Cholesterol Developed MI Didn’t Develop MI Total &gt; 250 10 125 135 &lt;= 250 21 449 470 Source: Shurtleff, D. The Framingham Study: An Epidemiologic Investigation of Cardiovascular Disease , Section 26. Washington, DC, U.S. Government Printing Office. Cited in Kahn and Sempos (1989), p. 61, Table 3-8 The statistical logic properly begins by asking: How likely is that the two observed groups “really” came from the same “population” with respect to infarction rates? That is, we start with this question: How sure should one be that there is a difference in myocardial infarction rates between the highand low-cholesterol groups? Operationally, we address this issue by asking how likely it is that two groups as different in disease rates as the observed groups would be produced by the same “statistical universe.” Key step: We assume that the relevant “benchmark” or “nullhypothesis” population (universe) is the composite of the two observed groups. That is, if there were no “true” difference in infarction rates between the two serum-cholesterol groups, and the observed disease differences occurred just because of sampling variation, the most reasonable representation of the population from which they came is the composite of the two observed groups. Therefore, we compose a hypothetical “benchmark” universe containing (135 + 470 =) 605 men at risk, and designate (10 + 21 =) 31 of them as infarction cases. We want to determine how likely it is that a universe like this one would produce—just by chance—two groups that differ as much as do the actually observed groups. That is, how often would random sampling from this universe produce one sub-sample of 135 men containing a large enough number of infarctions, and the other sub-sample of 470 men producing few enough infarctions, that the difference in occurrence rates would be as high as the observed difference of .029? (10/135 = .074, and 21/470 = .045, and .074—.045 = .029). So far, everything that has been said applies both to the conventional formulaic method and to the “new statistics” resampling method. But the logic is seldom explained to the reader of a piece of research—if indeed the researcher her/ himself grasps what the formula is doing. And if one just grabs for a formula with a prayer that it is the right one, one need never analyze the statistical logic of the problem at hand. Now we tackle this problem with a method that you would think of yourself if you began with the following mind-set: How can I simulate the mechanism whose operation I wish to understand? These steps will do the job: Step 1: Fill a bucket with 605 balls, 31 red (infarction) and the rest (605—31 = 574) green (no infarction). Step 2: Draw a sample of 135 (simulating the high serum-cholesterol group), one ball at a time and throwing it back after it is drawn to keep the simulated probability of an infarction the same throughout the sample; record the number of reds. Then do the same with another sample of 470 (the low serum-cholesterol group). Step 3: Calculate the difference in infarction rates for the two simulated groups, and compare it to the actual difference of .029; if the simulated difference is that large, record “Yes” for this trial; if not, record “No.” Step 4: Repeat steps 2 and 3 until a total of (say) 400 or 1000 trials have been completed. Compute the frequency with which the simulated groups produce a difference as great as actually observed. This frequency is an estimate of the probability that a difference as great as actually observed in Framingham would occur even if serum cholesterol has no effect upon myocardial infarction. The procedure above can be carried out with balls in a bucket in a few hours. Yet it is natural to seek the added convenience of the computer to draw the samples. Here is a RESAMPLING STATS program (“myocar1”): URN 31#1 574#2 men An bucket called “men” with 31 “1’s” (=infarctions) and 574 “2’s” (=no infarction) REPEAT 1000 SAMPLE 135 men high Sample (with replacement!) 135 of the numbers in this bucket, give this group the name “high” SAMPLE 470 men low Same for a group of 470, call it “low” COUNT high =1 a Count infarctions in first group DIVIDE a 135 aa Express as a proportion COUNT low =1 b Count infarctions in second group DIVIDE b 470 bb Express as a proportion SUBTRACT aa bb c Find the difference in infarction rates SCORE c z Keep score of this difference END HISTOGRAM z COUNT z &gt;=.029 k How often was the resampled difference &gt;= the observed difference? DIVIDE k 1000 kk Convert this result to a proportion PRINT kk Difference between paired resamples (propor tion with infarction) Result: kk = 0.102 (the proportion of resample pairs with a difference &gt;= .029) The results of the test using this program may be seen in the histogram. We find—perhaps surprisingly—that a difference as large as observed would occur by chance fully 10 percent of the time. (If we were not guided by the theoretical expectation that high serum cholesterol produces heart disease, we might include the 10 percent difference going in the other direction, giving a 20 percent chance). Even a ten percent chance is sufficient to call into question the conclusion that high serum cholesterol is dangerous. At a minimum, this statistical result should call for more research before taking any strong action clinically or otherwise. Where should one look to determine which procedures should be used to deal with a problem such as set forth above? Unlike the formulaic approach, the basic source is not a manual which sets forth a menu of formulas together with sets of rules about when they are appropriate. Rather, you consult your own understanding about what is happening in (say) the Framingham situation, and the question that needs to be answered, and then you construct a “model” that is as faithful to the facts as is possible. The bucket-sampling described above is such a model for the case at hand. To connect up what we have done with the conventional approach, one could apply a z test (conceptually similar to the t test, but applicable to yes-no data; it is the Normal-distribution approximation to the large binomial distribution). Do so, we find that the results are much the same as the resampling result—an eleven percent probability. Someone may ask: Why do a resampling test when you can use a standard device such as a z or t test? The great advantage of resampling is that it avoids using the wrong method. The researcher is more likely to arrive at sound conclusions with resampling because s/he can understand what s/he is doing, instead of blindly grabbing a formula which may be in error. The textbook from which the problem is drawn is an excellent one; the difficulty of its presentation is an inescapable consequence of the formulaic approach to probability and statistics. The body of complex algebra and tables that only a rare expert understands down to the foundations constitutes an impenetrable wall to understanding. Yet without such understanding, there can be only rote practice, which leads to frustration and error. Example 15-8: Is One Pig Ration More Effective Than the Other? (Testing For a Difference in Means With a Two-by-Two Classification) (Program “Pigs1”) Each of two new types of ration is fed to twelve pigs. A farmer wants to know whether ration A or ration B is better. 2 The weight gains in pounds for pigs fed on rations A and B are: A: 31, 34, 29, 26, 32, 35, 38, 34, 31, 29, 32, 31 B: 26, 24, 28, 29, 30, 29, 31, 29, 32, 26, 28, 32 The statistical question may be framed as follows: should one consider that the pigs fed on the different rations come from the same universe with respect to weight gains? In the actual experiment, 9 of the 12 pigs who were fed ration A also were in the top half of weight gains. How likely is it that one group of 12 randomly-chosen pigs would contain 9 of the 12 top weight gainers? One approach to the problem is to divide the pigs into two groups—the twelve with the highest weight gains, and the twelve with the lowest weight gains—and examine whether an unusually large number of high-weight-gain pigs were fed on one or the other of the rations. We can make this test by ordering and grouping the twentyfour pigs: High-weight group: 38 (ration A), 35 (A), 34 (A), 34 (A), 32 (B), 32 (A), 32 (A), 32 (B), 31 (A), 31 (B), 31 (A), 31 (A) Low-weight group: 30 (B), 29 (A), 29 (A), 29 (B), 29 (B), 29 (B), 28 (B), 28 (B), 26 (A), 26 (B), 26 (B), 24 (B). Among the twelve high-weight-gain pigs, nine were fed on ration A. We ask: Is this further from an even split than we are likely to get by chance? Let us take twelve red and twelve black cards, shuffle them, and deal out twelve cards (the other twelve need not be dealt out). Count the proportion of the hands in which one ration comes up nine or more times in the first twelve cards, to reflect ration A’s appearance nine times among the highest twelve weight gains. More specifically: Step 1. Constitute a deck of twelve red and twelve black cards, and shuffle. Step 2. Deal out twelve cards, count the number red, and record “yes” if there are nine or more of either red or black. Step 3. Repeat step 2 perhaps fifty times. Step 4. Compute the proportion “yes.” This proportion estimates the probability sought. Table 15-4 shows the results of fifty trials. In three (marked by asterisks) of the fifty (that is, 6 percent of the trials) there were nine or more either red or black cards in the first twelve cards. Again the results suggest that it would be slightly unusual for the results to favor one ration or the other so strongly just by chance if they come from the same universe. Table 15-4 Results of Fifty Random Trials for Problem “PIGS1” Trial Blk Red Trial Blk Red Trial Blk Red 1 7 5 19 5 7 37 6 6 2 7 5 20 5 7 38 5 7 3 6 6 21 4 8 39 7 5 4 6 6 *22 9 3 40 5 7 5 6 6 23 7 5 41 6 6 6 4 8 24 5 7 42 4 8 7 6 6 25 5 7 43 7 5 8 6 6 26 7 5 44 5 7 9 5 7 27 6 6 45 8 4 10 8 4 *28 9 3 46 5 7 11 6 6 29 7 5 47 5 7 12 7 5 30 7 5 48 6 6 13 8 4 31 8 4 49 6 6 14 5 7 *32 3 9 50 6 6 15 6 6 33 5 7 16 7 5 34 6 6 17 8 4 35 5 7 18 8 4 36 5 7 Now a RESAMPLING STATS procedure to answer the question: The NUMBERS statement creates an array of numbers “1” through “24,” which will represent the rankings of weight gains for each of the 24 pigs. We REPEAT the following procedure for 1000 trials. First we SHUFFLE the elements of array A so that the rank numbers for weight gains are randomized and placed in array B. We then TAKE the first 12 elements of B and place them in C; this represents the rankings of a randomly-selected group of 12 pigs. We next COUNT in C the number of pigs whose rankings for weight gain were in the top half—that is, a rank of less than 13. We SCORE that number and END the loop. Since we did not know beforehand the direction of the effect of ration A on weight gain, we want to count the times that either more than 8 of the random selection of 12 pigs were in the top half of the rankings, or that fewer than 4 of these pigs were in the top half of the weight gain rankings—(The latter is the same as counting the number of times that more than 8 of the 12 non-selected random pigs were in the top half in weight gain.) We do so with the final two COUNT statements. By adding the two results J and K together, we have the number of times out of 1000 that differences in weight gains in two groups as dramatic as those obtained in the actual experiment would occur by chance. NUMBERS 1,24 a Constitute the set of the weight gain rank orders. A is now a vector consisting of the numbers 1—24, in that order. REPEAT 1000 Do the following experiment 1000 times. SHUFFLE a b Shuffle the ranks of the weight gains, put the shuffled ranks in b. TAKE b 1,12 c Take the first 12 ranks, put them in c COUNT c &lt;= 12 d Determine how many of these randomly selected 12 ranks are less than 12 (i.e. 1-12), put that result in d. SCORE d z Keep track of each trial result in z. END End one experiment, go back and repeat until 1000 trials are complete. HISTOGRAM z Produce a histogram of the trial results. PIGS1: Random Selection of 12 Weight Gains (24 Pigs) # of top half weight gains picked We see from the histogram that, in about 3 percent of the trials, either more than 8 or fewer than 4 top half ranks (1-12) made it into the random group of twelve that we selected. RESAMPLING STATS will calculate this for us as follows: COUNT z &gt;= 9 j Determine how many of the trials yielded 9 or more top ranks. COUNT z &lt;= 3 k Determine how many trials yielded 3 or fewer of the top ranks. ADD j k m Add the two together. DIVIDE m 1000 mm Convert to a proportion. PRINT mm Print the results. Note: The file “pigs1” on the Resampling Stats software disk contains this set of commands. The decisions that are warranted on the basis of the results depend upon one’s purpose. If writing a scientific paper on the merits of ration A is the ultimate purpose, it would be sensible to test another batch of pigs to get further evidence. (Or you could proceed to employ another sort of test for a slightly more precise evaluation.) But if the goal is a decision on which type of ration to buy for a small farm and they are the same price, just go ahead and buy ration A because, even if it is no better than ration B, you have strong evidence that it is no worse . Example 15-9: Do Planet Densities Differ? Consider the five planets known to the ancient world. Mosteller and Rourke (1973) (pp. 17-19) ask us to compare the densities of the three planets farther from the sun than is the earth (Mars, density 0.71; Jupiter, 0.24; and Saturn, 0.12) against the densities of the planets closer to the sun than is the earth (Mercury, 0.68; Venus, 0.94). The average density of the distant planets is .357, of the closer planets is .81. Is this difference (.353) statistically significant, or is it likely to occur in a chance ordering of these planets? We can answer this question with a permutation test; such sampling without replacement makes sense here because we are considering the entire set of planets, rather than a sample drawn from a larger population of planets (the word “population” is used here, rather than “universe,” to avoid confusion.) And because the number of objects is so small, one could examine all possible arrangements (permutations), and see how many have (say) differences in mean densities between the two groups as large as observed. Another method that Mosteller and Rourke suggest is by a comparison of the density ranks of the two sets, where Saturn has rank 1 and Venus has rank 5. This might have a scientific advantage if the sample data are dominated by a single “outlier,” whose domination is removed when we rank the data. We see that the sum of the ranks for the “closer” set is 3+5=8. We can then ask: If the ranks were assigned at random, how likely is it that a set of two planets would have a sum as large as 8? Again, because the sample is small, we can examine all the possible permutations, as Mosteller and Rourke do in Table 3-1 (p. 56) (Substitute “Closer” for “B,” “Further” for “A”). In two of the ten permutations, a sum of ranks as great as 8 is observed, so the probability of a result as great as observed happening by chance is 20 percent, using these data. (We could just as well consider the difference in mean ranks between the two groups—(8/2 – 7/3 =) 1.6.) To illuminate the logic of this test, consider comparing the heights of two samples of trees. If sample A has the five tallest trees, and sample B has the five shortest trees, the difference in mean ranks will be (6+7+8+9+10=) 40—(1+2+3+4+5=) 15, the largest possible difference. If the groups are less sharply differentiated—for example, if sample A has #3 and sample B has #8—the difference in ranks will be less than the maximum of 40, as you can quickly verify. The method we have just used is called a Mann-Whitney test, though that label is usually applied when the data are too many to examine all the possible permutations; in that case one conventionally uses a table prepared by formula. In the case where there are too many for a complete permutation test, our resampling algorithm is as follows (though we’ll continue with the planets example): Compute the mean ranks of the two groups. Calculate the difference between the means computed in step 1. Create a bucket containing the ranks from 1 to the number of observations (5, in the case of the planets), and shuffle them. Since we are working with the ranked data, we must draw without replacement, because there can only be one #3, one #7, and so on. So draw the number of observations corresponding to the number of observations—2 “Closer” and 3 “Further.” Compute the mean ranks of the two simulated groups of planets. Calculate the difference between the means computed in step 5 and record. Repeat steps 3 to 7 perhaps 1000 times. Count how often the shuffled difference in ranks exceeds the observed difference of 1.6. NUMBERS 1,5 ranks step 3 above REPEAT 1000 step 7 END step 7 SHUFFLE ranks ranks$ step 3 TAKE ranks$ 1,2 closer step 4 TAKE ranks$ 3,5 further step 4 MEAN closer m_close step 5 MEAN farther m_far step 5 SUBTRACT m_close m_far diff step 6 SCORE diff z step 6 COUNT z &gt;=1.6 k step 8 DIVIDE k 1000 prob PRINT prob Result: prob = 0.21 Interpretation: 21 percent of the time, random shufflings produced a difference in ranks as great as or greater than observed. Hence, on the strength of this evidence, we should not conclude that there is a statistically significant difference in densities between the faurther planets and the closer planets. This chapter has begun the actual work of testing hypotheses. The next chapter continues with discussion of somewhat more complex problems with counted data—more complex to think about, but no more difficult to actually treat mathematically with resampling simulation. If you have understood the general logic of the procedures used up until this point, you are in command of all the necessary conceptual knowledge to construct your own tests to answer any statistical question. A lot more practice, working on a variety of problems, obviously would help. But the key elements are simple: 1) Model the real situation accurately, 2) experiment with the model, and 3) compare the results of the model with the observed results. References "],["the-concept-of-statistical-significance-in-testing-hypotheses.html", "19 The Concept of Statistical Significance in Testing Hypotheses 19.1 The logic of hypothesis tests 19.2 The concept of statistical significance", " 19 The Concept of Statistical Significance in Testing Hypotheses This chapter offers an interpretation of the meaning of the concept of statistical significance and the term “significant” in connection with the logic of significance tests. It also discusses the concept of “level of significance.” 19.1 The logic of hypothesis tests Let’s address the logic of hypothesis tests by considering a variety of examples in everyday thinking: Consider the nine-year-old who tells the teacher that the dog ate the homework. Why does the teacher not accept the child’s excuse? Clearly it is because the event would be too “unusual.” But why do we think that way? Let’s speculate that you survey a million adults, and only three report that they have ever heard of a real case where a dog ate somebody’s homework. You are a teacher, and a student comes in without homework and says that a dog ate the homework. It could have happened—your survey reports that it really has happened in three lifetimes out of a million. But the event happens only very infrequently . Therefore, you probably conclude that because the event is so unlikely, something else must have happened—and the likeliest alternative is that the student did not do the homework. The logic is that if an event seems very unlikely, it would therefore surprise us greatly if it were to actually happen, and therefore we assume that there must be a better explanation. This is why we look askance at unlikely coincidences when they are to someone’s benefit. Chapter 16—The Concept of Statistical Significance in Testing Hypotheses 239 The same line of reasoning was the logic of John Arbuthnot’s hypothesis test about the ratio of births by sex in the first published hypothesis test, though his extension of his logic to God’s design as an alternative hypothesis goes beyond the standard modern framework. It is also the implicit logic in the research on puerperal fever, cholera, and beri-beri, the data for which were shown in Chapter 11, though no explicit mention was made of probability in those cases. Two students sat next to each other at an ACT college-entrance examination in Kentucky in 1987. Out of 219 questions, 211 of the answers were identical, including many that were wrong. Student A was a high school athlete in Kentucky who had failed two previous SAT exams, and Student B thought he saw Student A copying from him. Should one believe that Student A cheated? ( The Washington Post , April 19, 1992, p. D2.) You say to yourself: It would be most unlikely that the two test-takers would answer that many questions identically by chance—and we can compute how unlikely that event would be. Because that event is so unlikely, we therefore conclude that one or both cheated. And indeed, the testing service invalidated the athlete’s exam. On the other hand, if all the questions that were answered identically were correct , the result might not be unreasonable. If we knew in how many cases they made the same mistakes , the inquiry would have been clearer, but the newspaper did not contain those details. The court is hearing a murder case. There is no eye-witness, and the evidence consists of such facts as the height and weight and age of the person charged, and other circumstantial evidence. Only one person in 50 million has such characteristics, and you find such a person. Will you convict the person, or will you believe that the evidence was just a coincidence? Of course the evidence might have occurred by bad luck, but the probability is very, very small (1 in 50 million). Will you therefore conclude that because the chance is so small, it is reasonable to assume that the person charged committed the crime? Sometimes the unusual really happens—the court errs by judging that the wrong person did it, and that person goes to prison or even is executed. The best we can do is to make the criterion strict: “Beyond a reasonable doubt.” (People ask: What 240 Resampling: The New Statistics probability does that criterion represent? But the court will not provide a numerical answer.) Somebody says to you: I am going to deal out five cards and it will be a royal flush—ten, jack, queen, king, and ace of the same suit. The person deals the cards and lo and behold! the royal flush appears. Do you think the occurrence happened just by chance? No, you are likely to be very dubious that it happened by chance. Therefore, you believe there must be some other explanation—that the person fixed the cards, for example. Note: You don’t attach the same meaning to any other permutation (say 3, 6, 7, 7, and king of various suits), even though that permutation is just as rare— unless the person announced exactly that permutation in advance. Indeed, even if the person says nothing , you will be surprised at a royal flush, because this hand has meaning , whereas another given set of five cards do not have any special meaning. You see six Volvos in one home’s driveway, and you conclude that it is a Volvo club meeting, or a Volvo salesperson’s meeting. Why? Because it is unlikely that six people not connected formally by Volvo ownership would be friends of the same person. Two important points complicate the concept of statistical significance: With a large enough sample, every treatment or variable will seem different from every other. Two faces of even a good die (say, “1” and “2”) will produce different results in the very very long run. Statistical significance does not imply economic or social significance. Two faces of a die may be statistically different in a huge sample of throws, but a 1/10,000 difference between them is too small to make an economic difference in betting. Statistical significance is only a filter . If it appears, one should then proceed to decide whether there is substantive significance. Interpreting statistical significance is sometimes complex, especially when the interpretation depends heavily upon your prior expectations—as it often does. For example, how should a basketball coach decide whether or not to bench a player for poor performance after a series of missed shots at the basket? Chapter 16—The Concept of Statistical Significance in Testing Hypotheses 241 Consider Coach John Thompson who, after Charles Smith missed 10 of 12 shots in the 1989 Georgetown-Notre Dame NCAA game, took Smith out of the game for a time ( The Washington Post , March 20, 1989, p. C1). The scientific or decision problem is: Should the coach consider that Smith is not now a 47 percent shooter as he normally is, and therefore the coach should bench him? The statistical question is: How likely is a shooter with a 47 percent average to produce 10 of 12 misses? The key issue in the statistical question concerns the total number of shot attempts we should consider. Would Coach Thompson take Smith out of the game after he missed one shot? Clearly not. Why not? Because one “expects” Smith to miss a shot half the time, and missing one shot therefore does not seem unusual. How about after Smith misses two shots in a row? For the same reason the coach still would not bench him, because this event happens “often”—more specifically, about once in every sequence of four shots. How about after 9 misses out of ten shots? Notice the difference between this case and 9 females among ten calves. In the case of the calves, we expected half females because the experiment is a single isolated trial. The event considered by itself has a small enough probability that it seems unexpected rather than expected. (“Unexpected” seems to be closely related to “happens seldom” or “unusual” in our psychology.) And an event that happens seldom seems to call for explanation, and also seems to promise that it will yield itself to explanation by some unusual concatenation of forces. That is, unusual events lead us to think that they have unusual causes; that is the nub of the matter. (But on the other hand, one can sometimes benefit by paying attention to unusual events, as scientists know when they investigate outliers.) In basketball shooting, we expect 47 percent of Smith’s individual shots to be successful, and we also expect that average for each set of shots. But we also expect some sets of shots to be far from that average because we observe many sets; such variation is inevitable. So when we see a single set of 9 misses in ten shots, we are not very surprised. But how about 29 misses in 30 shots? At some point, one must start to pay attention. (And of course we would pay more attention if beforehand, and never at any other time, the player said, “I can’t see the basket today. My eyes are dim.”) 242 Resampling: The New Statistics So, how should one proceed? Perhaps proceed the same way as with a coin that keeps coming down heads a very large proportion of the throws, over a long series of tosses: At some point you examine it to see if it has two heads. But if your investigation is negative, in the absence of an indication other than the behavior in question , you continue to believe that there is no explanation and you assume that the event is “chance” and should not be acted upon . In the same way, a coach might ask a player if there is an explanation for the many misses. But if the player answers “no,” the coach should not bench him. (There are difficulties here with truth-telling, of course, but let that go for now.) The key point for the basketball case and other repetitive situations is not to judge that there is an unusual explanation from the behavior of a single sample alone , just as with a short sequence of stock-price changes. We all need to learn that “irregular” (a good word here) sequences are less unusual than they seem to the naked intuition. A streak of 10 out of 12 misses for a 47 percent shooter occurs about 3 percent of the time. That is, about every 33 shots Smith takes, he will begin a sequence of 12 shots that will end with 3 or fewer baskets—perhaps once in every couple of games. This does not seem “very” unusual, perhaps. And if the coach treats each such case as unusual, he will be losing some of the services of a better player than he replaces him with. In brief, how hard one should search for an explanation should depend on the probability of the event. But one should (almost) assume the absence of an explanation unless one actually finds it. Bayesian analysis could be brought to bear upon the matter, bringing in your prior probabilities based on the knowledge of research that has shown that there is no such thing as a “hot hand” in basketball (see Chapter 9), together with some sort of cost-benefit error-loss calculation comparing Smith and the next best available player. Chapter 16—The Concept of Statistical Significance in Testing Hypotheses 243 19.2 The concept of statistical significance “Significance level” is a common term in probability statistics. It corresponds roughly to the probability that the assumed benchmark universe could give rise to a sample as extreme as the observed sample by chance. The results of Example 16-1 would be phrased as follows: The hypothesis that the radiation treatment affects the sex of the fruit fly offspring is accepted as true at the probability level of .16 (sometimes stated as the 16 percent level of significance). (A more common way of expressing this idea would be to say that the hypothesis is not rejected at the .16 probability level or the 16 percent level of significance. But “not rejected” and “accepted” really do mean much the same thing, despite some arguments to the contrary.) This kind of statistical work is called hypothesis testing. The question of which significance level should be considered “significant” is difficult. How great must a coincidence be before you refuse to believe that it is only a coincidence? It has been conventional in social science to say that if the probability that something happens by chance is less than 5 percent, it is significant. But sometimes the stiffer standard of 1 percent is used. Actually, any fixed cut-off significance level is arbitrary. (And even the whole notion of saying that a hypothesis “is true” or “is not true” is sometimes not useful.) Whether a one-tailed or two-tailed test is used will influence your significance level, and this is why care must be taken in making that choice. "],["the-statistics-of-hypothesis-testing-with-counted-data-part-2.html", "20 The Statistics of Hypothesis-Testing with Counted Data, Part 2 20.1 Comparisons among more than two samples of counted data 20.2 Paired Comparisons With Counted Data 20.3 Endnotes", " 20 The Statistics of Hypothesis-Testing with Counted Data, Part 2 Here’s the bad-news-good-news message again: The bad news is that the subject of inferential statistics is extremely difficult— not because it is complex but rather because it is subtle. The cause of the difficulty is that the world around us is difficult to understand, and spoon-fed mathematical simplifications which you manipulate mechanically simply mislead you into thinking you understand that about which you have not got a clue. The good news is that you—and that means you , even if you say you are “no good at math”—can understand these problems with a layperson’s hard thinking, even if you have no mathematical background beyond arithmetic and you think that you have no mathematical capability. That’s because the difficulty lies in such matters as pin-pointing the right question, and understanding how to interpret your results. The problems in the previous chapter were tough enough. But this chapter considers problems with additional complications, such as when there are more than two groups, or paired comparisons for the same units of observation. 20.1 Comparisons among more than two samples of counted data Example 17-1: Do Any of Four Treatments Affect the Sex Ratio in Fruit Flies? (When the Benchmark Universe Proportion is Known, Is the Propor tion of the Binomial Population Affected by Any of the Treatments?) (Program “4treat”) Suppose that, instead of experimenting with just one type of radiation treatment on the flies (as in Example 15-1), you try four different treatments, which we shall label A, B, C, and D. Treatment A produces fourteen males and six females, but treatments B, C, and D produce ten, eleven, and ten males, respectively. It is immediately obvious that there is no reason to think that treatment B, C, or D affects the sex ratio. But what about treatment A? A frequent and dangerous mistake made by young scientists is to scrounge around in the data for the most extreme result, and then treat it as if it were the only result. In the context of this example, it would be fallacious to think that the probability of the fourteen-males-to-six females split observed for treatment A is the same as the probability that we figured for a single experiment in Example 15-1. Instead, we must consider that our benchmark universe is composed of four sets of twenty trials, each trial having a 50-50 probability of being male. We can consider that our previous trials 1-4 in Example 15-1 constitute a single new trial, and each subsequent set of four previous trials constitute another new trial. We then ask how likely a new trial of our sets of twenty flips is to produce one set with fourteen or more of one or the other sex. Let us make the procedure explicit, but using random numbers instead of coins this time: Step 1. Let “1-5” = males, “6-0” = females Step 2. Choose four groups of twenty numbers. If for any group there are 14 or more males, record “yes”; if 13 or less, record “no.” Step 3. Repeat perhaps 1000 times. Step 4. Calculate the proportion “yes” in the 1000 trials. This proportion estimates the probability that a fruit fly population with a proportion of 50 percent males will produce as many as 14 males in at least one of four samples of 20 flies. We begin the trials with data as in Table 17-1. In two of the six simulation trials, more than one sample shows 14 or more males. Another trial shows fourteen or more females . Without even concerning ourselves about whether we should be looking at males or females, or just males, or needing to do more trials, we can see that it would be very common indeed to have one of four treatments show fourteen or more of one sex just by chance. This discovery clearly indicates that a result that would be fairly unusual (three in twenty-five) for a single sample alone is commonplace in one of four observed samples. Table 17-1 Number of “Males” in Groups of 20 (Based on Random Numbers) Trial Group A Group B Group C Group D Yes / No &gt;= 14 or &lt;= 6 1 11 12 8 12 No 2 12 7 9 8 No 3 6 10 10 10 Yes 4 9 9 12 7 No 5 14 12 13 10 Yes 6 11 14 9 7 Yes A key point of the RESAMPLING STATS program “4TREAT” is that each sample consists of four sets of 20 randomly generated hypothetical fruit flies. And if we consider 1000 trials, we will be examining 4000 sets of 20 fruit flies. In each trial we GENERATE up to 4 random samples of 20 fruit flies, and for each, we count the number of males (“1”s) and then check whether that group has more than 13 of either sex (actually, more than 13 “1”s or less than 7 “1”s). If it does, then we change J to 1, which informs us that for this sample, at least 1 group of 20 fruit flies had results as unusual as the results from the fruit flies exposed to the four treatments. After the 1000 runs are made, we count the number of trials where one sample had a group of fruit flies with 14 or more of either sex, and PRINT the results. REPEAT 1000 Do 1000 experiments. COPY (0) j j indicates whether we have obtained a trial group with 14 or more of either sex. We start at “0” (= no). REPEAT 4 Repeat the following steps 4 times to constitute 4 trial groups of 20 flies each. GENERATE 20 1,2 a Generate randomly 20 “1”s and “2”s and put them in a; let “1” = male. COUNT a =1 b Count the number of males, put the result in b. IF b &gt;= 14 If the result is 14 or more males, then END COPY (1) j Set the indicator to “1.” End the IF condition. IF b &lt;= 6 If the result is 6 or fewer males (the same as 14 or more females), then END COPY (1) j Set the indicator to “1.” End the IF condition. END END End the procedure for one group, go back and repeat until all four groups have been done. SCORE j z j now tells us whether we got a result as extreme as that observed (j = “1” if we did, j = “0” if not). We must keep track in z of this result for each experiment. End one experiment, go back and repeat until all 1000 are complete. COUNT z =1 k Count the number of experiments in which we had results as extreme as those observed. DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the result. Note: The file “4treat” on the Resampling Stats software disk contains this set of commands. In one set of 1000 trials, there were more than 13 or less than 7 males 33 percent of the time—clearly not an unusual occurrence. Example 17-2: Do Four Psychological Treatments Differ in Effectiveness? (Do Several Two-Outcome Samples Differ Among Themselves in Their Propor tions? (Program “4treat1”) Consider four different psychological treatments designed to rehabilitate juvenile delinquents. Instead of a numerical test score, there is only a “yes” or a “no” answer as to whether the juvenile has been rehabilitated or has gotten into trouble again. Label the treatments P, R, S, and T, each of which is administered to a separate group of twenty juvenile delinquents. The number of rehabilitations per group has been: P, 17; R, 10; S, 10; T, 7. Is it improbable that all four groups come from the same universe? This problem is like the placebo vs. cancer-cure problem, but now there are more than two samples. It is also like the four-sample irradiated-fruit flies example (Example 17-1), except that now we are not asking whether any or some of the samples differ from a given universe (50-50 sex ratio in that case). Rather, we are now asking whether there are differences among the samples themselves. Please keep in mind that we are still dealing with two-outcome (yes-or-no, well-or-sick) problems. Later we shall take up problems that are similar except that the outcomes are “quantitative.” If all four groups were drawn from the same universe, that universe has an estimated rehabilitation rate of 17/20 + 10/20 + 10/20 + 7/20 = 44/80 = 55/100, because the observed data taken as a whole constitute our best guess as to the nature of the universe from which they come—again, if they all come from the same universe. (Please think this matter over a bit, because it is important and subtle. It may help you to notice the absence of any other information about the universe from which they have all come, if they have come from the same universe.) Therefore, select twenty two-digit numbers for each group from the random-number table, marking “yes” for each number “1-55” and “no” for each number “56-100.” Conduct a number of such trials. Then count the proportion of times that the difference between the highest and lowest groups is larger than the widest observed difference, the difference between P and T (17-7 = 10). In Table 17-2, none of the first six trials shows anywhere near as large a difference as the observed range of 10, suggesting that it would be rare for four treatments that are “really” similar to show so great a difference. There is thus reason to believe that P and T differ in their effects. Table 7-2 Results of Six Random Trials for Problem “Delinquents” Trial P R S T Largest Minus Smallest 1 11 9 8 12 4 2 10 10 12 12 2 3 9 12 8 12 4 4 9 11 12 10 3 5 10 10 11 12 1 6 11 11 9 11 2 The strategy of the RESAMPLING STATS solution to “Delinquents” is similar to the strategy for previous problems in this chapter. The benchmark (null) hypothesis is that the treatments do not differ in their effects observed, and we estimate the probability that the observed results would occur by chance using the benchmark universe. The only new twist is that we must instruct the computer to find the groups with the highest and the lowest numbers of rehabilitations. Using RESAMPLING STATS we GENERATE four “treatments,” each represented by 20 numbers, each number randomly selected between 1 and 100. We let 1-55 = success, 56-100 = failure. Follow along in the program for the rest of the procedure: REPEAT 1000 Do 1000 trials GENERATE 20 1,100 a The first treatment group, where “1-55” = success, “56-100” = failure GENERATE 20 1,100 b The second group GENERATE 20 1,100 c The third group GENERATE 20 1,100 d The fourth group COUNT a &lt;=55 aa Count the first group’s successes COUNT b &lt;=55 bb Same for second, third &amp; fourth groups COUNT c &lt;=55 cc COUNT d &lt;=55 dd END SUBTRACT aa bb ab Now find all the pairwise differences in successes among the groups SUBTRACT aa cc ac SUBTRACT aa dd ad SUBTRACT bb cc bc SUBTRACT bb dd bd SUBTRACT cc dd cd CONCAT ab ac ad bc bd cd e Concatenate, or join, all the differences in a single vector e ABS e f Since we are interested only in the magnitude of the difference, not its direction, we take the ABSolute value of all the differences. MAX f g Find the largest of all the differences SCORE g z Keep score of the largest End a trial, go back and repeat until all 1000 are complete. COUNT z &gt;=10 k How many of the trials yielded a maximum difference greater than the observed maximum difference? DIVIDE k 1000 kk Convert to a proportion PRINT kk Note: The file “4treat1” on the Resampling Stats software disk contains this set of commands. One percent of the experiments with randomly generated treatments from a common success rate of .55 produced differences in excess of the observed maximum difference (10). An alternative approach to this problem would be to deal with each result’s departure from the mean, rather than the largest difference among the pairs. Once again, we want to deal with absolute departures, since we are interested only in magnitude of difference. We could take the absolute value of the differences, as above, but we will try something different here. Squaring the differences also renders them all positive: this is a common approach in statistics. The first step is to examine our data and calculate this measure: The mean is 11, the differences are 6, 1, 1, and 4, the squared differences are 36, 1, 1, and 16, and their sum is 54. Our experiment will be, as before, to constitute four groups of 20 at random from a universe with a 55 percent rehabilitation rate. We then calculate this same measure for the random groups. If it is frequently larger than 54, then we conclude that a uniform cure rate of 55 percent could easily have produced the observed results. The program that follows also GENER- ATES the four treatments by using a REPEAT loop, rather than spelling out the GENERATE command 4 times as above. In RESAMPLING STATS: REPEAT 1000 Do 1000 trials REPEAT 4 Repeat the following steps 4 times to constitute 4 groups of 20 and count their rehabilitation rates. GENERATE 20 1,100 a Randomly generate 20 numbers between 1 and 100 and put them in a; let 1-55 = rehabilitation, 56-100 no rehab. COUNT a between 1 55 b Count the number of rehabs, put the result in b. SCORE b w Keep track of the 4 rehab rates for the group of 20. END End the procedure for one group of 20, go back and repeat until all 4 are done. MEAN w x Calculate the mean SUMSQRDEV w x y Find the sum of squared deviations between group rehab rates (w) and the overall rate (x). SCORE y z Keep track of the result for each trial. CLEAR w Erase the contents of w to prepare for the next trial. END End one experiment, go back and repeat until all 1000 are complete. HISTOGRAM z Produce a histogram of trial results. 4 Treatments sum of squared differences From this histogram, we see that in only 1 percent of the cases did our trial sum of squared differences equal or exceed 54, confirming our conclusion that this is an unusual result. We can have RESAMPLING STATS calculate this proportion: COUNT z &gt;= 54 k Determine how many trials produced differences as great as those observed. DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the results. Note: The file “4treat2” on the Resampling Stats software disk contains this set of commands. The conventional way to approach this problem would be with what is known as a “chi-square test.” Example 17-3: Three-way Comparison In a national election poll of 750 respondents in May, 1992, George Bush got 36 percent of the preferences (270 voters), Ross Perot got 30 percent (225 voters), and Bill Clinton got 28 percent (210 voters) ( Wall Street Journal , October 29, 1992, A16). Assuming that the poll was representative of actual voting, how likely is it that Bush was actually behind and just came out ahead in this poll by chance? Or to put it differently, what was the probability that Bush actually had a plurality of support, rather than that his apparent advantage was a matter of sampling variability? We test this by constructing a universe in which Bush is slightly behind (in practice, just equal), and then drawing samples to see how likely it is that those samples will show Bush ahead. We must first find that universe—among all possible universes that yield a conclusion contrary to the conclusion shown by the data, and one in which we are interested—that has the highest probability of producing the observed sample. With a two-person race the universe is obvious: a universe that is evenly split except for a single vote against “our” candidate who is now in the lead, i.e. in practice a 50-50 universe. In that simple case we then ask the probability that that universe would produce a sample as far out in the direction of the conclusion drawn from the observed sample as the observed sample. With a three-person race, however, the decision is not obvious (and if this problem becomes too murky for you, skip over it; it is included here more for fun than anything else). And there is no standard method for handling this problem in conventional statistics (a solution in terms of a confidence interval was first offered in 1992, and that one is very complicated and not very satisfactory to me). But the sort of thinking that we must labor to accomplish is also required for any conventional solution; the difficulty is inherent in the problem, rather than being inherent in resampling, and resampling will be at least as simple and understandable as any formulaic approach. The relevant universe is (or so I think) a universe that is 35 Bush—35 Perot—30 Clinton (for a race where the poll indicates a 36-30-28 split); the 35-35-30 universe is of interest because it is the universe that is closest to the observed sample that does not provide a win for Bush (leaving out the “undecideds” for convenience); it is roughly analogous to the 50-50 split in the two-person race, though a clear-cut argument would require a lot more discussion. A universe that is split 34-34-32, or any of the other possible universes, is less likely to produce a 36-30-28 sample (such as was observed) than is a 35-35-30 universe, I believe, but that is a checkable matter. (In technical terms, it might be a “maximum likelihood universe” that we are looking for.) We might also try a 36-36-28 universe to see if that produces a result very different than the 35-35-30 universe. Among those universes where Bush is behind (or equal), a universe that is split 50-50-0 (with just one extra vote for the closest opponent to Bush) would be the most likely to produce a 6 percent difference between the top two candidates by chance, but we are not prepared to believe that the voters are split in such a fashion. This assumption shows that we are bringing some judgments to bear from outside the observed data. For now, the point is not how to discover the appropriate benchmark hypothesis, but rather its criterion —which is, I repeat, that universe (among all possible universes) that yields a conclusion contrary to the conclusion shown by the data (and in which we are interested) and that (among such universes that yield such a conclusion) has the highest probability of producing the observed sample. Let’s go through the logic again: 1) Bush apparently has a 6 percent lead over the second-place candidate. 2) We ask if the second-place candidate might be ahead if all voters were polled. We test that by setting up a universe in which the second-place candidate is infinitesimally ahead (in practice, we make the two top candidates equal in our hypothetical universe). And we make the third-place candidate somewhere close to the top two candidates. 3) We then draw samples from this universe and observe how often the result is a 6 percent lead for the top candidate (who starts off just below equal in the universe). From here on, the procedure is straightforward: Determine how likely that universe is to produce a sample as far (or further) away in the direction of “our” candidate winning. (One could do something like this even if the candidate of interest were not now in the lead.) This problem teaches again that one must think explicitly about the choice of a benchmark hypothesis. The grounds for the choice of the benchmark hypothesis should precede the program, or should be included as an extended comment within the program. This program embodies the previous line of thought. URN 35#1 35#2 30#3 univ 1= Bush, 2= Perot, 3=Clinton REPEAT 1000 SAMPLE 750 univ samp Take a sample of 750 votes COUNT samp =1 bush Count the Bush voters, etc. COUNT samp =2 pero Perot voters COUNT samp =3 clin Clinton voters CONCAT pero clin others Join Perot &amp; Clinton votes MAX others second Find the larger of the other two SUBTRACT bush second d Find Bush’s margin over 2nd SCORE d z END HISTOGRAM z COUNT z &gt;=46 m Compare to the observed margin in the sample of 750 corresponding to a 6 percent margin by Bush over 2nd place finisher (rounded) DIVIDE m 1000 mm PRINT mm Samples of 750 Voters Bush’s margin over 2 nd mm = 0.018 When we run this program with a 36-36-28 split, we also get a similar result—2.6 percent. That is, the analysis shows a probability of only 2.6 percent that Bush would score a 6 percentage point “victory” in the sample, by chance, if the universe were split as specified. So Bush could feels reasonably confident that at the time the poll was taken, he was ahead of the other two candidates. 20.2 Paired Comparisons With Counted Data Example 17-4: The Pig Rations Again, But Comparing Pairs of Pigs (Paired-Comparison Test) (Program “Pigs2”) To illustrate how several different procedures can reasonably be used to deal with a given problem, here is another way to decide whether pig ration A is “really” better: We can assume that the order of the pig scores listed within each ration group is random—perhaps the order of the stalls the pigs were kept in, or their alphabetical-name order, or any other random order not related to their weights . Match the first pig eating ration A with the first pig eating ration B, and also match the second pigs, the third pigs, and so forth. Then count the number of matched pairs on which ration A does better. On nine of twelve pairings ration A does better, that is, 31.0 &gt; 26.0, 34.0 &gt; 24.0, and so forth. Now we can ask: If the two rations are equally good, how often will one ration exceed the other nine or more times out of twelve, just by chance? This is the same as asking how often either heads or tails will come up nine or more times in twelve tosses. (This is a “two-tailed” test because, as far as we know, either ration may be as good as or better than the other.) Once we have decided to treat the problem in this manner, it is quite similar to Example 15-1 (the first fruitfly irradiation problem). We ask how likely it is that the outcome will be as far away as the observed outcome (9 “heads” of 12) from 6 of 12 (which is what we expect to get by chance in this case if the two rations are similar). So we conduct perhaps fifty trials as in Table 17-3, where an asterisk denotes nine or more heads or tails. Step 1. Let odd numbers equal “A better” and even numbers equal “B better.” Step 2. Examine 12 random digits and check whether 9 or more, or 3 or less, are odd. If so, record “yes,” otherwise “no.” Step 3. Repeat step 2 fifty times. Step 4. Compute the proportion “yes,” which estimates the probability sought. The results are shown in Table 17-3. In 8 of 50 simulation trials, one or the other ration had nine or more tosses in its favor. Therefore, we estimate the probability to be .16 (eight of fifty) that samples this different would be generated by chance if the samples came from the same universe. Table 17-3 Results From Fifty Simulation Trials Of The Problem “Pigs2” Trial Heads\" or Odds\" (Ration A) “Tails” or “Evems” (Ration B) Trial “Heads” or Odds\" (Ration A) “Tails” or “Evens” (Ration B) 1 6 6 26 6 6 2 4 8 27 5 7 3 6 6 28 7 5 4 7 5 29 4 8 * 5 3 9 30 6 6 6 5 7 * 31 9 3 7 8 4 * 32 2 10 8 6 6 33 7 5 9 7 5 34 5 7 *10 9 3 35 6 6 11 7 5 36 8 4 *12 3 9 37 6 6 13 5 7 38 4 8 14 6 6 39 5 7 15 6 6 40 8 4 16 8 4 41 5 7 17 5 7 42 6 6 *18 9 3 43 5 7 19 6 6 44 7 5 20 7 5 45 6 6 21 4 8 46 4 8 * 22 10 2 47 5 7 23 6 6 48 5 7 24 5 7 49 8 4 *25 3 9 50 7 5 Now for a RESAMPLING STATS program and results. “Pigs2” is different from “Pigs1” in that it compares the weight-gain results of pairs of pigs, instead of simply looking at the rankings for weight gains. The key to “Pigs2” is the GENERATE statement. If we assume that ration A does not have an effect on weight gain (which is the “benchmark” or “null” hypothesis), then the results of the actual experiment would be no different than if we randomly GENERATE numbers “1” and “2” and treat a “1” as a larger weight gain for the ration A pig, and a “2” as a larger weight gain for the ration B pig. Both events have a .5 chance of oc- curring for each pair of pigs because if the rations had no effect on weight gain (the null hypothesis), ration A pigs would have larger weight gains about half of the time. The next step is to COUNT the number of times that the weight gains of one group (call it the group fed with ration A) were larger than the weight gains of the other (call it the group fed with ration B). The complete program follows: REPEAT 1000 Do 1000 trials GENERATE 12 1,2 a Generate randomly 12 “1”s and “2”s, put them in a. This represents 12 “pairings” where “1” = ration a “wins,” “2” = ration b = “wins.” COUNT a =1 b Count the number of “pairings” where ration a won, put the result in b. SCORE b z Keep track of the result in z END End the trial, go back and repeat until all 100 trials are complete. COUNT z &gt;= 9 j Determine how often we got 9 or more “wins” for ration a. COUNT z &lt;= 3 k Determine how often we got 3 or fewer “wins” for ration a. ADD j k m Add the two together DIVIDE m 100 mm Convert to a proportion PRINT mm Print the result. Note: The file “pigs2” on the Resampling Stats software disk contains this set of commands. Notice how we proceeded in Examples 15-6 and 17-4. The data were originally quantitative—weight gains in pounds for each pig. But for simplicity we classified the data into simpler counted-data formats. The first format (Example 15-6) was a rank order, from highest to lowest. The second format (Example 17-4) was simply higher-lower, obtained by randomly pairing the observations (using alphabetical letter, or pig’s stall number, or whatever was the cause of the order in which the data were presented to be random). Classifying the data in either of these ways loses some information and makes the subsequent tests somewhat cruder than more refined analysis could provide (as we shall see in the next chapter), but the loss of efficiency is not crucial in many such cases. We shall see how to deal directly with the quantitative data in Chapter 18. Example 17-5: Merged Firms Compared to Two Non-Merged Groups In a study by Simon, Mokhtari, and Simon (1996), a set of 33 advertising agencies that merged over a period of years were each compared to entities within two groups (each also of 33 firms) that did not merge; one non-merging group contained firms of roughly the same size as the final merged entities, and the other non-merging group contained pairs of non-merging firms whose total size was roughly the same as the total size of the merging entities. The idea beind the matching was that each pair of merged firms was compared against a pair of contemporaneous firms that were roughly the same size as the merging firms before the merger, and a single firm that was roughly the same size as the merged entity after the merger. Here (Table 17-4) are the data (provided by the authors): Table 17-4 Revenue Growth In Year 1 Following Merger Set # Merged Match1 Match2 1 -0.20000 0.02564 0.000000 2 -0.34831 -0.12500 0.080460 3 0.07514 0.06322 -0.023121 4 0.12613 -0.04199 0.164671 5 -0.10169 0.08000 0.277778 6 0.03784 0.14907 0.430168 7 0.11616 0.15183 0.142857 8 -0.09836 0.03774 0.040000 9 0.02137 0.07661 .0111111 10 -0.01711 0.28434 0.189139 11 -0.36478 0.13907 0.038869 12 0.08814 0.03874 0.094792 13 -0.26316 0.05641 0.045139 14 -0.04938 0.05371 0.008333 15 0.01146 0.04805 0.094817 16 0.00975 0.19816 0.060929 17 0.07143 0.42083 -0.024823 18 0.00183 0.07432 0.053191 19 0.00482 -0.00707 0.050083 20 -0.05399 0.17152 0.109524 21 0.02270 0.02788 -0.022456 22 0.05984 0.04857 0.167064 23 -0.05987 0.02643 0.020676 24 -0.08861 -0.05927 0.077067 25 -0.02483 -0.01839 0.059633 26 0.07643 0.01262 0.034635 27 -0.00170 -0.04549 0.053571 28 -0.21975 0.34309 0.042789 29 0.38237 0.22105 0.115773 30 -0.00676 0.25494 0.237047 31 -0.16298 0.01124 0.190476 32 0.19182 0.15048 0.151994 33 0.06116 0.17045 0.093525 Comparisons were made in several years before and after the mergings to see whether the merged entities did better or worse than the non-merging entities they were matched with by the researchers, but for simplicity we may focus on just one of the more important years in which they were compared— say, the revenue growth rates in the year after the merger. Here are those average revenue growth rates for the three groups: Year’s rev. growth MERGED -0.0213 MATCH 1 0.092085 MATCH 2 0.095931 We could do a general test to determine whether there are differences among the means of the three groups, as was done in the “Differences Among 4 Pig Rations” problem (chapter 18). However, we note that there may be considerable variation from one matched set to another – variation which can obscure the overall results if we resample from a large general bucket. Therefore, we use the following resampling procedure that maintains the separation between matched sets by converting each observation into a rank (1, 2 or 3) within the matched set. Here (Table 17-5) are those ranks: Table 17-5 Ranked Within Matched Set (1 = worst, 3 = best) Set # Merged Match1 Match2 1 1 3 2 2 1 2 3 3 3 2 1 4 2 1 3 5 1 2 3 6 1 3 2 7 1 3 2 8 1 2 3 9 1 2 3 10 1 2 3 11 1 3 2 12 2 1 3 13 1 3 2 14 1 3 2 15 1 2 3 16 1 3 2 17 2 3 1 18 1 3 2 Set # Merged Match1 Match2 19 2 1 3 20 1 3 2 21 2 2 3 22 2 2 3 23 1 3 2 24 1 2 3 25 1 2 3 26 3 1 2 27 2 1 3 28 1 3 2 29 3 2 1 30 1 3 2 31 1 2 3 32 3 1 2 33 1 3 2 These are the average ranks for the three groups (1 = worst, 3 = best): MERGED 1.45 MATCH 1 2.18 MATCH 2 2.36 Is it possible that the merged group received such a low (poor) average ranking just by chance? The null hypothesis is that the ranks within each set were assigned randomly, and that “merged” came out so poorly just by chance. The following procedure simulates random assignment of ranks to the “merged” group: Randomly select 33 integers between “1” and “3” (inclusive). Find the average rank &amp; record. Repeat steps 1 and 2, say, 1000 times. Find out how often the average rank is as low as 1.45 Here’s a RESAMPLING STATS program (“merge.sta”): REPEAT 1000 GENERATE 33 (1 2 3) ranks MEAN ranks ranksum SCORE ranksum z END HISTOGRAM z COUNT z &lt;=1.45 k DIVIDE k 1000 kk PRINT kk Result: kk = 0 Interpretation: 1000 random selections of 33 ranks never produced an average as low as the observed average. Therefore we rule out chance as an explanation for the poor ranking of the merged firms. Exactly the same technique might be used in experimental medical studies wherein subjects in an experimental group are matched with two different entities that receive placebos or control treatments. For example, there have been several recent three-way tests of treatments for depression: drug therapy versus cognitive therapy versus combined drug and cognitive therapy. If we are interested in the combined drug-therapy treatment in particular, comparing it to standard existing treatments, we can proceed in the same fashion as in the merger problem. We might just as well consider the real data from the merger as hypothetical data for a proposed test in 33 triplets of people that have been matched within triplet by sex, age, and years of education. The three treatments were to be chosen randomly within each triplet. Assume that we now switch scales from the merger data, so that #1 = best and #3 = worst, and that the outcomes on a series of tests were ranked from best (#1) to worst (#3) within each triplet. Assume that the combined drug-and-therapy regime has the best average rank. How sure can we be that the observed result would not occur by chance? Here are the data from the merger study, seen here as Table 17-5-b: Table 17-5-b Ranked Therapies Within Matched Patient Triplets (hypothetical data identical to merger data) (1 = best, 3 = worst) Triplet # Therapy Only Combined Drug Only 1 1 3 2 2 1 2 3 3 3 2 1 4 2 1 3 5 1 2 3 6 1 3 2 7 1 3 2 8 1 2 3 9 1 2 3 10 1 2 3 11 1 3 2 12 2 1 3 13 1 3 2 14 1 3 2 15 1 2 3 16 1 3 2 17 2 3 1 18 1 3 2 19 2 1 3 20 1 3 2 21 2 1 3 22 2 1 3 23 1 3 2 24 1 2 3 25 1 2 3 26 3 1 2 27 2 1 3 28 1 3 2 29 3 2 1 30 1 3 2 31 1 2 3 32 3 1 2 33 1 3 2 These are the average ranks for the three groups (“1” = best, “3”= worst): Combined 1.45 Drug 2.18 Therapy 2.36 In these hypothetical data, the average rank for the drug and therapy regime is 1.45. Is it likely that the regimes do not “really” differ with respect to effectiveness, and that the drug and therapy regime came out with the best rank just by the luck of the draw? We test by asking, “If there is no difference, what is the probability that the treatment of interest will get an average rank this good, just by chance?” We proceed exactly as with the solution for the merger problem (see above). In the above problems, we did not concern ourselves with chance outcomes for the other therapies (or the matched firms) because they were not our primary focus. If, in actual fact, one of them had done exceptionally well or poorly, we would have paid little notice because their performance was not the object of the study. We needed, therefore, only to guard against the possibility that chance good luck for our therapy of interest might have led us to a hasty conclusion. Suppose now that we are not interested primarily in the combined drug-therapy treatment, and that we have three treatments being tested, all on equal footing. It is no longer sufficient to ask the question “What is the probability that the combined therapy could come out this well just by chance?” We must now ask “What is the probability that any of the therapies could have come out this well by chance?” (Perhaps you can guess that this probability will be higher than the probability that our chosen therapy will do so well by chance.) Here is a resampling procedure that will answer this question: Put the numbers “1”, “2” and “3” (corresponding to ranks) in a bucket Shuffle the numbers and deal them out to three locations that correspond to treatments (call the locations “t1,” “t2,” and “t3”) Repeat step two another 32 times (for a total of 33 repetitions, for 33 matched triplets) Find the average rank for each location (treatment. Record the minimum (best) score. Repeat steps 2-4, say, 1000 times. Find out how often the minimum average rank for any treatment is as low as 1.45 NUMBERS (1 2 3) a Step 1 above REPEAT 1000 Step 6 REPEAT 33 Step 3 SHUFFLE a a Step 2 SCORE a t1 t2 t3 Step 2 END END Step 3 MEAN t1 tt1 Step 4 MEAN t2 tt2 MEAN t3 tt3 CLEAR t1 Clear the vectors where we’ve stored the ranks for this trial (must do this whenever we have a SCORE statement that’s part of a “nested” repeat loop) CLEAR t2 CLEAR t3 CONCAT tt1 tt2 tt3 b Part of step 5 MIN b bb Part of step 5 SCORE bb z Part of step 5 Step 6 HISTOGRAM z COUNT z &lt;=1.45 k Step 7 DIVIDE k 1000 kk PRINT kk Interpretation: 1000 random shufflings of 33 ranks, apportioned to three “treatments,” never produced for the best treatment in the three an average as low as the observed average, therefore we rule out chance as an explanation for the success of the combined therapy. An interesting feature of the mergers (or depression treatment) problem is that it would be hard to find a conventional test that would handle this three-way comparison in an efficient manner. Certainly it would be impossible to find a test that does not require formulae and tables that only a talented professional statistician could manage satisfactorily, and even s/ he is not likely to fully understand those formulaic procedures. Result: kk = 0 20.3 Endnotes Technical note: Some of the tests introduced in this chapter are similar to standard nonparametric rank and sign tests. They differ less in the structure of the test statistic than in the way in which significance is assessed (the comparison is to multiple simulations of a model based on the benchmark hypothesis, rather than to critical values calculated analytically). If you are very knowledgeable, you may do some in-between figuring (with what is known as “Bayesian analysis”), but leave this alone unless you know well what you are doing. The data for this example are based on W. J. Dixon and F. J. Massey (1969, p. 117), who offer an orthodox method of handling the problem with a t-test. "],["the-statistics-of-hypothesis-testing-with-measured-data.html", "21 The Statistics of Hypothesis-Testing With Measured Data 21.1 Differences among four means 21.2 Using Squared Differences 21.3 Endnotes 21.4 Exercises", " 21 The Statistics of Hypothesis-Testing With Measured Data Chapters 15 and 17 discussed testing a hypothesis with data that either arrive in dichotomized (yes-no) form, or come as data in situations where it is convenient to dichotomize. We next consider hypothesis testing using measured data. Conventional statistical practice employs such devices as the “t-test” and “analysis of variance.” In contrast to those complex devices, the resampling method does not differ greatly from what has been discussed in previous chapters. Example 18-1: The Pig Rations Still Once Again, Using Measured Data (Testing for the Difference Between Means of Two Equal-Sized Samples of Measured-Data Observations) (Program “Pigs3”) Let us now treat the pig-food problem without converting the quantitative data into qualitative data, because a conversion always loses information. The term “lose information” can be understood intuitively. Consider two sets of three sacks of corn. Set A includes sacks containing, respectively, one pound, two pounds, and three pounds. Set B includes sacks of one pound, two pounds, and a hundred pounds. If we rank the sacks by weight, the two sets can no longer be distinguished. The one-pound and two-pound sacks have ranks one and two in both cases, and their relative places in their sets are the same. But if we know not only that the one-pound sack is the smallest of its set and the three-pound or hundred-pound sack is the largest, but also that the largest sack is three pounds (or a hundred pounds), we have more information about a set than if we only know the ranks of its sacks. Rank data are also known as “ordinal” data, whereas data measured in (say) pounds are known as “cardinal” data. Even though converting from cardinal (measured) to ordinal (ranked) data loses information, the conversion may increase convenience, and may therefore be worth doing in some cases. We begin a measured-data procedure by noting that if the two pig foods are the same, then each of the observed weight gains came from the same benchmark universe . This is the basic tactic in our statistical strategy. That is, if the two foods came from the same universe, our best guess about the composition of that universe is that it includes weight gains just like the twenty-four we have observed , and in the same proportions, because that is all the information that we have about the universe; this is the bootstrap method. Since ours is (by definition) a sample from an infinite (or at least, a very large) universe of possible weight gains, we assume that there are many weight gains in the universe just like the ones we have observed, in the same proportion as we have observed them. For example, we assume that 2/24 of the universe is composed of 34-pound weight gains, as seen in Figure 18-1: Figure 18-1 We recognize, of course, that weight gains other than the exact ones we observed certainly would occur in repeated experiments. And if we thought it reasonable to do so, we could assume that the “distribution” of the weight gains would follow a regular “smooth” shape such as Figure 18-2. But deciding just how to draw Figure 18-2 from the data in Figure 18-1 requires that we make arbitrary assumptions about unknown conditions. And if we were to draw Figure 18-2 in a form that would be sufficiently regular for conventional mathematical analysis, we might have to make some very strong assumptions going far beyond the observed data. Drawing a smooth curve such as Figure 18-2 from the raw data in Figure 18-1 might be satisfactory—if done with wisdom and good judgment. But there is no necessity to draw such a smooth curve, in this case or in most cases. We can proceed by assuming simply that the benchmark universe—the universe to which we shall compare our samples, conventionally Relative Probability called the “null” or “hypothetical” universe—is composed only of elements similar to the observations we have in hand. We thereby lose no efficiency and avoid making unsound assumptions. Size of Weight Gain, 30.2 = Mean Figure 18-2 To carry out our procedure in practice: 1) Write down each of the twenty-four weight gains on a blank index card. We then have one card each for 31, 34, 29, 26, and so on. 2) Shuffle the twenty-four cards thoroughly, and pick one card. 3) Record the weight gain, and replace the card. (Recall that we are treating the weight gains as if they come from an infinite universe— that is, as if the probability of selecting any amount is the same no matter which others are selected randomly. Another way to say this is to state that each selection is independent of each other selection. If we did not replace the card before selecting the next weight gain, the selections would no longer be independent. See Chapter 6 for further discussion of this issue.) 4) Repeat this process until you have made two sets of 12 observations. 5) Call the first hand “food A” and the second hand “food B.” Determine the average weight gain for the two hands, and record it as in Table 18-1. Repeat this procedure many times. In operational steps: Step 1. Write down each observed weight gain on a card, e.g. 31, 34, 29... Step 2. Shuffle and deal a card. Step 3. Record the weight and replace the card. Step 4. Repeat steps 2 and 3 eleven more times; call this group A. Step 5. Repeat steps 2-3 another twelve times; call this group B. Step 6. Calculate the mean weight gain of each group. Step 7. Subtract the mean of group A from the mean of group B and record. If larger (more positive) than 3.16 (the difference between the observed means) or more negative than -3.16, record “more.” Otherwise record “less.” Step 8. Repeat this procedure perhaps fifty times, and calculate the proportion “more.” This estimates the probability sought. In none of the first ten simulated trials did the difference in the means of the random hands exceed the observed difference (3.16 pounds, in the top line in the table) between foods A and B. (The difference between group totals tells the same story and is faster, requiring no division calculations.) In the old days before a computer was always easily available, I would quit making trials at such a point, confident that a difference in means as great as observed is not likely to happen by chance. (Using the convenient “multiplication rule” described in Chapter 5, we can estimate the probability of such an occurrence happening by chance in 10 successive trials as 1/2 x 1/2 x 1/2 ... = (1/2) 10 = 1/1024 ~ .001 = .1 percent, a small chance indeed.) Nevertheless, let us press on to do 50 trials. Table 18-1 Trial # Mean of First 12 Observation s (First Hand) Mean of Second 12 Observation s (Second Hand) Differenc e Greater or Less Than Observed Differenc Observed 382 / 12=31.83 344 / 12=28.67 3.16 1 368 / 12=30.67 357 / 12=29.75 .87 Less 2 364 / 12=30.33 361 / 12=30.08 .25 Less 3 352 / 12=29.33 373 / 12=31.08 (1.75) Less 4 378 / 12=31.50 347 / 12=28.92 2.58 Less 5 365 / 12=30.42 360 / 12=30.00 .42 Less 6 352 / 12=29.33 373 / 12=31.08 (1.75) Less 7 355 / 12=29.58 370 / 12=30.83 (1.25) Less 8 366 / 12=30.50 359 / 12=29.92 .58 Less 9 360 / 12=30.00 365 / 12=30.42 (.42) Less 10 355 / 12=29.58 370 / 12=30.83 (1.25) Less 11 359 / 12=29.92 366 / 12=30.50 (.58) Less 12 369 / 12=30.75 356 / 12=29.67 1.08 \" Results of Fifty Random Samples for the Problem “PIGS3” e Trial # Mean of First 12 Observation s (First Hand) Mean of Second 12 Observation s (Second Hand) Differenc e Greater or Less Than Observed Differenc Observed 382 / 12=31.83 344 / 12=28.67 3.16 13 360 / 12=30.00 365 / 12=30.42 (.42) Less 14 377 / 12=31.42 348 / 12=29.00 2.42 Less 15 365 / 12=30.42 360 / 12=30.00 .42 Less 16 364 / 12=30.33 361 / 12=30.08 .25 Less 17 363 / 12=30.25 362 / 12=30.17 .08 Less 18 365 / 12=30.42 360 / 12=30.00 .42 Less 19 369 / 12=30.75 356 / 12=29.67 1.08 Less 20 369 / 12=30.75 356 / 12=29.67 1.08 Less 21 369 / 12=30.75 356 / 12=29.67 1.08 Less 22 364 / 12=30.33 361 / 12=30.08 .25 Less 23 363 / 12=30.25 362 / 12=30.17 .08 Less 24 363 / 12=30.25 362 / 12=30.17 .08 Less 25 364 / 12=30.33 361 / 12=30.08 .25 Less 26 359 / 12=29.92 366 / 12=30.50 (.58) Less 27 362 / 12=30.17 363 / 12=30.25 (.08) Less 28 362 / 12=30.17 363 / 12=30.25 (.08) Less 29 373 / 12=31.08 352 / 12=29.33 1.75 Less 30 367 / 12=30.58 358 / 12=29.83 .75 Less 31 376 / 12=31.33 349 / 12=29.08 2.25 Less 32 365 / 12=30.42 360 / 12=30.00 .42 Less 33 357 / 12=29.75 368 / 12=30.67 (1.42) Less 34 349 / 12=29.08 376 / 12=31.33 2.25 Less 35 356 / 12=29.67 396 / 12=30.75 (1.08) Less 36 359 / 12=29.92 366 / 12=30.50 (.58) Less 37 372 / 12=31.00 353 / 12=29.42 1.58 Less 38 368 / 12=30.67 357 / 12=29.75 .92 Less 39 344 / 12=28.67 382 / 12=31.81 (3.16) Equal 40 365 / 12=30.42 360 / 12=30.00 .42 Less 41 375 / 12=31.25 350 / 12=29.17 2.08 Less 42 353 / 12=29.42 372 / 12=31.00 (1.58) Less 43 357 / 12=29.75 368 / 12=30.67 (.92) Less 44 363 / 12=30.25 362 / 12=30.17 .08 Less 45 353 / 12=29.42 372 / 12=31.00 (1.58) Less 46 354 / 12=29.50 371 / 12=30.92 (1.42) Less 47 353 / 12=29.42 372 / 12=31.00 (1.58) Less 48 366 / 12=30.50 350 / 12=29.92 .58 Less 49 364 / 12=30.53 361 / 12=30.08 .25 Less 50 370 / 12=30.83 355 / 12=29.58 1.25 Less Table 18-1 (continued) e Table 18-1 shows fifty trials of which only one (the thirty-ninth) is as “far out” as the observed samples. These data give us an estimate of the probability that, if the two foods come from the same universe, a difference this great or greater would occur just by chance. (Compare this 2 percent estimate with the probability of roughly 1 percent estimated with the conventional t test—a “significance level” of 1 percent.) On the average, the test described in this section yields a significance level as high as such mathematical-probability tests as the t test— that is, it is just as efficient—though the tests described in Examples 15-6 and 17-1 are likely to be less efficient because they convert measured data to ranked or classified data. \\[1\\] It is not appropriate to say that these data give us an estimate of the probability that the foods “do not come” from the same universe. This is because we can never state a probability that a sample came from a given universe unless the alternatives are fully specified in advance. 1 This example also illustrates how the dispersion within samples affects the difficulty of finding out whether the samples differ from each other. For example, the average weight gain for food A was 32 pounds, versus 29 pounds for food B. If all the food A-fed pigs had gained weight within a range of say 29.9 and 30.1 pounds, and if all the food B-fed pigs had gained weight within a range of 28.9 and 29.1 pounds—that is, if the highest weight gain in food B had been lower than the lowest weight gain in food A—then there would be no question that food A is better, and even fewer observations would have made this statistically conclusive. Variation (dispersion) is thus of great importance in statistics and in the social sciences. The larger the dispersion among the observations within the samples, the larger the sample size necessary to make a conclusive comparison between two groups or reliable estimates of summarization statistics. (The dispersion might be measured by the mean absolute deviation (the average absolute difference between the mean and the individual observations, treating both plus and minus differences as positive), the variance (the average squared difference between the mean and the observations), the standard deviation (the square root of the variance), the range (the difference between the smallest and largest observations), or some other device.) 1 This short comment is the tip of the iceberg of an argument that has been going on for 200 years among statisticians. It needs much more discussion to be understandable or persuasive; for such discussion, see Simon (forthcoming). If you are performing your tests by hand rather than using a computer (a good exercise even nowadays when computers are so accessible), you might prefer to work with the median instead of the mean, because the median requires less computation. (The median also has the advantage of being less influenced by a single far-out observation that might be quite atypical; all measures have their special advantages and disadvantages.) Simply compare the difference in medians of the twelve-pig resamples to the difference in medians of the actual samples, just as was done with the means. The only operational difference is to substitute the word “median” for the word “mean” in the steps listed above. You may need a somewhat larger number of trials when working with medians, however, for they tend to be less precise than means. The RESAMPLING STATS program compares the difference in the sums of the weight gains for the actual pigs against the difference resulting from two randomly-chosen groups of pigs, using the same numerical weight gains of individual pigs as were obtained in the actual experiment. If the differences in average weight gains of the randomly ordered groups are rarely as large as the difference in weight gains from the actual sets of pigs fed food A-alpha and food B-beta, then we can conclude that the foods do make a difference in pigs’ weight gains. Note first that pigs in group A gained a total of 382 pounds while group B gained a total of 344 pounds—38 fewer. To minimize computations, we will deal with totals like these, not averages. First we construct vectors A and B of the weight gains of the pigs fed with the two foods. Then we combine the two vectors into one long vector and select two groups of 12 randomly and with replacement (the two SAMPLE commands). We SUM the weight gains for the two resamples, and calculate the difference. We keep SCORE of those differences, graph them on a HISTOGRAM, and see how many times resample A exceeded resample B by at least 38 pounds, or vice versa (we are testing whether the two are different, not whether food A produces larger weight gains). NUMBERS (31 34 29 26 32 35 38 34 31 29 32 31) a Record group a’s weight gains. NUMBERS (26 24 28 29 30 29 31 29 32 26 28 32) b Record group b’s weight gains. CONCAT a b c Combine a and b together in one long vector. REPEAT 1000 Do 1000 experiments. SAMPLE 12 c d Take a “resample” of 12 with replacement from c and put it in d. SAMPLE 12 c e Take another “resample.” SUM d dd Sum the first “resample.” SUM e ee Sum the second “resample.” SUBTRACT dd ee f Calculate the difference between the two resamples. SCORE f z Keep track of each trial result. END End one experiment, go back and repeat until all trials are complete, then proceed. HISTOGRAM z Produce a histogram of trial results. PIGS3: Difference Between Two Resamples Sum of Weight Gains 1 st resample less 2 nd From this histogram we see that none of the trials produced a difference between groups as large as that observed (or larger). RESAMPLING STATS will calculate this for us with the following commands: COUNT z &gt;= 38 k Determine how many of the trials produced a difference between resamples &gt;= 38. COUNT z &lt;= -38 l Likewise for a difference of -38. ADD k l m Add the two together. DIVIDE m 1000 mm Convert to a proportion. PRINT mm Print the result. Note: The file “pigs3” on the Resampling Stats software disk contains this set of commands. Example 18-2: Is There a Difference in Liquor Prices Between State-Run and Privately-Run Systems? (Testing for Differences Between Means of Unequal-Sized Samples of Measured Data) In the 1960s I studied the price of liquor in the sixteen “monopoly” states (where the state government owns the retail liquor stores) compared to the twenty-six states in which retail liquor stores are privately owned. (Some states were omitted for technical reasons. And it is interesting to note that the situation and the price pattern has changed radically since then.) These data were introduced in the context of a problem in probability in Chapter 7. These were the representative 1961 prices of a fifth of Seagram 7 Crown whiskey in the two sets of states: 16 monopoly states: $4.65, $4.55, $4.11, $4.15, $4.20, $4.55, $3.80, $4.00, $4.19, $4.75, $4.74, $4.50, $4.10, $4.00, $5.05, $4.20 Mean = $4.35 26 private-ownership states: $4.82, $5.29, $4.89, $4.95, $4.55, $4.90, $5.25, $5.30, $4.29, $4.85, $4.54, $4.75, $4.85, $4.85, $4.50, $4.75, $4.79, $4.85, $4.79, $4.95, $4.95, $4.75, $5.20, $5.10, $4.80, $4.29. Mean = $4.84 The economic question that underlay the investigation—having both theoretical and policy ramifications—is as follows: Does state ownership affect prices? The empirical question is whether the prices in the two sets of states were systematically different. In statistical terms, we wish to test the hypothesis that there was a difference between the groups of states related to their mode of liquor distribution, or whether the observed $.49 differential in means might well have occurred by happenstance. In other words, we want to know whether the two sub-groups of states differed systematically in their liquor prices, or whether the observed pattern could well have been produced by chance variability. The first step is to examine the two sets of data graphically to see whether there was such a clear-cut difference between them—of the order of Snow’s data on cholera, or the Japanese Navy data on beri-beri—that no test was necessary. The separate displays, and then the two combined together, are shown in Figure 7-2; the answer is not clear-cut and hence a formal test is necessary. GOVERNMENT 5 0 350 400 450 500 550 Cents Mean: $4.35 PRIVATE 5 0 350 400 450 500 550 Cents Mean: $4.84 PRIVATE + GOVERNMENT 5 0 350 400 450 500 550 Cents Figure 7-2 (repeated) At first I used a resampling permutation test as follows: Assuming that the entire universe of possible prices consists of the set of events that were observed, because that is all the information available about the universe, I wrote each of the forty-two observed state prices on a separate card. The shuffled deck simulated a situation in which each state has an equal chance for each price. On the “null hypothesis” that the two groups’ prices do not reflect different price-setting mechanisms, but rather differ only by chance, I then examined how often that simulated universe stochastically produces groups with results as different as observed in 1961. I repeatedly dealt groups of 16 and 26 cards, without replacing the cards, to simulate hypothetical monopoly-state and private-state samples, each time calculating the difference in mean prices. The probability that the benchmark null-hypothesis universe would produce a difference between groups as large or larger than observed in 1961 is estimated by how frequently the mean of the group of randomly-chosen sixteen prices from the simulated state-ownership universe is less than (or equal to) the mean of the actual sixteen state-ownership prices. If the simulated difference between the randomly-chosen groups was frequently equal to or greater than observed in 1961, one would not conclude that the observed difference was due to the type of retailing system because it could well have been due to chance variation. The results—not even one “success” in 10,000 trials—imply that there is a very small probability that two groups with mean prices as different as were observed would happen by chance if drawn from the universe of 42 observed prices. So we “reject the null hypothesis” and instead find persuasive the proposition that the type of liquor distribution system influences the prices that consumers pay. As I shall discuss later, the logical framework of this resampling version of the permutation test differs greatly from the formulaic version, which would have required heavy computation. The standard conventional alternative would be a Student’s t-test, in which the user simply plugs into an unintuitive formula and reads the result from a table. And because of the unequal numbers of cases and unequal dispersions in the two samples, an appropriate t-test is far from obvious, whereas resampling is not made more difficult by such realistic complications. A program to handle the liquor problem with an infinite-universe bootstrap distribution simply substitutes the random sampling command SAMPLE for the SHUFFLE/TAKE commands. The results of the new test are indistinguishable from those in the program given above. Still another difficult question is whether any hypothesis test is appropriate, because the states were not randomly selected for inclusion in one group or another, and the results could be caused by factors other than the liquor system; this applies to both the above methods. The states constitute the entire universe in which we are interested, rather than being a sample taken from some larger universe as with a biological experiment or a small survey sample. But this objection pertains to a conventional test as well as to resampling methods. And a similar question arises throughout medical and social science— to the two wells between which John Snow detected vast differences in cholera rates, to rates of lung cancer in human smokers, to analyses of changes in speeding laws, and so on. The appropriate question is not whether the units were assigned randomly, however, but whether there is strong reason to believe that the results are not meaningful because they are the result of a particular “hidden” variable. These debates about fundamentals illustrate the unsettled state of statistical thinking about basic issues. Other disciplines also have their controversies about fundamentals. But in statistics these issues arise as early as the introductory course, because all but the most contrived problems are shot through with these questions. Instructors and researchers usually gloss over these matters, as Gigerenzer et al., show ( The Empire of Chance ). Again, because with resampling one does not become immersed in the difficult mathematical techniques that underlie conventional methods, one is quicker to see these difficult questions, which apply equally to conventional methods and resampling. Example 18-3: Is There a Difference Between Treatments to Prevent Low Birthweights? Next we consider the use of resampling with measured data to test the hypothesis that drug A prevents low birthweights (Rosner, 1982, p. 257). The data for the treatment and control groups are shown in Table 18-2. Table 18-2 Birthweights in a Clinical Trial to Test a Drug for Preventing Low Birthweights Treatment Group Control Group 6.9 6.4 7.6 6.7 7.3 5.4 7.6 8.2 6.8 5.3 7.2 6.6 8.0 5.8 5.5 5.7 5.8 6.2 7.3 7.1 8.2 7.0 6.9 6.9 6.8 5.6 5.7 4.2 8.6 6.8 Average: 7.08 6.26 Source: Rosner, Table 8.7 The treatment group averaged .82 pounds more than the control group. Here is a resampling approach to the problem: If the drug has no effect, our best guess about the “universe” of birthweights is that it is composed of (say) a million each of the observed weights, all lumped together. In other words, in the absence of any other information or compelling theory, we assume that the combination of our samples is our best estimate of the universe. Hence let us write each of the birthweights on a card, and put them into a hat. Drawing them one by one and then replacing them is the operational equivalent of a very large (but equal) number of each birthweight. Repeatedly draw two samples of 15 birthweights each, and check how frequently the observed difference is as large as, or larger than, the actual difference of .82 pounds. We find in the RESAMPLING STATS program below that only 1 percent of the pairs of hypothetical resamples produced means that differed by as much as .82. We therefore conclude that the observed difference is unlikely to have occurred by chance. NUMBERS (6.9 7.6 7.3 7.6 6.8 7.2 8.0 5.5 5.8 7.3 8.2 6.9 6.8 5.7 8.6) treat NUMBERS (6.4 6.7 5.4 8.2 5.3 6.6 5.8 5.7 6.2 7.1 7.0 6.9 5.6 4.2 6.8) control CONCAT treat control all Combine all birthweight observations in same vector REPEAT 1000 Do 1000 simulations SAMPLE 15 all treat$ Take a resample of 15 from all birth weights (the $ indicates a resampling counterpart to a real sample) SAMPLE 15 all control$ Take a second, similar resample MEAN treat$ mt Find the means of the two resamples MEAN control$ mc SUBTRACT mt mc dif Find the difference between the means of the two resamples SCORE dif z Keep score of the result END End the simulation experiment, go back and repeat HISTOGRAM z Produce a histogram of the resample differences COUNT z &gt;= .82 k How often did resample differences exceed the observed difference of .82? Resample differences in pounds Result: Only 1.3 percent of the pairs of resamples produced means that differed by as much as .82. We can conclude that the observed difference is unlikely to have occurred by chance. Example 18-4: Bootstrap Sampling with Replacement Efron and Tibshirani (1993, p. 11) present this as their basic problem illustrating the bootstrap method: Seven mice were given a new medical treatment intended to improve their survival rates after surgery, and nine mice were not treated. The numbers of days the treated mice survived were 94, 38, 23, 197, 99, 16 and 14, whereas the numbers of days the untreated mice (the control group) survived were 52, 10, 40, 104, 51, 27, 146, 30, and 46. The question we ask is: Did the treatment prolong survival, or might chance variation be responsible for the observed difference in mean survival times? We start by supposing the treatment did NOT prolong survival and that chance was responsible. If that is so, then we consider that the two groups came from the same universe. Now we’d like to know how likely it is that two groups drawn from this common universe would differ as much as the two observed groups differ. If we had unlimited time and money, we would seek additional samples in the same way that we obtained these. Lacking time and money, we create a hypothetical universe that embodies everything we know about such a common universe. We imagine replicating each sample element millions of times to create an almost infinite universe that looks just like our samples. Then we can take resamples from this hypothetical universe and see how they behave. Even on a computer, creating such a large universe is tedious so we use a shortcut. We replace each element after we pick it for a resample. That way, our hypothetical (bootstrap) universe is effectively infinite. The following procedure will serve: Calculate the difference between the means of the two observed samples – it’s 30.63 days in favor of the treated mice. Consider the two samples combined (16 observations) as the relevant universe to resample from. Draw 7 hypothetical observations with replacement and designate them “Treatment”; draw 9 hypothetical observations with replacement and designate them “Control.” Compute and record the difference between the means of the two samples. Repeat steps 2 and 3 perhaps 1000 times. Determine how often the resampled difference exceeds the observed difference of 30.63. The following program (“mice2smp”) follows the above procedure: NUMBERS (94 38 23 197 99 16 141) treatmt treatment group NUMBERS (52 10 40 104 51 27 146 30 46) control control group CONCAT treatmt control u U is our universe (step 2 above) REPEAT 1000 step 5 above SAMPLE 7 u treatmt$ step 3 above SAMPLE 9 u control$ step 3 MEAN treatmt$ tmean step 4 MEAN control$ cmean step 4 SUBTRACT tmean cmean diff step 4 SCORE diff scrboard step 4 END step 5 HISTOGRAM scrboard COUNT scrboard &gt;=30.63 k step 6 DIVIDE k 1000 prob PRINT prob Result: PROB = 0.112 Interpretation: 1000 simulated resamples (of sizes 7 and 9) from a combined universe produced a difference as big as 30.63 more than 11 percent of the time. We cannot rule out the possibility that chance might be responsible for the observed advantage of the treatment group. Example 18-5: Permutation Sampling Without Replacement This section discusses at some length the question of when sampling with replacement (the bootstrap), and sampling without replacement (permutation or “exact” test) are the appropriate resampling methods. The case at hand seems like a clearcut case where the bootstrap is appropriate. (Note that in this case we draw both samples from a combined universe consisting of all observations, whether we do so with or without replacement.) Nevertheless, let us see how the technique would differ if one were to consider that the permutation test is appropriate. The algorithm would then be as follows (with the steps that are the same as above labeled “a” and those that are different labeled “b”): 1a. Calculate the difference between the means of the two observed samples – it’s 30.63 days in favor of the treated mice. 2a. Consider the two samples combined (16 observations) as the relevant universe to resample from. 3b. Draw 7 hypothetical observations without replacement and designate them “Treatment”; draw 9 hypothetical observations with replacement and designate them “Control.” 4a. Compute and record the difference between the means of the two samples. 5a. Repeat steps 2 and 3 perhaps 1000 times 6a. Determine how often the resampled difference exceeds the observed difference of 30.63. Here is the RESAMPLING STATS program: NUMBERS (94 38 23 197 99 16 141) treatmt treatment group NUMBERS (52 10 40 104 51 27 146 30 46) control control group CONCAT treatmt control u U is our universe (step 2 above) REPEAT 1000 step 5 above SHUFFLE u ushuf TAKE ushuf 1,7 treatmt$ step 3 above TAKE ushuf 8,16 control$ step 3 MEAN treatmt$ tmean step 4 MEAN control$ cmean step 4 SUBTRACT tmean cmean diff step 4 SCORE diff scrboard step 4 END step 5 HISTOGRAM scrboard COUNT scrboard &gt;=30.63 k step 6 DIVIDE k 1000 prob PRINT prob Result: prob = 0.145 Interpretation: 1000 simulated resamples (of sizes 7 and 9) from a combined universe produced a difference as big as 30.63 more than 14 percent of the time. We therefore should not rule out the possibility that chance might be responsible for the observed advantage of the treatment group. 21.1 Differences among four means Example 18-6: Differences Among Four Pig Rations (Test for Differences Among Means of More Than Two Samples of Measured Data) (File “PIGS4”) In Examples 15-1 and 15-4 we investigated whether or not the results shown by a single sample are sufficiently different from a null (benchmark) hypothesis so that the sample is unlikely to have come from the null-hypothesis benchmark universe. In Examples 15-7, 17-1, and 18-1 we then investigated whether or not the results shown by two samples suggest that both had come from the same universe, a universe that was assumed to be the composite of the two samples. Now as in Example 17-2 we investigate whether or not several samples come from the same universe, except that now we work with measured data rather than with counted data. If one experiments with each of 100 different pig foods on twelve pigs, some of the foods will show much better results than will others just by chance , just as one family in sixteen is likely to have the very “high” number of 4 daughters in its first four children. Therefore, it is wrong reasoning to try out the 100 pig foods, select the food that shows the best results, and then compare it statistically with the average (sum) of all the other foods (or worse, with the poorest food). With such a procedure and enough samples, you will surely find one (or more) that seems very atypical statistically. A bridge hand with 12 or 13 spades seems very atypical, too, but if you deal enough bridge hands you will sooner or later get one with 12 or 13 spades—as a purely chance phenomenon, dealt randomly from a standard deck. Therefore we need a test that prevents our falling into such traps. Such a test usually operates by taking into account the differences among all the foods that were tried. The method of Example 18-1 can be extended to handle this problem. Assume that four foods were each tested on twelve pigs. The weight gains in pounds for the pigs fed on foods A and B were as before. For foods C and D the weight gains were: Ration C: 30, 30, 32, 31, 29, 27, 25, 30, 31, 32, 34, 33 Ration D: 32, 25, 31, 26, 32, 27, 28, 29, 29, 28, 23, 25 Now construct a benchmark universe of forty-eight index cards, one for each weight gain. Then deal out sets of four hands randomly. More specifically: Step 1. Constitute a universe of the forty-eight observed weight gains in the four samples, writing the weight gains on cards. Step 2. Draw four groups of twelve weight gains, with replacement, since we are drawing from a hypothesized infinite universe in which consecutive draws are independent. Determine whether the difference between the lowest and highest group means is as large or larger than the observed difference. If so write “yes,” otherwise “no.” Step 3. Repeat step 2 fifty times. Step 4. Count the trials in which the differences between the simulated groups with the highest and lowest means are as large or larger than the differences between the means of the highest and lowest observed samples. The proportion of such trials to the total number of trials is the probability that all four samples would differ as much as do the observed samples if they (in technical terms) come from the same universe. The problem “Pigs4,” as handled by the steps given above, is quite similar to the way we handled Example 7-2, except that the data are measured (in pounds of weight gain) rather than simply counted (the number of rehabilitations). Instead of working through a program for the procedure out- lined above, let us consider a different approach to the problem—computing the difference between each pair of foods, six differences in all, converting all minus (-) signs to (+) differences. Then we can total the six differences, and compare the total with the sum of the six differences in the observed sample. The proportion of the resampling trials in which the observed sample sum is exceeded by the sum of the differences in the trials is the probability that the observed samples would differ as much as they do if they come from the same universe.\\[4\\] One naturally wonders whether this latter test statistic is better than the range, as discussed above. It would seem obvious that using the information contained in all four samples should increase the precision of the estimate. And indeed it is so, as you can confirm for yourself by comparing the results of the two approaches. But in the long run, the estimate provided by the two approaches would be much the same. That is, there is no reason to think that one or another of the estimates is biased . However, successive samples from the population would steady down faster to the true value using the four-groupbased estimate than they would using the range. That is, the four-group-based estimate would require a smaller sample of pigs. Is there reason to prefer one or the other approach from the point of view of some decision that might be made? One might think that the range procedure throws light on which one of the foods is best in a way that the four-group-based approach does not. But this is not correct. Both approaches answer this question, and only this question: Are the results from the four foods likely to have resulted from the same “universe” of weight gains or not? If one wants to know whether the best food is similar to, say, all the other three, the appropriate approach would be a two -sample approach similar to various two -sample examples discussed earlier. (It would be still another question to ask whether the best food is different from the worst. One would then use a procedure different from either of those discussed above.) If the foods cost the same, one would not need even a twosample analysis to decide which food to feed. Feed the one whose results are best in the experiment, without bothering to ask whether it is “really” the best; you can’t go wrong as long as it doesn’t cost more to use it. (One could inquire about the probability that the food yielding the best results in the experiment would attain those results by chance even if it was worse than the others by some stipulated amount, but pursu- ing that line of thought may be left to the student as an exercise.) In the problem “Pigs4,” we want a measure of how the groups differ. The obvious first step is to add up the total weight gains for each group: 382, 344, 364, 335. The next step is to calculate the differences between all the possible combinations of groups: 382-344=38, 382-364=18, 382-335=47, 344-364= -20, 344-335=9, 364-335=29. 21.2 Using Squared Differences Here we face a choice. We could work with the absolute differences—that is, the results of the subtractions—treating each result as a positive number even if it is negative. We have seen this approach before. Therefore let us now take the opportunity of showing another approach. Instead of working with the absolute differences, we square each difference, and then SUM the squares. An advantage of working with the squares is that they are positive—a negative number squared is positive—which is convenient. Additionally, conventional statistics works mainly with squared quantities, and therefore it is worth getting familiar with that point of view. The squared differences in this case add up to 5096. Using RESAMPLING STATS, we shuffle all the weight gains together, select four random groups, and determine whether the squared differences in the resample exceed 5096. If they do so with regularity, then we conclude that the observed differences could easily have occurred by chance. With the CONCAT command, we string the four vectors into a single vector. After SHUFFLEing the 48-pig weight-gain vector G into H, we TAKE four randomized samples. And we compute the squared differences between the pairs of groups and SUM the squared differences just as we did above for the observed groups. Last, we examine how often the simulated-trials data produce differences among the groups as large as (or larger than) the actually observed data—5096. NUMBERS (34 29 26 32 35 38 31 34 30 29 32 31) a NUMBERS (26 24 28 29 30 29 32 26 31 29 32 28) b NUMBERS (30 30 32 31 29 27 25 30 31 32 34 33) c NUMBERS (32 25 31 26 32 27 28 29 29 28 23 25) d (Record the data for the 4 foods) CONCAT a b c d g Combine the four vectors into g REPEAT 1000 Do 1000 trials SHUFFLE g h Shuffle all the weight gains. SAMPLE 12 h p Take 4 random samples, with replacement. SAMPLE 12 h q SAMPLE 12 h r SAMPLE 12 h s SUM p i Sum the weight gains for the 4 resamples. SUM q j SUM r k SUM s l SUBTRACT i j ij Find the differences between all the possible pairs of resamples. SUBTRACT i k ik SUBTRACT i l il SUBTRACT j k jk SUBTRACT j l jl SUBTRACT k l kl MULTIPLY ij ij ijsq Find the squared differences. MULTIPLY ik ik iksq MULTIPLY il il ilsq MULTIPLY jk jk jksq MULTIPLY jl jl jlsq MULTIPLY kl kl klsq ADD ijsq iksq ilsq jksq jlsq klsq total Add them together. SCORE total z Keep track of the total for each trial. END End one trial, go back and repeat until 1000 trials are complete. HISTOGRAM z Produce a histogram of the trial results. COUNT z &gt;= 5096 k Find out how many trials produced differences among groups as great as or greater than those observed. DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the result. Note: The file “pigs4” on the Resampling Stats software disk contains this set of commands. PIGS4: Differences Among Four Pig Rations sums of squares We find that our observed sum of squares—5096—was exceeded by randomly-drawn sums of squares in only 3 percent of our trials. We conclude that the four treatments are likely not all similar. 21.3 Endnotes Technical Note: The test described in this section is nonparametric and therefore makes no assumptions about the shapes of the distributions, which is good because we would be on soft ground if we assumed normality in the pig-food case, given the sample sizes. This test does not, however, throw away information as do the rank and median tests illustrated earlier. And indeed, this test proves to be more powerful than the other nonparametric tests. After developing this test, I discovered that its general logic follows the tradition of the “randomization” tests, based on an idea by R.A. Fisher (1935, 1951) and worked out for the two-sample cases by E.J.G. Pitman (1937). But the only earlier mentions of sampling from the universe of possibilities are in M. Dwass (1957) and J.H. Chung and D. Fraser (1958). I am grateful to J. Pratt for bringing the latter literature to my attention. The data are from The Liquor Handbook (1962, p. 68). Eight states are omitted for various reasons. For more information, see Simon and Simon (1987). Various tests indicate that the difference between the groups of states is highly significant. See J.L. Simon (1966b). Technical Note: Computing the sum of squared differences renders this test superficially more similar to the analysis of variance but will not alter the results. This test has not been discussed in the statistical literature, to my knowledge, except perhaps for a faint suggestion at the end of Chung and Fraser (1958). This and the two-sample test can easily be performed with canned computer programs as well as RESAMPLING STATS. In addition to their advantages of nonparametricity, they are equally efficient and vastly easier to teach and to understand than the t-test and the analysis of variance. Therefore, I believe that these tests should be “treatments of choice,” as the doctors say. 21.4 Exercises Solutions for problems may be found in the section titled, “Exercise Solutions” at the back of this book. Exercise 18-1 The data shown in Table 18-3 (Hollander-Wolfe, 1973, p. 29, Table 1) might be data for the outcomes of two different mechanics, showing the length of time until the next overhaul is needed for nine pairs of similar vehicles. Or they could be two readings made by different instruments on the same sample of rock. In fact, they represent data for two successive tests for depression on the Hamilton scale, before and after drug therapy. Table 18-3 Hamilton Depression Scale Values Patient # Score Before Score After 1 1.83 .878 2 .50 .647 3 1.62 .598 4 2.48 2.05 5 1.68 1.06 6 1.88 1.29 7 1.55 1.06 8 3.06 3.14 9 1.3 1.29 The task is to perform a test that will help decide whether there is a difference in the depression scores at the two visits (or the performances of the two mechanics). Perform both a bootstrap test and a permutation test, and give some reason for preferring one to the other in principle. How much do they differ in practice? Exercise 18-2 Thirty-six of 72 (.5) taxis surveyed in Pittsburgh had visible seatbelts. Seventy-seven of 129 taxis in Chicago (.597) had visible seatbelts. Calculate a confidence interval for the difference in proportions, estimated at -.097. (Source: Peskun, Peter H., “A New Confidence Interval Method Based on the Normal Approximation for the Difference of Two Binomial Probabilities,” Journal of the American Statistical Association , 6/93 p. 656). "],["general-procedures-for-testing-hypotheses.html", "22 General Procedures for Testing Hypotheses 22.1 Introduction 22.2 Canonical question-and-answer procedure for testing hypotheses 22.3 Skeleton procedure for testing hypotheses 22.4 An example: can the bio-engineer increase the female calf rate? 22.5 Conventional methods 22.6 Choice of the benchmark universe1 22.7 Why is statistics—and hypothesis testing—so difficult? 22.8 Endnote", " 22 General Procedures for Testing Hypotheses 22.1 Introduction The previous chapters have presented procedures for making statistical inferences that apply to both testing hypotheses and constructing confidence intervals: This chapter focuses on specific procedures for testing hypotheses. `The general idea in testing hypotheses is to ask: Is there some other universe which might well have produced the observed sample? So we consider alternative hypotheses. This is a straightforward exercise in probability, asking about behavior of one or more universes. The choice of another universe(s) to examine depends upon purposes and other considerations. 22.2 Canonical question-and-answer procedure for testing hypotheses 22.3 Skeleton procedure for testing hypotheses Akin to skeleton procedure for questions in probability and confidence intervals shown elsewhere The following series of questions will be repeated below in the context of a specific inference. What is the question? What is the purpose to be served by answering the question? Is this a “probability” or a “statistics” question? Assuming the Question is a Statistical Inference Question What is the form of the statistics question? Hypothesis test, or confidence interval, or other inference? One must first decide whether the conceptual-scientific question is of the form a) a test about the probability that some sample is likely to happen by chance rather than being very surprising (a test of a hypothesis), or b) a question about the accuracy of the estimate of a parameter of the population based upon sample evidence (a confidence interval): Assuming the Question Concerns Testing Hypotheses Will you state the costs and benefits of various outcomes, perhaps in the form of a “loss function”? If “yes,” what are they? How many samples of data have been observed? One, two, more than two? What is the description of the observed sample(s)? Raw data? Which characteristic(s) (parameters) of the population are of interest to you? What are the statistics of the sample(s) that refer to this (these) characteristics(s) in which you are interested? What comparison(s) to make? Samples to each other? Sample to particular universe(s)? If so, which? What is the benchmark (null) universe? This may include presenting the raw data and/or such summary statistics as the computed mean, median, standard deviation, range, interquartile range, other: If there is to be a Neyman-Pearson-type alternative universe, what is it? (In most cases the answer to this technical question is “no.”) Which symbols for the observed entities? Discrete or continuous? What values or ranges of values? Which sample(s) do you wish to compare to which, or to the null universe (and perhaps to the alternative universe)? (Answer: samples the same size as has been observed) \\[Here one may continue with the conventional method, using perhaps a *t* or *f* or chi-square test or whatever: Everything up to now is the same whether continuing with resampling or with standard parametric test.\\] What procedure will be used to produce the resampled entities? Randomly drawn? Simple (single step) or complex (multiple “if” drawings)? What procedure to produce resample? Which universe will you draw them from? With or without replacement? What size resamples? Number of resample trials? What to record as outcome of each resample trial? Mean, median, or whatever of resample? Classifying the outcomes What is the criterion of significance to be used in evaluating the results of the test? Stating the distribution of results Graph of each statistic recorded—occurrences for each value. Count the outcomes that exceed criterion and divide by number of trials. 22.4 An example: can the bio-engineer increase the female calf rate? The question. \\[From Hodges and Lehman, 1970\\]: Female calves are more valuable than male calves. A bio-engineer claims to have a method that can produce more females. He tests the procedure on ten of your pregnant cows, and the result is nine females. Should you believe that his method has some effect? That is, what is the probability of a result this surprising occurring by chance? The purpose: Female calves are more valuable than male. Inference? Yes. Test of hypothesis? Yes. Will you state the costs and benefits of various outcomes (or a loss function)? We need only say that the benefits of a method that works are very large, and if the results are promising, it is worth gathering more data to confirm results. How many samples of data are part of the significance test? One What is the size of the first sample about which you wish to make significance statements? Ten. What comparison(s) to make? Compare sample to benchmark universe. What is the benchmark universe that embodies the null hypothesis? 50-50 female, or 100/206 female. If there is to be a Neyman-Pearson alternative universe , what is it? None. Which symbols for the observed entities? Balls in bucket, or numbers. What values or ranges of values? 0-1, (1-100), or 101-206. Finite or infinite? Infinite. Which sample(s) do you wish to compare to which, or to the null universe (and perhaps to the alternative universe)? Ten calves compared to universe. What procedure to produce entities? Sampling with replacement, Simple (single step) or complex (multiple “if” drawings)? One can think of it either way. What to record as outcome of each resample trial? The proportion (or number) of females. What is the criterion to be used in the test? The probability that in a sample of ten calves, nine (or more) females would be drawn by chance from the benchmark universe of half females. (Or frame in terms of a significance level.) “One-tail” or “two-tail” test? One tail, because the farmer is only interested in females: Finding a large proportion of males would not be of interest, and would not cause one to reject the null hypothesis. Computation of the probability sought. The actual computation of probability may be done with several formulaic or sample-space methods, and with several resampling methods: I will first show a resampling method and then several con- ventional methods. The following material, which allows one to compare resampling and conventional methods, is more germane to the earlier explication of resampling taken altogether in earlier chapters than it is to the theory of hypothesis tests discussed in this chapter, but it is more expedient to present it here. Computation of Probabilities with Resampling We can do the problem by hand as follows: Constitute a bucket with either one blue and one pink ball, or 106 blue and 100 pink balls. Draw ten balls with replacement, count pinks, and record. Repeat step (2) say 400 times. Calculate proportion of results with 9 or 10 pinks. Or, we can take advantage of the speed and efficiency of the computer as follows: REPEAT 1000 GENERATE 10 1,2 a COUNT a =1 b SCORE b z END HISTOGRAM z COUNT z &gt;=9 k DIVIDE k 1000 kk PRINT kk Result: kk = 0.0108 This outcome implies that there is roughly a one percent chance that one would observe 9 or 10 female births in a single sample of 10 calves if the probability of a female on each birth is .5. This outcome should help the decision-maker decide about the plausibility of the bio-engineer’s claim to be able to increase the probability of female calves being born. 22.5 Conventional methods The Sample Space and First Principles Assume for a moment that our problem is a smaller one and therefore much easier—the probability of getting two females in two calves if the probability of a female is .5. One could then map out what mathematicians call the “sample space,” a technique that (in its simplest form) assigns to each outcome a single point, and find the proportion of points that correspond to a “success.” We list all four possible combinations—FF, FM, MF, MM. Now we look at the ratio of the number of combinations that have 2 females to the total, which is 1/4. We may then interpret this probability. We might also use this method for (say) five female calves in a row . We can make a list of possibilities such as FFFFF, MFFFF, MMFFF, MMMFFF...MFMFM...MMMMM. There will be 2*2*2*2*2 = 32 possibilities, and 64 and 128 possibilities for six and seven calves respectively. But when we get as high as ten calves, this method would become very troublesome. Sample Space Calculations For two females in a row, we could use the well known, and very simple, multiplication rule; we could do so even for ten females in a row. But calculating the probability of nine females in ten is a bit more complex. Pascal’s Triangle One can use Pascal’s Triangle to obtain binomial coefficients for p = .5 and a sample size of 10, focusing on those for 9 or 10 successes. Then calculate the proportion of the total cases with 9 or 10 “successes” in one direction, to find the proportion of cases that pass beyond the criterion of 9 females. The method of Pascal’s Triangle requires more complete understanding of the probabilistic system than does the resampling simulation described above because Pascal’s Triangle requires that one understand the entire structure; simulation requires only that you follow the rules of the model. The Quincunx The quincunx—a device that filters tiny balls through a set of bumper points not unlike a pinball machine, mentioned here simply for completeness—is more a simulation method than theoretical, but it may be considered “conventional.” Hence, it is included here. Table of Binomial Coefficients Pascal’s Triangle becomes cumbersome or impractical with large numbers—say, 17 females of 20 births—or with probabilities other than .5. One might produce the binomial coefficients by algebraic multiplication, but that, too, becomes tedious even with small sample sizes. One can also use the pre-computed table of binomial coefficients found in any standard text. But the probabilities for n = 10 and 9 or 10 females are too small to be shown. Binomial Formula For larger sample sizes, one can use the binomial formula. The binomial formula gives no deeper understanding of the statistical structure than does the Triangle (but it does yield a deeper understanding of the pure mathematics). With very large numbers, even the binomial formula is cumbersome. The Normal Approximation When the sample size becomes too large for any of the above methods, one can then use the Normal approximation, which yields results close to the binomial (as seen very nicely in the output of the quincunx). But use of the Normal distribution requires an estimate of the standard deviation, which can be derived either by formula or by resampling. (See a more extended parallel discussion in Chapter 21 on confidence intervals for the Bush-Dukakis comparison.) The desired probability can be obtained from the Z formula and a standard table of the Normal distribution found in every elementary text. The Z table can be made less mysterious if we generate it with simulation, or with graph paper or Archimedes’ method, using as raw material (say) five “continuous” (that is, non-binomial) distributions, many of which are skewed: 1) Draw samples of (say) 50 or 100. 2) Plot the means to see that the Normal shape is the outcome. Then 3) standardize with the standard deviation by marking the standard deviations onto the histograms. The aim of the above exercise and the heart of the conventional parametric method is to compare the sample result—the mean—to a standardized plot of the means of samples drawn from the universe of interest to see how likely it is that that universe produces means deviating as much from the universe mean as does our observed sample mean. The steps are: Establish the Normal shape—from the exercise above, or from the quincunx or Pascal’s Triangle or the binomial formula or the formula for the Normal approximation or some other device. Standardize that shape in standard deviations. Compute the Z score for the sample mean—that is, its deviation from the universe mean in standard deviations. Examine the Normal (or really, tables computed from graph paper, etc.) to find the probability of a mean deviating that far by chance. This is the canon of the procedure for most parametric work in statistics. (For some small samples, accuracy is improved with an adjustment.) 22.6 Choice of the benchmark universe1 In the example of the ten calves, the choice of a benchmark universe—a universe that (on average) produces equal proportions of males and females—seems rather straightforward and even automatic, requiring no difficult judgments. But in other cases the process requires more judgments. Let’s consider another case where the choice of a benchmark universe requires no difficult judgments. Assume the U.S. Department of Labor’s Bureau of Labor Statistics takes a very large sample—say, 20,000 persons—and finds a 10 percent unemployment rate. At some later time another but smaller sample is drawn—2,000 persons—showing an 11 percent unemployment rate. Should BLS conclude that unemployment has risen, or is there a large chance that the difference between 10 percent and 11 percent is due to sample variability? In this case, it makes rather obvious sense to ask how often a sample of 2,000 drawn from a universe of 10 percent unemployment (ignoring the variability in the larger sample) will be as different as 11 percent due solely to sample variability? This problem differs from that of the calves only in the proportions and the sizes of the samples. Let’s change the facts and assume that a very large sample had not been drawn and only a sample of 2,000 had been taken, indicating 11 percent unemployment. A policy-maker asks the probability that unemployment is above ten percent. It would still seem rather straightforward to ask how often a universe of 10 percent unemployment would produce a sample of 2000 with a proportion of 11 percent unemployed. Still another problem where the choice of benchmark hypothesis is relatively straightforward: Say that BLS takes two samples of 2000 persons a month apart, and asks whether there is a difference in the results. Pooling the two samples and examining how often two samples drawn from the pooled universe would be as different as observed seems obvious. One of the reasons that the above cases—especially the two-sample case—seem so clearcut is that the variance of the benchmark hypothesis is not an issue, being implied by the fact that the samples deal with proportions. If the data were continuous, however, this issue would quickly arise. Consider, for example, that the BLS might take the same sorts of samples and ask unemployed persons the lengths of time they had been unemployed. Comparing a small sample to a very large one would be easy to decide about. And even comparing two small samples might be straightforward—simply pooling them as is. But what about if you have a sample of 2,000 with data on lengths of unemployment spells with a mean of 30 days, and you are asked the probability that it comes from a universe with a mean of 25 days? Now there arises the question about the amount of variability to assume for that benchmark universe. Should it be the variability observed in the sample? That is probably an overestimate, because a universe with a smaller mean would probably have a smaller variance, too. So some judgment is required; there cannot be an automatic “objective” process here, whether one proceeds with the conventional or the resampling method. The example of the comparison of liquor retailing systems in Chapter 18 provides more material on this subject. 22.7 Why is statistics—and hypothesis testing—so difficult? Why is statistics such a difficult subject? The aforegoing procedural outline provides a window to the explanation. Hypothesis testing—as is also true of the construction of confidence intervals (but unlike simple probability problems)—involves a very long chain of reasoning, perhaps longer than in any other realm of systematic thinking. Furthermore, many decisions in the process require judgment that goes beyond technical analysis. All this emerges as one proceeds through the skeleton procedure above with any specific example. (Bayes’ rule also is very difficult intuitively, but that probably is a result of the twists and turns required in all complex problems in conditional probability. Decision-tree analysis is counter-intuitive, too, probably because it starts at the end instead of the beginning of the story, as we are usually accustomed to doing.) 22.8 Endnote 1. This is one of many issues that Peter Bruce first raised, and whose treatment here reflects back-and-forth discussion between us. "],["confidence-intervals-part-1-assessing-the-accuracy-of-samples.html", "23 Confidence Intervals, Part 1: Assessing the Accuracy of Samples 23.1 Introduction 23.2 Estimating the accuracy of a sample mean 23.3 The logic of confidence intervals 23.4 Computing confidence intervals 23.5 Procedure for estimating confidence intervals 23.6 Summary", " 23 Confidence Intervals, Part 1: Assessing the Accuracy of Samples 23.1 Introduction This chapter discusses how to assess the accuracy of a point estimate of the mean, median, or other statistic of a sample. We want to know: How close is our estimate of (say) the sample mean likely to be to the population mean? The chapter begins with an intuitive discussion of the relationship between a) a statistic derived from sample data, and b) a parameter of a universe from which the sample is drawn. Then we discuss the actual construction of confidence intervals using two different approaches which produce the same numbers though they have different logic. The following chapter shows illustrations of these procedures. The accuracy of an estimate is a hard intellectual nut to crack, so hard that for hundreds of years statisticians and scientists wrestled with the problem with little success; it was not until the last century or two that much progress was made. The kernel of the problem is learning the extent of the variation in the population. But whereas the sample mean can be used straightforwardly to estimate the population mean, the extent of variation in the sample does not directly estimate the extent of the variation in the population, because the variation differs at different places in the distribution, and there is no reason to expect it to be symmetrical around the estimate or the mean. The intellectual difficulty of confidence intervals is one reason why they are less prominent in statistics literature and practice than are tests of hypotheses (though statisticians often favor confidence intervals). Another reason is that tests of hypotheses are more fundamental for pure science because 308 Resampling: The New Statistics they address the question that is at the heart of all knowledge-getting: “Should these groups be considered different or the same ?” The statistical inference represented by confidence limits addresses what seems to be a secondary question in most sciences (though not in astronomy or perhaps physics): “How reliable is the estimate?” Still, confidence intervals are very important in some applied sciences such as geology—estimating the variation in grades of ores, for example—and in some parts of business and industry. Confidence intervals and hypothesis tests are not disjoint ideas. Indeed, hypothesis testing of a single sample against a benchmark value is (in all schools of thought, I believe) operationally identical with the most common way (Approach 1 below) of constructing a confidence interval and checking whether it includes that benchmark value. But the underlying reasoning is different for confidence limits and hypothesis tests. The logic of confidence intervals is on shakier ground, in my judgment, than that of hypothesis testing, though there are many thoughtful and respected statisticians who argue that the logic of confidence intervals is better grounded and leads less often to error. Confidence intervals are considered by many to be part of the same topic as estimation , being an estimation of accuracy, in their view. And confidence intervals and hypothesis testing are seen as sub-cases of each other by some people. Whatever the importance of these distinctions among these intellectual tasks in other contexts, they need not concern us here. 23.2 Estimating the accuracy of a sample mean If one draws a sample that is very, very large—large enough so that one need not worry about sample size and dispersion in the case at hand—from a universe whose characteristics one knows , one then can deduce the probability that the sample mean will fall within a given distance of the population mean. Intuitively, it seems as if one should also be able to reverse the process—to infer something about the location of the population mean from the sample mean . But this inverse inference turns out to be a slippery business indeed. Let’s put it differently: It is all very well to say—as one logically may—that on average the sample mean (or other point estimator) equals a population parameter in most situations. Chapter 20—Confidence Intervals, Par t 1: Assessing the Accuracy of Samples 309 But what about the result of any particular sample? How accurate or inaccurate an estimate of the population mean is the sample likely to produce? Because the logic of confidence intervals is subtle, most statistics texts skim right past the conceptual difficulties, and go directly to computation. Indeed, the topic of confidence intervals has been so controversial that some eminent statisticians refuse to discuss it at all. And when the concept is combined with the conventional algebraic treatment, the composite is truly baffling; the formal mathematics makes impossible any intuitive understanding. For students, “pluginski” is the only viable option for passing exams. With the resampling method, however, the estimation of confidence intervals is easy. The topic then is manageable though subtle and challenging—sometimes pleasurably so. Even beginning undergraduates can enjoy the subtlety and find that it feels good to stretch the brain and get down to fundamentals. One thing is clear: Despite the subtlety of the topic, the accuracy of estimates must be dealt with, one way or another. I hope the discussion below resolves much of the confusion of the topic. 23.3 The logic of confidence intervals To preview the treatment of confidence intervals presented below: We do not learn about the reliability of sample estimates of the mean (and other parameters) by logical inference from any one particular sample to any one particular universe, because this cannot be done in principle . Instead, we investigate the behavior of various universes in the neighborhood of the sample, universes whose characteristics are chosen on the basis of their similarity to the sample. In this way the estimation of confidence intervals is like all other statistical inference: One investigates the probabilistic behavior of one or more hypothesized universes that are implicitly suggested by the sample evidence but are not logically implied by that evidence. The examples worked in the following chapter help explain why statistics is a difficult subject. The procedure required to transit successfully from the original question to a statistical probability, and then through a sensible interpretation of the 310 Resampling: The New Statistics probability, involves a great many choices about the appropriate model based on analysis of the problem at hand; a wrong choice at any point dooms the procedure. The actual computation of the probability—whether done with formulaic probability theory or with resampling simulation—is only a very small part of the procedure, and it is the least difficult part if one proceeds with resampling. The difficulties in the statistical process are not mathematical but rather stem from the hard clear thinking needed to understand the nature of the situation and to ascertain the appropriate way to model it. Again, the purpose of a confidence interval is to help us assess the reliability of a statistic of the sample—for example, its mean or median—as an estimator of the parameter of the universe. The line of thought runs as follows: It is possible to map the distribution of the means (or other such parameter) of samples of any given size (the size of interest in any investigation usually being the size of the observed sample) and of any given pattern of dispersion (which we will assume for now can be estimated from the sample) that a universe in the neighborhood of the sample will produce. For example, we can compute how large an interval to the right and left of a postulated universe’s mean is required to include 45 percent of the samples on either side of the mean. What cannot be done is to draw conclusions from sample evidence about the nature of the universe from which it was drawn, in the absence of some information about the set of universes from which it might have been drawn. That is, one can investigate the behavior of one or more specified universes, and discover the absolute and relative probabilities that the given specified universe(s) might produce such a sample. But the universe(s) to be so investigated must be specified in advance (which is consistent with the Bayesian view of statistics). To put it differently, we can employ probability theory to learn the pattern(s) of results produced by samples drawn from a particular specified universe, and then compare that pattern to the observed sample. But we cannot infer the probability that that sample was drawn from any given universe in the absence of knowledge of the other possible sources of the sample. That is a subtle difference, I know, but I hope that the following discussion makes it understandable. Chapter 20—Confidence Intervals, Par t 1: Assessing the Accuracy of Samples 311 23.4 Computing confidence intervals In the first part of the discussion we shall leave aside the issue of estimating the extent of the dispersion—a troublesome matter, but one which seldom will result in unsound conclusions even if handled crudely. To start from scratch again: The first— and seemingly straightforward—step is to estimate the mean of the population based on the sample data. The next and more complex step is to ask about the range of values (and their probabilities) that the estimate of the mean might take—that is, the construction of confidence intervals. It seems natural to assume that if our best guess about the population mean is the value of the sample mean, our best guesses about the various values that the population mean might take if unbiased sampling error causes discrepancies between population parameters and sample statistics, should be values clustering around the sample mean in a symmetrical fashion (assuming that asymmetry is not forced by the distribution—as for example, the binomial is close to symmetric near its middle values). But how far away from the sample mean might the population mean be? Let’s walk slowly through the logic, going back to basics to enhance intuition. Let’s start with the familiar saying, “The apple doesn’t fall far from the tree.” Imagine that you are in a very hypothetical place where an apple tree is above you, and you are not allowed to look up at the tree, whose trunk has an infinitely thin diameter. You see an apple on the ground. You must now guess where the trunk (center) of the tree is. The obvious guess for the location of the trunk is right above the apple. But the trunk is not likely to be exactly above the apple because of the small probability of the trunk being at any particular location, due to sampling dispersion. Though you find it easy to make a best guess about where the mean is (the true trunk), with the given information alone you have no way of making an estimate of the probability that the mean is one place or another, other than that the probability is the same that the tree is to the north or south, east or west, of you. You have no idea about how far the center of the tree is from you. You cannot even put a maximum on the distance it is from you, and without a maximum you could not even reasonably assume a rectangular distribution, or a Normal distribution, or any other. 312 Resampling: The New Statistics Next you see two apples. What guesses do you make now? The midpoint between the two obviously is your best guess about the location of the center of the tree. But still there is no way to estimate the probability distribution of the location of the center of the tree. Now assume you are given still another piece of information: The outermost spread of the tree’s branches (the range) equals the distance between the two apples you see. With this information, you could immediately locate the boundaries of the location of the center of the tree. But this is only because the answer you sought was given to you in disguised form. You could, however, come up with some statements of relative probabilities. In the absence of prior information on where the tree might be, you would offer higher odds that the center (the trunk) is in any unit of area close to the center of your two apples than in a unit of area far from the center. That is, if you are told that either one apple, or two apples, came from one of two specified trees whose locations are given , with no reason to believe it is one tree or the other (later, we can put other prior probabilities on the two trees), and you are also told the dispersions, you now can put relative probabilities on one tree or the other being the source. (Note to the advanced student: This is like the Neyman-Pearson procedure, and it is easily reconciled with the Bayesian point of view to be explored later. One can also connect this concept of relative probability to the Fisherian concept of maximum likelihood—which is a probability relative to all others). And you could list from high to low the probabilities for each unit of area in the neighborhood of your apple sample. But this procedure is quite different from making any single absolute numerical probability estimate of the location of the mean. Now let’s say you see 10 apples on the ground. Of course your best estimate is that the trunk of the tree is at their arithmetic center. But how close to the actual tree trunk (the population mean) is your estimate likely to be? This is the question involved in confidence intervals. We want to estimate a range (around the center, which we estimate with the center mean of the sample, we said) within which we are pretty sure that the trunk lies. To simplify, we consider variation along only one dimension— that is, on (say) a north-south line rather than on two dimensions (the entire surface). Chapter 20—Confidence Intervals, Par t 1: Assessing the Accuracy of Samples 313 We first note that you have no reason to estimate the trunk’s location to be outside the sample pattern, or at its edge, though it could be so in principle. If the pattern of the 10 apples is tight, you imagine the pattern of the likely locations of the population mean to be tight; if not, not. That is, it is intuitively clear that there is some connection between how spread out are the sample obervations and your confidence about the location of the population mean . For example, consider two patterns of a thousand apples, one with twice the spread of another, where we measure spread by (say) the diameter of the circle that holds the inner half of the apples for each tree, or by the standard deviation. It makes sense that if the two patterns have the same center point (mean), you would put higher odds on the tree with the smaller spread being within some given distance—say, a foot—of the estimated mean. But what odds would you give on that bet? 23.5 Procedure for estimating confidence intervals Here is a canonical list of questions that help organize one’s thinking when constructing confidence intervals. The list is comparable to the lists for questions in probability and for hypothesis testing provided in earlier chapters. This set of questions will be applied operationally in Chapter 21. What Is The Question? What is the purpose to be served by answering the question? Is this a “probability” or a “statistics” question? If the Question Is a Statistical Inference Question: What is the form of the statistics question? Hypothesis test or confidence limits or other inference? Assuming Question Is About Confidence Limits: What is the description of the sample that has been observed? Raw data? Statistics of the sample? 314 Resampling: The New Statistics Which universe? Assuming that the observed sample is representative of the universe from which it is drawn, what is your best guess of the properties of the universe whose parameter you wish to make statements about? Finite or infinite? Bayesian possibilities? Which parameter do you wish to make statements about? Mean, median, standard deviation, range, interquartile range, other? Which symbols for the observed entities? Discrete or continuous? What values or ranges of values? If the universe is as guessed at, for which samples do you wish to estimate the variation? (Answer: samples the same size as has been observed) Here one may continue with the conventional method, using perhaps a t or F or chi-square test or whatever. Everything up to now is the same whether continuing with resampling or with standard parametric test. What procedure to produce the original entities in the sample? What universe will you draw them from? Random selection? What size resample? Simple (single step) or complex (multiple “if” drawings)? What procedure to produce resamples? With or without replacement? Number of drawings? What to record as result of resample drawing? Mean, median, or whatever of resample Stating the Distribution of Results Histogram, frequency distribution, other? Choice Of Confidence Bounds Oneor two-tailed? 90%, 95%, etc.? Computation of Probabilities Within Chosen Bounds Chapter 20—Confidence Intervals, Par t 1: Assessing the Accuracy of Samples 315 23.6 Summary This chapter discussed the theoretical basis for assessing the accuracy of population averages from sample data. The following chapter shows two very different approaches to confidence intervals, and provides examples of the computations. "],["confidence-intervals-part-2-the-two-approaches-to-estimating-confidence-intervals.html", "24 Confidence Intervals, Part 2: The Two Approaches to Estimating Confidence Intervals 24.1 Approach 1: The distance between sample and population mean 24.2 Conventional Calculational Methods 24.3 Confidence Intervals Empirically—With Resampling 24.4 Approach 2: Probability of various universes producing this sample 24.5 Interpretation of Approach 2 24.6 Exercises", " 24 Confidence Intervals, Part 2: The Two Approaches to Estimating Confidence Intervals There are two broad conceptual approaches to the question at hand: 1) Study the probability of various distances between the sample mean and the likeliest population mean; and 2) study the behavior of particular border universes. Computationally, both approaches often yield the same result, but their interpretations differ. Approach 1 follows the conventional logic although carrying out the calculations with resampling simulation. 24.1 Approach 1: The distance between sample and population mean If the study of probability can tell us the probability that a given population will produce a sample with a mean at a given distance x from the population mean, and if a sample is an unbiased estimator of the population, then it seems natural to turn the matter around and interpret the same sort of data as telling us the probability that the estimate of the population mean is that far from the “actual” population mean. A fly in the ointment is our lack of knowledge of the dispersion, but we can safely put that aside for now. (See below, however.) This first approach begins by assuming that the universe that actually produced the sample has the same amount of dispersion (but not necessarily the same mean) that one would estimate from the sample. One then produces (either with resampling or with Normal distribution theory) the distribution of sample means that would occur with repeated sampling from that designated universe with samples the size of the observed sample. One can then compute the distance between the (assumed) population mean and (say) the inner 45 percent of sample means on each side of the actuallyobserved sample mean. The crucial step is to shift vantage points. We look from the sample to the universe, instead of from a hypothesized universe to simulated samples (as we have done so far). This same interval as computed above must be the relevant distance as when one looks from the sample to the universe. Putting this algebraically, we can state (on the basis of either simulation or formal calculation) that for any given population S, and for any given distance d from its mean mu, that p(mu xbar) &lt; d) = alpha, where xbar is a randomlygenerated sample mean and alpha is the probability resulting from the simulation or calculation. The above equation focuses on the deviation of various sample means (xbar) from a stated population mean (mu). But we are logically entitled to read the algebra in another fashion, focusing on the deviation of mu from a randomly generated sample mean. This implies that for any given randomly generated sample mean we observe, the same probability (alpha) describes the probability that mu will be at a distance d or less from the observed xbar. (I believe that this is the logic underlying the conventional view of confidence intervals, but I have yet to find a clear-cut statement of it; in any case, it appears to be logically correct.) To repeat this difficult idea in slightly different words: If one draws a sample (large enough to not worry about sample size and dispersion), one can say in advance that there is a probability p that the sample mean (xbar) will fall within z standard deviations of the population mean (mu). One estimates the population dispersion from the sample. If there is a probability p that xbar is within z standard deviations of mu, then with probability p, mu must be within that same z standard deviations of xbar. To repeat, this is, I believe, the heart of the standard concept of the confidence interval, to the extent that there is thoughtthrough consensus on the matter. So we can state for such populations the probability that the distance between the population and sample means will be d or less. Or with respect to a given distance, we can say that the probability that the population and sample means will be that close together is p. That is, we start by focusing on how much the sample mean diverges from the known population mean. But then—and to repeat once more this key conceptual step—we refocus our attention to begin with the sample mean and then discuss the probability that the population mean will be within a given distance. The resulting distance is what we call the “confidence interval.” Please notice that the distribution (universe) assumed at the beginning of this approach did not include the assumption that the distribution is centered on the sample mean or anywhere else. It is true that the sample mean is used for purposes of reporting the location of the estimated universe mean . But despite how the subject is treated in the conventional approach, the estimated population mean is not part of the work of constructing confidence intervals. Rather, the calculations apply in the same way to all universes in the neighborhood of the sample (which are assumed, for the purpose of the work, to have the same dispersion). And indeed, it must be so, because the probability that the universe from which the sample was drawn is centered exactly at the sample mean is very small. This independence of the confidence-intervals construction from the mean of the sample (and the mean of the estimated universe) is surprising at first, but after a bit of thought it makes sense. In this first approach, as noted more generally above, we do not make estimates of the confidence intervals on the basis of any logical inference from any one particular sample to any one particular universe, because this cannot be done in principle ; it is the futile search for this connection that for decades roiled the brains of so many statisticians and now continues to trouble the minds of so many students. Instead, we investigate the behavior of (in this first approach) the universe that has a higher probability of producing the observed sample than does any other universe (in the absence of any additional evidence to the contrary), and whose characteristics are chosen on the basis of its resemblance to the sample. In this way the estimation of confidence intervals is like all other statistical inference: One investigates the probabilistic behavior of one or more hypothesized universes, the universe(s) being implicitly suggested by the sample evidence but not logically implied by that evidence. And there are no grounds for dispute about exactly what is being done—only about how to interpret the results. One difficulty with the above approach is that the estimate of the population dispersion does not rest on sound foundations; this matter will be discussed later, but it is not likely to lead to a seriously misleading conclusion. A second difficulty with this approach is in interpreting the result. What is the justification for focusing our attention on a universe centered on the sample mean? While this particular universe may be more likely than any other, it undoubtedly has a low probability. And indeed, the statement of the confidence intervals refers to the probabilities that the sample has come from universes other than the universe centered at the sample mean, and quite a distance from it. My answer to this question does not rest on a set of meaningful mathematical axioms, and I assert that a meaningful axiomatic answer is impossible in principle. Rather, I reason that we should consider the behavior of this universe because other universes near it will produce much the same results, differing only in dispersion from this one, and this difference is not likely to be crucial; this last assumption is all-important, of course. True, we do not know what the dispersion might be for the “true” universe. But elsewhere (Simon, forthcoming) I argue that the concept of the “true universe” is not helpful— or maybe even worse than nothing—and should be foresworn. And we can postulate a dispersion for any other universe we choose to investigate. That is, for this postulation we unabashedly bring in any other knowledge we may have. The defense for such an almost-arbitrary move would be that this is a second-order matter relative to the location of the estimated universe mean, and therefore it is not likely to lead to serious error. (This sort of approximative guessing sticks in the throats of many trained mathematicians, of course, who want to feel an unbroken logic leading backwards into the mists of axiom formation. But the axioms themselves inevitably are chosen arbitrarily just as there is arbitrariness in the practice at hand, though the choice process for axioms is less obvious and more hallowed by having been done by the masterminds of the past. \\[See Simon, forthcoming, on the necessity for judgment.\\] The absence of a sequence of equations leading from some first principles to the procedure described in the paragraph above is evidence of what is felt to be missing by those who crave logical justification. The key equation in this approach is formally unassailable, but it seems to come from nowhere.) In the examples in the following chapter may be found computations for two population distributions—one binomial and one quantitative—of the histograms of the sample means produced with this procedure. Operationally, we use the observed sample mean, together with an estimate of the dispersion from the sample, to estimate a mean and dispersion for the population. Then with reference to the sample mean we state a combination of a dis- tance (on each side) and a probability pertaining to the population mean. The computational examples will illustrate this procedure. Once we have obtained a numerical answer, we must decide how to interpret it. There is a natural and almost irresistible tendency to talk about the probability that the mean of the universe lies within the intervals, but this has proven confusing and controversial. Interpretation in terms of a repeated process is not very satisfying intuitively 1 . In my view, it is not 1 An example of this sort of interpretation is as follows: specific sample mean Xbar that we happen to observe is almost certain to be a bit high or a bit low. Accordingly, if we want to be reasonably confident that our inference is correct, we cannot claim that mu is precisely equal to the observed Xbar. Instead, we must construct an interval estimate or confidence interval of the form: mu = Xbar + sampling error The crucial question is: How wide must this allowance for sampling error be? The answer, of course, will depend on how much Xbar fluctuates... Constructing 95% confidence intervals is like pitching horseshoes. In each there is a fixed target, either the population mu or the stake. We are trying to bracket it with some chancy device, either the random interval or the horseshoe… There are two important ways, however, that confidence intervals differ from pitching horseshoes. First, only one confidence interval is customarily constructed. Second, the target mu is not visible like a horseshoe stake. Thus, whereas the horseshoe player always knows the score (and specifically, whether or not the last toss bracketed the stake), the statistician does not. He continues to “throw in the dark,” without knowing whether or not a specific interval estimate has bracketed mu. All he has to go on is the statistical theory that assures him that, in the long run, he will succeed 95% of the time. Wonnacott and Wonnacott (1990), (p. 258). Savage refers to this type of interpretation as follows: whenever its advocates talk of making assertions that have high probability, whether in connection with testing or estimation, they do not actually make such assertions themselves, but endlessly pass the buck, saying in effect, “This assertion has arisen according to a system that will seldom lead you to make false assertions, if you adopt it. As for myself, I assert nothing but the properties of the system.”(1972, pp. 260261) Lee writes at greater length: \"\\[T\\]he statement that a 95% confidence interval for an unknown parameter ran from 2 to +2 sounded as if the parameter lay in that interval with 95% probability and yet I was warned that all I could say was that if I carried out similar procedures time after time then the unknown parameters would lie in the confidence intervals I constructed 95% of the time. “Subsequently, I discovered that the whole theory had been worked out in very considerable detail in such books as Lehmann (1959, 1986). But attempts such as those that Lehmann describes to put everything on a firm foundation raised even more questions.” (Lee, 1989, p. vii) worth arguing about any “true” interpretation of these computations. One could sensibly interpret the computations in terms of the odds a decisionmaker, given the evidence, would reasonably offer about the relative probabilities that the sample came from one of two specified universes (one of them probably being centered on the sample); this does provide some information on reliability, but this procedure departs from the concept of confidence intervals. Example 21-1: Counted Data: The Accuracy of Political Polls Consider the reliability of a randomly selected 1988 presidential election poll, showing 840 intended votes for Bush and 660 intended votes for Dukakis out of 1500 (Wonnacott and Wonnacott 1990) (p. 5). Let us work through the logic of this example. What is the question? Stated technically, what are the 95% confidence limits for the proportion of Bush supporters in the population? (The proportion is the mean of a binomial population or sample, of course.) More broadly, within which bounds could one confidently believe that the population proportion was likely to lie? At this stage of the work, we must already have translated the conceptual question (in this case, a decisionmaking question from the point of view of the candidates) into a statistical question. (See Chapter 14 on translating questions into statistical form.) What is the purpose to be served by answering this question? There is no sharp and clear answer in this case. The goal could be to satisfy public curiosity, or strategy planning for a candidate (though a national proportion is not as helpful for planning strategy as state data would be). A secondary goal might be to help guide decisions about the sample size of subsequent polls. Is this a “probability” or a “probability-statistics” question? The latter; we wish to infer from sample to population rather than the converse. Given that this is a statistics question: What is the form of the statistics question—confidence limits or hypothesis testing? Confidence limits. Given that the question is about confidence limits: What is the description of the sample that has been observed? a) The raw sample data—the observed numbers of interviewees are 840 for Bush and 660 for Dukakis—constitutes the best description of the universe. The statistics of the sample are the given proportions—56 percent for Bush, 44 percent for Dukakis. Which universe? (Assuming that the observed sample is representative of the universe from which it is drawn, what is your best guess about the properties of the universe about whose parameter you wish to make statements? The best guess is that the population proportion is the sample proportion—that is, the population contains 56 percent Bush votes, 44 percent Dukakis votes. Possibilities for Bayesian analysis ? Not in this case, unless you believe that the sample was biased somehow. Which parameter(s) do you wish to make statements about? Mean, median, standard deviation, range, interquartile range, other? We wish to estimate the proportion in favor of Bush (or Dukakis). Which symbols for the observed entities? Perhaps 56 green and 44 yellow balls, if a bucket is used, or “0” and “1” if the computer is used. Discrete or continuous distribution? In principle, discrete. ( All distributions must be discrete in practice .) What values or ranges of values? “0” or “1.” Finite or infinite? Infinite—the sample is small relative to the population. If the universe is what you guess it to be, for which samples do you wish to estimate the variation? A sample the same size as the observed poll. Here one may continue either with resampling or with the conventional method. Everything done up to now would be the same whether continuing with resampling or with a standard parametric test. 24.2 Conventional Calculational Methods Estimating the Distribution of Differences Between Sample and Population Means With the Normal Distribution . In the conventional approach, one could in principle work from first principles with lists and sample space, but that would surely be too cumbersome. One could work with binomial proportions, but this problem has too large a sample for tree-drawing and quincunx techniques; even the ordinary textbook table of binomial coefficients is too small for this job. Calculating binomial coefficients also is a big job. So instead one would use the Normal approximation to the binomial formula. (Note to the beginner: The distribution of means that we manipulate has the Normal shape because of the operation of the Law of Large Numbers (The Central Limit theorem). Sums and averages, when the sample is reasonably large, take on this shape even if the underlying distribution is not Normal. This is a truly astonishing property of randomlydrawn samples— the distribution of their means quickly comes to resemble a “Normal” distribution, no matter the shape of the underlying distribution. We then standardize it with the standard deviation or other devices so that we can state the probability distribution of the sampling error of the mean for any sample of reasonable size.) The exercise of creating the Normal shape empirically is simply a generalization of particular cases such as we will later create here for the poll by resampling simulation. One can also go one step further and use the formula of de Moivre-Laplace- Gauss to describe the empirical distributions, and to serve instead of the empirical distributions. Looking ahead now, the difference between resampling and the conventional approach can be said to be that in the conventional approach we simply plot the Gaussian distribution very carefully, and use a formula instead of the empirical histograms, afterwards putting the results in a standardized table so that we can read them quickly without having to recreate the curve each time we use it. More about the nature of the Normal distribution may be found in Simon (forthcoming). All the work done above uses the information specified previously—the sample size of 1500, the drawing with replacement, the observed proportion as the criterion. 24.3 Confidence Intervals Empirically—With Resampling Estimating the Distribution of Differences Between Sample and Population Means By Resampling What procedure to produce entities? Random selection from bucket or computer. Simple (single step) or complex (multiple “if” drawings)? Simple. What procedure to produce resamples? That is, with or without replacement? With replacement. Number of drawings observations in actual sample, and hence, number of drawings in resamples ? 1500. What to record as result of each resample drawing? Mean, median, or whatever of resample? The proportion is what we seek. Stating the distribution of results : The distribution of proportions for the trial samples. Choice of confidence bounds? : 95%, two tails (choice made by the textbook that posed the problem). Computation of probabilities within chosen bounds : Read the probabilistic result from the histogram of results. Computation of upper and lower confidence bounds: Locate the values corresponding to the 2.5 th and 97.5 th percentile of the resampled proportions. Because the theory of confidence intervals is so abstract (even with the resampling method of computation), let us now walk through this resampling demonstration slowly, using the conventional Approach 1 described previously. We first produce a sample, and then see how the process works in reverse to estimate the reliability of the sample, using the Bush-Dukakis poll as an example. The computer program follows below. Step 1: Draw a sample of 1500 voters from a universe that, based on the observed sample, is 56 percent for Bush, 44 percent for Dukakis. The first such sample produced by the computer happens to be 53 percent for Bush; it might have been 58 percent, or 55 percent, or very rarely, 49 percent for Bush. Step 2: Repeat step 1 perhaps 400 or 1000 times. Step 3: Estimate the distribution of means (proportions) of samples of size 1500 drawn from this 56-44 percent Bush- Dukakis universe; the resampling result is shown below. Step 4: In a fashion similar to what was done in steps 13, now compute the 95 percent confidence intervals for some other postulated universe mean—say 53% for Bush, 47% for Dukakis. This step produces a confidence interval that is not centered on the sample mean and the estimated universe mean, and hence it shows the independence of the procedure from that magnitude. And we now compare the breadth of the estimated confidence interval generated with the 53-47 percent universe against the confidence interval derived from the corresponding distribution of sample means generated by the “true” Bush-Dukakis population of 56 percent—44 percent. If the procedure works well, the results of the two procedures should be similar. Now we interpret the results using this first approach. The histogram shows the probability that the difference between the sample mean and the population mean—the error in the sample result—will be about 2.5 percentage points too low. It follows that about 47.5 percent (half of 95 percent) of the time, a sample like this one will be between the population mean and 2.5 percent too low. We do not know the actual population mean. But for any observed sample like this one, we can say that there is a 47.5 percent chance that the distance between it and the mean of the population that generated it is minus 2.5 percent or less. Now a crucial step: We turn around the statement just above, and say that there is an 47.5 percent chance that the population mean is less than three percentage points higher than the mean of a sample drawn like this one, but at or above the sample mean. (And we do the same for the other side of the sample mean.) So to recapitulate: We observe a sample and its mean. We estimate the error by experimenting with one or more universes in that neighborhood, and we then give the probability that the population mean is within that margin of error from the sample mean. Example 21-2: Measured Data Example—the Bootstrap: A Feed Merchant Experiences Varied Pig Weight Gains With a New Ration and Wants to be Safe in Advertising an Average Weight Gain A feed merchant decides to experiment with a new pig ration— ration A—on twelve pigs. To obtain a random sample, he provides twelve customers (selected at random) with sufficient food for one pig. After 4 weeks, the 12 pigs experience an average gain of 508 ounces. The weight gain of the individual pigs are as follows: 496, 544, 464, 416, 512, 560, 608, 544, 480, 466, 512, 496. The merchant sees that the ration produces results that are quite variable (from a low of 466 ounces to a high of 560 ounces) and is therefore reluctant to advertise an average weight gain of 508 ounces. He speculates that a different sample of pigs might well produce a different average weight gain. Unfortunately, it is impractical to sample additional pigs to gain additional information about the universe of weight gains. The merchant must rely on the data already gathered. How can these data be used to tell us more about the sampling variability of the average weight gain? Recalling that all we know about the universe of weight gains is the sample we have observed, we can replicate that sample millions of times, creating a “pseudo-universe” that embodies all our knowledge about the real universe. We can then draw additional samples from this pseudo-universe and see how they behave. More specifically, we replicate each observed weight gain millions of times—we can imagine writing each result that many times on separate pieces of paper—then shuffle those weight gains and pick out a sample of 12. Average the weight gain for that sample, and record the result. Take repeated samples, and record the result for each. We can then make a histogram of the results; it might look something like this: Though we do not know the true average weight gain, we can use this histogram to estimate the bounds within which it falls. The merchant can consider various weight gains for advertising purposes, and estimate the probability that the true weight gain falls below the value. For example, he might wish to advertise a weight gain of 500 ounces. Examining the histogram, we see that about 36% of our samples yielded weight gains less than 500 ounces. The merchant might wish to choose a lower weight gain to advertise, to reduce the risk of overstating the effectiveness of the ration. This illustrates the “bootstrap” method. By re-using our original sample many times (and using nothing else), we are able to make inferences about the population from which the sample came. This problem would conventionally be addressed with the “t-test.” Example 21-3: Measured Data Example: Estimating Tree Diameters What is the question? A horticulturist is experimenting with a new type of tree. She plants 20 of them on a plot of land, and measures their trunk diameter after two years. She wants to establish a 90% confidence interval for the population average trunk diameter. For the data given below, calculate the mean of the sample and calculate (or describe a simulation procedure for calculating) a 90% confidence interval around the mean. Here are the 20 diameters, in centimeters and in no particular order: Table 21-1 Tree Diameters, in Centimeters 8.5 7.6 9.3 5.5 11.4 6.9 6.5 12.9 8.7 4.8 4.2 8.1 6.5 5.8 6.7 2.4 11.1 7.1 8.8 7.2 What is the purpose to be served by answering the question? Either research &amp; development, or pure science. Is this a “probability” or a “statistics” question? Statistics. What is the form of the statistics question? Confidence limits. What is the description of the sample that has been observed? The raw data as shown above. Statistics of the sample ? Mean of the tree data. Which universe? Assuming that the observed sample is representative of the universe from which it is drawn, what is your best guess about the properties of the universe whose parameter you wish to make statements about? Answer: The universe is like the sample above but much, much bigger. That is, in the absence of other information, we imagine this “bootstrap” universe as a collection of (say) one million trees of 8.5 centimeters width, one million of 7.2 centimeters, and so on. We’ll see in a moment that the device of sampling with replacement makes it unnecessary for us to work with such a large universe; by replacing each element after we draw it in a resample, we achieve the same effect as creating an almost-infinite universe from which to draw the resamples. (Are there possibilities for Bayesian analysis?) No Bayesian prior information will be included. Which parameter do you wish to make statements about? The mean. Which symbols for the observed entities? Cards or computer entries with numbers 8.5…7.2, sample of an infinite size. If the universe is as guessed at, for which samples do you wish to estimate the variation? Samples of size 20. Here one may continue with the conventional method. Everything up to now is the same whether continuing with resampling or with a standard parametric test. The information listed above is the basis for a conventional test. Continuing with resampling: What procedure will be used to produce the trial entities? Random selection: Simple (single step), not complex (multiple “if”) sample drawings). What procedure to produce resamples? With replacement. As noted above, sampling with replacement allows us to forego creating a very large bootstrap universe; replacing the elements after we draw them achieves the same effect as would an infinite universe. Number of drawings ? 20 trees What to record as result of resample drawing? The mean. How to state the distribution of results? See histogram. Choice of confidence bounds : 90%, two-tailed. Computation of values of the resample statistic corresponding to chosen confidence bounds: Read from histogram. As has been discussed in Chapter 13, it often is more appropriate to work with the median than with the mean. One reason is that the median is not so sensitive to the extreme observations as is the mean. Another reason is that one need not assume a Normal distribution for the universe under study: this consideration affects conventional statistics but usually does not affect resampling, but it is worth keeping mind when a statistician is making a choice between a parametric (that is, Normal-based) and a non-parametric procedure. Example 21-4: Determining a Confidence Interval for the Median Aluminum Content in Theban Jars Data for the percentages of aluminum content in a sample of 18 ancient Theban jars (Desphpande et. al., 1996, p. 31) are as follows, arranged in ascending order: 11.4, 13.4, 13.5, 13.8, 13.9, 14.4, 14.5, 15.0, 15.1, 15.8, 16.0, 16.3, 16.5, 16.9, 17.0, 17.2, 17.5, 19.0. Consider now putting a confidence interval around the median of 15.45 (halfway between the middle observations 15.1 and 15.8). One may simply estimate a confidence interval around the median with a bootstrap procedure by substituting the median for the mean in the usual bootstrap procedure for estimating a confidence limit around the mean, as follows: DATA (11.4 13.4 13.5 13.8 13.9 14.4 14.5 15.0 15.1 15.8 16.0 16.3 16.5 16.9 17.0 17.2 17.5 19.0) c REPEAT 1000 SAMPLE 18 c c$ MEDIAN c$ d$ SCORE d$ z END HISTOGRAM z PERCENTILE z (2.5 97.5) k PRINT K This problem would be approached conventionally with a binomial procedure leading to quite wide confidence intervals (Deshpande, p. 32). Example 21-5: Determining a Confidence Interval for the Median Price Elasticity of Demand for Cigarettes The data for a measure of responsiveness of demand to a price change (the “elasticity”—percent change in demand divided by percent change in price) are shown for cigarette price changes as follows: Table 21-2 1.725 1.139 .957 .863 .802 .517 .407 .304 .204 .125 .122 .106 .031 -.032 -.1 -.142 -.174 -.234 -.240 -.251 -.277 -.301 -.302 -.302 -.307 -.328 -.329 -.346 -.357 -.376 -.377 -.383 -.385 -.393 -.444 -.482 -.511 -.538 -.541 -.549 -.554 -.600 -.613 -.644 -.692 -.713 -.724 -.734 -.749 -.752 -.753 -.766 -.805 -.866 -.926 -.971 -.972 -.975 -1.018 -1.024 -1.066 -1.118 -1.145 -1.146 -1.157 -1.282 -1.339 -1.420 -1.443 -1.478 -2.041 -2.092 -7.100 Price elasticity of demand in various states at various dates (computed from cigarette sales data preceding and following a tax change in a state) (Lyon &amp; Simon, 1968) The positive observations (implying an increase in demand when the price rises) run against all theory, but can be considered to be the result simply of measurement errors, and treated as they stand. Aside from this minor complication, the reader may work this example similarly to the case of the Theban jars. Here is a RESAMPLING STATS program (“cigarett”). READ file “cigaret.dat” elast Read in the data from an external file MEDIAN elast med-elas REPEAT 1000 SAMPLE 73 elast elast$ A bootstrap sample (note that the “$” indicates a bootstrap counterpart to the observed sample) MEDIAN elast$ med$ SCORE med$ scrboard END HISTOGRAM scrboard PERCENTILE scrboard (2.5 97.5) interval PRINT med-elas interval Result: MED-ELAS = -0.511 \\[observed value\\] INTERVAL = -0.692 -0.357 \\[estimated 95 percent confidence interval\\] Example 21-6: Measured Data Example: Confidence Intervals For a Difference Between Two Means, the Mice Data Again Returning to the data on the survival times of the two groups of mice in Example 18-4: It is the view of this book that confidence intervals should be calculated for a difference between two groups only if one is reasonably satisfied that the difference is not due to chance. Some statisticians might choose to compute a confidence interval in this case nevertheless, some because they believe that the confidence-interval machinery is more appropriate to deciding whether the difference is the likely outcome of chance than is the machinery of a hypothesis test in which you are concerned with the behavior of a benchmark or null universe. So let us calculate a confidence interval for these data, which will in any case demonstrate the technique for determining a confidence interval for a difference between two samples. Our starting point is our estimate for the difference in mean survival times between the two samples—30.63 days. We ask “How much might this estimate be in error? If we drew additional samples from the control universe and additional samples from the treatment universe, how much might they differ from this result?” We do not have the ability to go back to these universes and draw more samples, but from the samples themselves we can create hypothetical universes that embody all that we know about the treatment and control universes. We imagine replicating each element in each sample millions of times to create a hypothetical control universe and (separately) a hypothetical treatment universe. Then we can draw samples (separately) from these hypothetical universes to see how reliable is our original estimate of the difference in means (30.63 days). Actually, we use a shortcut —instead of copying each sample element a million times, we simply replace it after drawing it for our resample, thus creating a universe that is effectively infinite. Here are the steps: Step 1: Consider the two samples separately as the relevant universes. Step 2: Draw a sample of 7 with replacement from the treatment group and calculate the mean. Step 3: Draw a sample of 9 with replacement from the control group and calculate the mean. Step 4: Calculate the difference in means (treatment minus control) &amp; record. Step 5: Repeat steps 2-4 many times. Step 6: Review the distribution of resample means; the 5th and 95th percentiles are estimates of the endpoints of a 90 percent confidence interval. Here is a RESAMPLING STATS program (“mice-ci”): NUMBERS (94 38 23 197 99 16 141) treatmt treatment group NUMBERS (52 10 40 104 51 27 146 30 46) control control group REPEAT 1000 step 5 above SAMPLE 7 treatmt treatmt$ step 2 above SAMPLE 9 control control$ step 3 MEAN treatmt$ tmean step 4 MEAN control$ cmean step 4 SUBTRACT tmean cmean diff step 4 SCORE diff scrboard step 4 END step 5 HISTOGRAM scrboard PERCENTILE scrboard (5 95) interval step 6 PRINT interval Result: interval = -15.016 73.825 Interpretation: This means that one can be 90 percent confident that the mean of the difference (which is estimated to be 30.63) falls between -15.016 and 73.825. So the reliability of the estimate of the mean is very small. Example 21-7: Counted Data Example: Confidence Limit on a Proportion, Framingham Cholesterol Data The Framingham cholesterol data were used in Chapter 15, exercise 15-7, to illustrate the first classic question in statistical inference—interpretation of sample data for testing hypotheses. Now we use the same data for the other main theme in statistical inference—the estimation of confidence intervals. Indeed, the bootstrap method discussed above was originally devised for estimation of confidence intervals. The bootstrap method may also be used to calculate the appropriate sample size for experiments and surveys, another important topic in statistics. Consider for now just the data for the sub-group of 135 highcholesterol men in Table 15-4. Our second classic statistical question is as follows: How much confidence should we have that if we were to take a much larger sample than was actually obtained, the sample mean (that is, the proportion 10/135 = .07) would be in some close vicinity of the observed sample mean? Let us first carry out a resampling procedure to answer the questions, waiting until afterwards to discuss the logic of the inference. Construct a bucket containing 135 balls—10 red (infarction) and 125 green (no infarction) to simulate the universe as we guess it to be. Mix, choose a ball, record its color, replace it, and repeat 135 times (to simulate a sample of 135 men). Record the number of red balls among the 135 balls drawn. Repeat steps 2-4 perhaps 1000 times, and observe how much the total number of reds varies from sample to sample. We arbitrarily denote the boundary lines that include 47.5 percent of the hypothetical samples on each side of the sample mean as the 95 percent “confidence limits” around the mean of the actual population. Here is a RESAMPLING STATS program (“myocar3”): URN 10#1 125#0 men An bucket (called “men”) with ten “1s” (infarctions) and 125 “0s” (no infarction) REPEAT 1000 Do 1000 trials SAMPLE 135 men a Sample (with replacement) 135 numbers from the bucket, put them in a COUNT a =1 b Count the infarctions DIVIDE b 135 c Express as a proportion SCORE c z Keep score of the result END End the trial, go back and repeat HISTOGRAM z Produce a histogram of all trial results PERCENTILE z (2.5 97.5) k Determine the 2.5th and 97.5th percentiles of all trial results; these points enclose 95 percent of the results PRINT k Propor tion with infarction Result: k = 0.037037 0.11852 (This is the 95 percent confidence interval, enclosing 95 percent of the resample results) The variation in the histogram above highlights the fact that a sample containing only 10 cases of infarction is very small, and the number of observed cases—or the proportion of cases— necessarily varies greatly from sample to sample. Perhaps the most important implication of this statistical analysis, then, is that we badly need to collect additional data. Again, this is a classic problem in confidence intervals, found in all subject fields. The language used in the cholesterol-infarction example is exactly the same as the language used for the Bush-Dukakis poll above except for labels and numbers. As noted above, the philosophic logic of confidence intervals is quite deep and controversial, less obvious than for the hypothesis test. The key idea is that we can estimate for any given universe the probability P that a sample’s mean will fall within any given distance D of the universe’s mean; we then turn this around and assume that if we know the sample mean, the probability is P that the universe mean is within distance D of it. This inversion is more slippery than it may seem. But the logic is exactly the same for the formulaic method and for resampling. The only difference is how one estimates the probabilities—either with a numerical resampling simulation (as here), or with a formula or other deductive mathematical device (such as counting and partitioning all the possibilities, as Galileo did when he answered a gambler’s question about three dice). And when one uses the resampling method, the probabilistic calculations are the least demanding part of the work. One then has mental capacity available to focus on the crucial part of the job—framing the original question soundly, choosing a model for the facts so as to properly resemble the actual situation, and drawing appropriate inferences from the simulation. 24.4 Approach 2: Probability of various universes producing this sample A second approach to the general question of estimate accuracy is to analyze the behavior of a variety of universes centered at other points on the line, rather than the universe centered on the sample mean. One can ask the probability that a distribution centered away from the sample mean, with a given dispersion, would produce (say) a 10-apple scatter having a mean as far away from the given point as the observed sample mean. If we assume the situation to be symmetric, we can find a point at which we can say that a distribution centered there would have only a (say) 5 percent chance of producing the observed sample. And we can also say that a distribution even further away from the sample mean would have an even lower probability of producing the given sample. But we cannot turn the matter around and say that there is any particular chance that the distribution that actually produced the observed sample is between that point and the center of the sample. Imagine a situation where you are standing on one side of a canyon, and you are hit by a baseball, the only ball in the vicinity that day. Based on experiments, you can estimate that a baseball thrower who you see standing on the other side of the canyon has only a 5 percent chance of hitting you with a single throw. But this does not imply that the source of the ball that hit you was someone else standing in the middle of the canyon, because that is patently impossible. That is, your knowledge about the behavior of the “boundary” universe does not logically imply anything about the existence and behavior of any other universes. But just as in the discussion of testing hypotheses, if you know that one possibility is unlikely, it is reasonable that as a result you will draw conclusions about other possibilities in the context of your general knowledge and judgment. We can find the “boundary” distribution(s) we seek if we a) specify a measure of dispersion, and b) try every point along the line leading away from the sample mean, until we find that distribution that produces samples such as that observed with a (say) 5 percent probability or less. To estimate the dispersion, in many cases we can safely use an estimate based on the sample dispersion, using either resampling or Normal distribution theory. The hardest cases for resampling are a) a very small sample of data, and b) a proportion near 0 or near 1.0 (because the presence or absence in the sample of a small number of observations can change the estimate radically, and therefore a large sample is needed for reliability). In such situations one should use additional outside information, or Normal distribution theory, or both. We can also create a confidence interval in the following fashion: We can first estimate the dispersion for a universe in the general neighborhood of the sample mean, using various devices to be “conservative,” if we like 1 . Given the estimated dispersion, we then estimate the probability distribution of various amounts of error between observed sample means and the population mean. We can do this with resampling simulation as follows: a) Create other universes at various distances from the sample mean, but with other characteristics similar to the universe that we postulate for the immediate neighborhood of the sample, and b) experiment with those universes. One can also apply the same logic with a more conventional parametric approach, using general knowledge of the sampling distribution of the mean, based on Normal distribution theory or previous experience with resampling. We shall not discuss the latter method here. As with approach 1, we do not make any probability statements about where the population mean may be found. Rather, we discuss only what various hypothetical universes might produce , and make inferences about the “actual” population’s characteristics by comparison with those hypothesized universes. If we are interested in (say) a 95 percent confidence interval, we want to find the distribution on each side of the sample mean that would produce a sample with a mean that far away only 2.5 percent of the time (2 * .025 = 1-.95). A shortcut to find these “border distributions” is to plot the sampling distribu- 2 More about this later; it is, as I said earlier, not of primary importance in estimating the accuracy of the confidence intervals; note, please, that as we talk about the accuracy of statements about accuracy, we are moving down the ladder of sizes of causes of error. tion of the mean at the center of the sample, as in Approach 1. Then find the (say) 2.5 percent cutoffs at each end of that distribution. On the assumption of equal dispersion at the two points along the line, we now reproduce the previously-plotted distribution with its centroid (mean) at those 2.5 percent points on the line. The new distributions will have 2.5 percent of their areas on the other side of the mean of the sample. Example 21-8: Approach 2 for Counted Data: the Bush- Dukakis Poll Let’s implement Approach 2 for counted data, using for comparison the Bush-Dukakis poll data discussed earlier in the context of Approach 1. We seek to state, for universes that we select on the basis that their results will interest us, the probability that they (or it, for a particular universe) would produce a sample as far or farther away from the mean of the universe in question as the mean of the observed sample—56 percent for Bush. The most interesting universe is that which produces such a sample only about 5 percent of the time, simply because of the correspondence of this value to a conventional breakpoint in statistical inference. So we could experiment with various universes by trial and error to find this universe. We can learn from our previous simulations of the Bush– Dukakis poll in Approach 1 that about 95 percent of the samples fall within .025 on either side of the sample mean (which we had been implicitly assuming is the location of the population mean). If we assume (and there seems no reason not to) that the dispersions of the universes we experiment with are the same, we will find (by symmetry) that the universe we seek is centered on those points .025 away from .56, or .535 and .585. From the standpoint of Approach 2, then, the conventional sample formula that is centered at the mean can be considered a shortcut to estimating the boundary distributions. We say that the boundary is at the point that centers a distribution which has only a (say) 2.5 percent chance of producing the observed sample; it is that distribution which is the subject of the discussion, and not the distribution which is centered at mu = xbar. Results of these simulations are shown in Figure 21-1. Figure 21-1 About these distributions centered at .535 and .585—or more importantly for understanding an election situation, the universe centered at .535—one can say: Even if the “true” value is as low as 53.5 percent for Bush, there is only a 2 ½ percent chance that a sample as high as 56 percent pro-Bush would be observed. (The values of a 2 ½ percent probability and a 2 ½ percent difference between 56 percent and 53.5 percent coincide only by chance in this case.) It would be even more revealing in an election situation to make a similar statement about the universe located at 50-50, but this would bring us almost entirely within the intellectual ambit of hypothesis testing. To restate, then: Moving progressively farther away from the sample mean, we can eventually find a universe that has only some (any) specified small probability of producing a sample like the one observed. One can then say that this point represents a “limit” or “boundary” so that the interval between it and the sample mean may be called a confidence interval. Example 21-9: Approach 2 for Measured Data: The Diameters of Trees To implement Approach 2 for measured data, one may proceed exactly as with Approach 1 above except that the output of the simulation with the sample mean as midpoint will be used for guidance about where to locate trial universes for Approach 2. The results for the tree diameter data (Table 21-1) are shown in Figure 21-2. Figure 21-2 24.5 Interpretation of Approach 2 Now to interpret the results of the second approach: Assume that the sample is not drawn in a biased fashion (such as the wind blowing all the apples in the same direction), and that the population has the same dispersion as the sample. We can then say that distributions centered at the two endpoints of the 95 percent confidence interval (each of them including a tail in the direction of the observed sample mean with 2.5 percent of the area), or even further away from the sample mean, will produce the observed sample only 5 percent of the time or less . The result of the second approach is more in the spirit of a hypothesis test than of the usual interpretation of confidence intervals. Another statement of the result of the second approach is: We postulate a given universe—say, a universe at (say) the two-tailed 95 percent boundary line. We then say: The probability that the observed sample would be produced by a universe with a mean as far (or further) from the observed sample’s mean as the universe under investigation is only 2.5 percent. This is similar to the probvalue interpretation of a hypothesis-test framework. It is not a direct statement about the location of the mean of the universe from which the sample has been drawn. But it is certainly reasonable to derive a betting-odds interpretation of the statement just above, to wit: The chances are 2 ½ in 100 (or, the odds are 2 ½ to 97 ½ ) that a population located here would generate a sample with a mean as far away as the observed sample. And it would seem legitimate to proceed to the further betting-odds statement that (assuming we have no additional information) the odds are 97 ½ to 2 ½ that the mean of the universe that generated this sample is no farther away from the sample mean than the mean of the boundary universe under discussion. About this statement there is nothing slippery, and its meaning should not be controversial. Here again the tactic for interpreting the statistical procedure is to restate the facts of the behavior of the universe that we are manipulating and examining at that moment. We use a heuristic device to find a particular distribution—the one that is at (say) the 97 ½ –2 ½ percent boundary—and simply state explicitly what the distribution tells us implicitly: The probability of this distribution generating the observed sample (or a sample even further removed) is 2 ½ percent. We could go on to say (if it were of interest to us at the moment) that because the probability of this universe generating the observed sample is as low as it is, we “reject” the “hypothesis” that the sample came from a universe this far away or further. Or in other words, we could say that because we would be very surprised if the sample were to have come from this universe, we instead believe that another hypothesis is true. The “other” hypothesis often is that the universe that generated the sample has a mean located at the sample mean or closer to it than the boundary universe. The behavior of the universe at the 97 ½ –2 ½ percent boundary line can also be interpreted in terms of our “confidence” about the location of the mean of the universe that generated the observed sample. We can say: At this boundary point lies the end of the region within which we would bet 97 ½ to 2 ½ that the mean of the universe that generated this sample lies to the (say) right of it. As noted in the preview to this chapter, we do not learn about the reliability of sample estimates of the population mean (and other parameters) by logical inference from any one particular sample to any one particular universe, because in principle this cannot be done . Instead, in this second approach we investigate the behavior of various universes at the borderline of the neighborhood of the sample, those universes being chosen on the basis of their resemblances to the sample. We seek, for example, to find the universes that would produce samples with the mean of the observed sample less than (say) 5 percent of the time. In this way the estimation of confidence intervals is like all other statistical inference: One investigates the probabilistic behavior of hypothesized universes, the hypotheses being implicitly suggested by the sample evidence but not logically implied by that evidence. Approaches 1 and 2 may (if one chooses) be seen as identical conceptually as well as (in many cases) computationally (except for the asymmetric distributions mentioned earlier). But as I see it, the interpretation of them is rather different, and distinguishing them helps one’s intuitive understanding. 24.6 Exercises Solutions for problems may be found in the section titled, “Exercise Solutions” at the back of this book. Exercise 21-1 In a sample of 200 people, 7 percent are found to be unemployed. Determine a 95 percent confidence interval for the true population proportion. Exercise 21-2 A sample of 20 batteries is tested, and the average lifetime is 28.85 months. Establish a 95 percent confidence interval for the true average value. The sample values (lifetimes in months) are listed below. 30 32 31 28 31 29 29 24 30 31 28 28 32 31 24 23 31 27 27 31 Exercise 21-3 Suppose we have 10 measurements of Optical Density on a batch of HIV negative control: .02 .026 .023 .017 .022 .019 .018 .018 .017 .022 Derive a 95 percent confidence interval for the sample mean. Are there enough measurements to produce a satisfactory answer? References "],["and-some-last-words-about-the-reliability-of-sample-averages.html", "25 And Some Last Words About the Reliability of Sample Averages 25.1 The problem of uncertainty about the dispersion 25.2 Notes on the use of confidence intervals 25.3 Overall summary and conclusions about confidence intervals 25.4 Endnote", " 25 And Some Last Words About the Reliability of Sample Averages 25.1 The problem of uncertainty about the dispersion The inescapable difficulty of estimating the amount of dispersion in the population has greatly exercised statisticians over the years. Hence I must try to clarify the matter. Yet in practice this issue turns out not to be the likely source of much error even if one is somewhat wrong about the extent of dispersion, and therefore we should not let it be a stumbling block in the way of our producing estimates of the accuracy of samples in estimating population parameters. Student’s t test was designed to get around the problem of the lack of knowledge of the population dispersion. But Wallis and Roberts wrote about the t test: “\\[F\\]ar-reaching as have been the consequences of the t distribution for technical statistics, in elementary applications it does not differ enough from the normal distribution…to justify giving beginners this added complexity.” [wallis1957statistics], p. x) “Although Student’s t and the F ratio are explained…the student…is advised not ordinarily to use them himself but to use the shortcut methods… These, being non-parametric and involving simpler computations, are more nearly foolproof in the hands of the beginner—and, ordinarily, only a little less powerful.” (p. xi) If we knew the population parameter—the proportion, in the case we will discuss—we could easily determine how inaccurate the sample proportion is likely to be. If, for example, we wanted to know about the likely inaccuracy of the proportion of a sample of 100 voters drawn from a population of a million that is 60% Democratic, we could simply simulate drawing (say) 200 samples of 100 voters from such a universe, and examine the average inaccuracy of the 200 sample proportions. But in fact we do not know the characteristics of the actual universe. Rather, the nature of the actual universe is what we seek to learn about. Of course, if the amount of variation among samples were the same no matter what the Republican-Democrat proportions in the universe, the issue would still be simple, because we could then estimate the average inaccuracy of the sample proportion for any universe and then assume that it would hold for our universe. But it is reasonable to suppose that the amount of variation among samples will be different for different Democrat-Republican proportions in the universe. Let us first see why the amount of variation among samples drawn from a given universe is different with different relative proportions of the events in the universe. Consider a universe of 999,999 Democrats and one Republican. Most samples of 100 taken from this universe will contain 100 Democrats. A few (and only a very, very few) samples will contain 99 Democrats and one Republican. So the biggest possible difference between the sample proportion and the population proportion (99.9999%) is less than one percent (for the very few samples of 99% Democrats). And most of the time the difference will only be the tiny difference between a sample of 100 Democrats (sample proportion = 100%), and the population proportion of 99.9999%. Compare the above to the possible difference between a sample of 100 from a universe of half a million Republicans and half a million Democrats. At worst a sample could be off by as much as 50% (if it got zero Republicans or zero Democrats), and at best it is unlikely to get exactly 50 of each. So it will almost always be off by 1% or more. It seems, therefore, intuitively reasonable (and in fact it is true) that the likely difference between a sample proportion and the population proportion is greatest with a 50%-50% universe, least with a 0%-100% universe, and somewhere in between for probabilities, in the fashion of Figure 22-1. Error in average sample in % .5 1.0 Population Proportion Figure 22-1: Relationship Between the Population Propor tion and the Likely Error In a Sample Perhaps it will help to clarify the issue of estimating dispersion if we consider this: If we compare estimates for a second sample based on a) the population , versus b) the first sample , the former will be more accurate than the latter, because of the sampling variation in the first sample that affects the latter estimate. But we cannot estimate that sampling variation without knowing more about the population. 25.2 Notes on the use of confidence intervals Confidence intervals are used more frequently in the physical sciences—indeed, the concept was developed for use in astronomy—than in bio-statistics and in the social sciences; in these latter fields, measurement is less often the main problem and the distinction between hypotheses often is difficult. Some statisticians suggest that one can do hypothesis tests with the confidence-interval concept. But that seems to me equivalent to suggesting that one can get from New York to Chicago by flying first to Los Angeles. Additionally, the logic of hypothesis tests is much clearer than the logic of confidence intervals, and it corresponds to our intuitions so much more easily. Discussions of confidence intervals sometimes assert that one cannot make a probability statement about where the population mean may be, yet can make statements about the probability that a particular set of samples may bound that mean. If one takes the operational-definition point of view (see discussion of that concept in connection with the concept of probability), and we agree that our interest is upcoming events and probably decision-making, then we obviously are interested in putting betting odds on the location of the population mean (and subsequent samples). And a statement about process will not help us with that, but only a probability statement. Moving progressively farther away from the sample mean, we can find a universe that has only some (any) specified small probability of producing a sample like the one observed. One can say that this point represents a “limit” or “boundary” between which and the sample mean may be called a confidence interval, I suppose. This issue is discussed in more detail in Simon (forthcoming). 25.3 Overall summary and conclusions about confidence intervals The first task in statistics is to measure how much—to make a quantitative estimate of the universe from which a given sample has been drawn, including especially the average and the dispersion; the theory of point estimation is discussed in Chapter 13. The next task is to make inferences about the meaning of the estimates. A hypothesis test helps us decide whether two or more universes are the same or different from each other. In contrast, the confidence interval concept helps us decide on the reliability of an estimate. Confidence intervals and hypothesis tests are not entirely disjoint. In fact, hypothesis testing of a single sample against a benchmark value is, under all interpretations, I think, operationally identical with constructing a confidence interval and checking whether it includes that benchmark value. But the underlying reasoning is different because the questions which they are designed to answer are different. Having now worked through the entire procedure of producing a confidence interval, it should be glaringly obvious why statistics is such a difficult subject. The procedure is very long, and involves a very large number of logical steps. Such a long logical train is very hard to control intellectually, and very hard to follow with one’s intuition. The actual computation of the probabilities is the very least of it, almost a trivial exercise. 25.4 Endnote 1. They go on to say, “Techniques and details, beyond a comparatively small range of fairly basic methods, are likely to do more harm than good in the hands of beginners…The great ideas…are lost… \\[N\\]onparametric \\[methods\\] involving simpler computations are more nearly foolproof in the hands of the beginner.” (1956, viii, xi) Their stance is very much in contrast to that of Fisher, who wrote somewhere about the t test as a “revolution.” "],["correlation-and-causation.html", "26 Correlation and Causation 26.1 Preview 26.2 Introduction to correlation and causation 26.3 A Note on Association Compared to Testing a Hypothesis 26.4 Correlation: sum of products 26.5 Exercises 26.6 Endnotes", " 26 Correlation and Causation 26.1 Preview The correlation (speaking in a loose way for now) between two variables measures the strength of the relationship between them. A positive “linear” correlation between two variables x and y implies that high values of x are associated with high values of y, and that low values of x are associated with low values of y. A negative correlation implies the opposite; high values of x are associated with low values of y. By definition a “correlation coefficient” close to zero indicates little or no linear relationship between two variables; correlation coefficients close to 1 and -1 denote a strong positive or negative relationship. We will generally use a simpler measure of correlation than the correlation coefficient, however. One way to measure correlation with the resampling method is to rank both variables from highest to lowest, and investigate how often in randomly-generated samples the rankings of the two variables are as close to each other as the rankings in the observed variables. A better approach, because it uses more of the quantitative information contained in the data though it requires more computation, is to multiply the values for the corresponding pairs of values for the two variables, and compare the sum of the resulting products to the analogous sum for randomly-generated pairs of the observed variable values. The last section of the chapter shows how the strength of a relationship can be determined when the data are counted, rather than measured. First comes some discussion of the philosophical issues involved in correlation and causation. 26.2 Introduction to correlation and causation The questions in Examples 7-1 to 8-3 have been stated in the following form: Does the independent variable (say, irradiation; or type of pig ration) have an effect upon the dependent variable (say, sex of fruit flies; or weight gain of pigs)? This is another way to state the following question: Is there a causal relationship between the independent variable(s) and the dependent variable? (“Independent” or “control” is the name we give to the variable(s) the researcher believes is (are) responsible for changes in the other variable, which we call the “dependent” or “response” variable.) A causal relationship cannot be defined perfectly neatly. Even an experiment does not determine perfectly whether a relationship deserves to be called “causal” because, among other reasons, the independent variable may not be clear-cut. For example, even if cigarette smoking experimentally produces cancer in rats, it might be the paper and not the tobacco that causes the cancer. Or consider the fabled gentlemen who got experimentally drunk on bourbon and soda on Monday night, scotch and soda on Tuesday night, and brandy and soda on Wednesday night—and stayed sober Thursday night by drinking nothing. With a vast inductive leap of scientific imagination, they treated their experience as an empirical demonstration that soda, the common element each evening, was the cause of the inebriated state they had experienced. Notice that their deduction was perfectly sound, given only the recent evidence they had. Other knowledge of the world is necessary to set them straight. That is, even in a controlled experiment there is often no way except subject-matter knowledge to avoid erroneous conclusions about causality. Nothing except substantive knowledge or scientific intuition would have led them to the recognition that it is the alcohol rather than the soda that made them drunk, as long as they always took soda with their drinks . And no statistical procedure can suggest to them that they ought to experiment with the presence and absence of soda. If this is true for an experiment, it must also be true for an uncontrolled study. Here are some tests that a relationship usually must pass to be called causal. That is, a working definition of a particular causal relationship is expressed in a statement that has these important characteristics: It is an association that is strong enough so that the observer believes it to have a predictive (explanatory) power great enough to be scientifically useful or interesting. For example, he is not likely to say that wearing glasses causes (or is a cause of) auto accidents if the observed correlation is .07, even if the sample is large enough to make the correlation statistically significant. In other words, unimportant relationships are not likely to be labeled causal. Various observers may well differ in judging whether or not an association is strong enough to be important and therefore “causal.” And the particular field in which the observer works may affect this judgment. This is an indication that whether or not a relationship is dubbed “causal” involves a good deal of human judgment and is subject to dispute. The “side conditions” must be sufficiently few and sufficiently observable so that the relationship will apply under a wide enough range of conditions to be considered useful or interesting. In other words, the relationship must not require too many “if”s, “and”s, and “but”s in order to hold . For example, one might say that an increase in income caused an increase in the birth rate if this relationship were observed everywhere. But, if the relationship were found to hold only in developed countries, among the educated classes, and among the higher-income groups, then it would be less likely to be called “causal”—even if the correlation were extremely high once the specified conditions had been met. A similar example can be made of the relationship between income and happiness. For a relationship to be called “causal,” there should be sound reason to believe that, even if the control variable were not the “real” cause (and it never is), other relevant “hidden” and “real” cause variables must also change consistently with changes in the control variables. That is, a variable being manipulated may reasonably be called “causal” if the real variable for which it is believed to be a proxy must always be tied intimately to it. (Between two variables, v and w, v may be said to be the “more real” cause and w a “spurious” cause, if v and w require the same side conditions, except that v does not require w as a side condition.) This third criterion (non-spuriousness) is of particular importance to policy makers. The difference between it and the previous criterion for side conditions is that a plenitude of very restrictive side conditions may take the relationship out of the class of causal relationships, even though the effects of the side conditions are known . This criterion of nonspuriousness concerns variables that are as yet unknown and unevaluated but that have a possible ability to upset the observed association. Examples of spurious relationships and hidden-third-factor causation are commonplace. For a single example, toy sales rise in December. There is no danger in saying that December causes an increase in toy sales, even though it is “really” Christmas that causes the increase, because Christmas and December practically always accompany each other. Belief that the relationship is not spurious is increased if many likely variables have been investigated and none removes the relationship. This is further demonstration that the test of whether or not an association should be called “causal” cannot be a logical one; there is no way that one can express in symbolic logic the fact that many other variables have been tried without changing the relationship in question. The more tightly a relationship is bound into (that is, deduced from, compatible with, and logically connected to) a general framework of theory, the stronger is its claim to be called “causal.” For an economics example, observed positive relationships between the interest rate and business investment and between profits and investment are more likely to be called “causal” than is the relationship between liquid assets and investment. This is so because the first two statements can be deduced from classical price theory, whereas the third statement cannot. Connection to a theoretical framework provides support for belief that the side conditions necessary for the statement to hold true are not restrictive and that the likelihood of spurious correlation is not great; because a statement is logically connected to the rest of the system, the statement tends to stand or fall as the rest of the system stands or falls. And, because the rest of the system of economic theory has, over a long period of time and in a wide variety of tests, been shown to have predictive power, a statement connected with it is cloaked in this mantle. The social sciences other than economics do not have such well-developed bodies of deductive theory, and therefore this criterion of causality does not weigh as heavily in sociology, for instance, as in economics. Rather, the other social sciences seem to substitute a weaker and more general criterion, that is, whether or not the statement of the relationship is accompanied by other statements that seem to “explain” the “mechanism” by which the relationship operates. Consider, for example, the relationship between the phases of the moon and the suicide rate. The reason that sociologists do not call it causal is that there are no auxiliary propositions that explain the relationship and describe an operative mechanism. On the other hand, the relationship between broken homes and juvenile delinquency is often referred to as “causal,” in large part because a large body of psychoanalytic theory serves to explain why a child raised without one or the other parent, or in the presence of parental strife, should not adjust readily. Furthermore, one can never decide with perfect certainty whether in any given situation one variable “causes” a particular change in another variable. At best, given your particular purposes in investigating a phenomena, you may be safe in judging that very likely there is causal influence. In brief, it is correct to say (as it is so often said) that correlation does not prove causation—if we add the word “completely” to make it “correlation does not completely prove causation.” On the other hand, causation can never be “proven” completely by correlation or any other tool or set of tools, including experimentation. The best we can do is make informed judgments about whether to call a relationship causal. It is clear, however, that in any situation where we are interested in the possibility of causation, we must at least know whether there is a relationship (correlation) between the variables of interest; the existence of a relationship is necessary for a relationship to be judged causal even if it is not sufficient to receive the causal label. And in other situations where we are not even interested in causality, but rather simply want to predict events or understand the structure of a system, we may be interested in the existence of relationships quite apart from questions about causations. Therefore our next set of problems deals with the probability of there being a relationship between two measured variables, variables that can take on any values (say, the values on a test of athletic scores) rather than just two values (say, whether or not there has been irradiation.) Another way to think about such problems is to ask whether two variables are independent of each other—that is, whether you know anything about the value of one variable if you know the value of the other in a particular case—or whether they are not independent but rather are related. 26.3 A Note on Association Compared to Testing a Hypothesis Problems in which we investigate a) whether there is an association , versus b) whether there is a difference between just two groups, often look very similar, especially when the data constitute a 2-by-2 table. There is this important difference between the two types of analysis, however: Questions about association refer to variables —say weight and age—and it never makes sense to ask whether there is a difference between variables (except when asking whether they measure the same quantity). Questions about similarity or difference refer to groups of individuals , and in such a situation it does make sense to ask whether or not two groups are observably different from each other. Example 23-1: Is Athletic Ability Directly Related to Intelligence? (Is There Correlation Between Two Variables or Are They Independent?) (Program “Ability1”) A scientist often wants to know whether or not two characteristics go together, that is, whether or not they are correlated (that is, related or associated). For example, do youths with high athletic ability tend to also have high I.Q.s? Hypothetical physical-education scores of a group of ten high-school boys are shown in Table 23-1, ordered from high to low, along with the I.Q. score for each boy. The ranks for each student’s athletic and I.Q. scores are then shown in columns 3 and 4. Table 23-1 Hypothetical Athletic and I.Q. Scores for High School Boys Athletic Score I.Q. Score Athletic Rank I.Q.Rank (1) (2) (3) (4) 97 114 1 3 94 120 2 1 93 107 3 7 90 113 4 4 87 118 5 2 86 101 6 8 86 109 7 6 85 110 8 5 81 100 9 9 76 99 10 10 We want to know whether a high score on athletic ability tends to be found along with a high I.Q. score more often than would be expected by chance. Therefore, our strategy is to see how often high scores on both variables are found by chance. We do this by disassociating the two variables and making two separate and independent universes, one composed of the athletic scores and another of the I.Q. scores. Then we draw pairs of observations from the two universes at random, and compare the experimental patterns that occur by chance to what actually is observed to occur in the world. The first testing scheme we shall use is similar to our first approach to the pig rations—splitting the results into just “highs” and “lows.” We take ten cards, one of each denomination from “ace” to “10,” shuffle, and deal five cards to correspond to the first five athletic ranks. The face values then correspond to the I.Q. ranks. Under the benchmark hypothesis the athletic ranks will not be associated with the I.Q. ranks. Add the face values in the first five cards in each trial; the first hand includes 2, 4, 5, 6, and 9, so the sum is 26. Record, shuffle, and repeat perhaps ten times. Then compare the random results to the sum of the observed ranks of the five top athletes, which equals 17. The following steps describe a slightly different procedure than that just described, because this one may be easier to understand: Step 1. Convert the athletic and I.Q. scores to ranks. Then constitute a universe of spades, “ace” to “10,” to correspond to the athletic ranks, and a universe of hearts, “ace” to “10,” to correspond to the IQ ranks. Step 2. Deal out the well-shuffled cards into pairs, each pair with an athletic score and an I.Q. score. Step 3. Locate the cards with the top five athletic ranks, and add the I.Q. rank scores on their paired cards. Compare this sum to the observed sum of 17. If 17 or less, indicate “yes,” otherwise “no.” (Why do we use “17 or less” rather than “less than 17”? Because we are asking the probability of a score this low or lower .) Step 4. Repeat steps 2 and 3 ten times. Step 5. Calculate the proportion “yes.” This estimates the probability sought. In Table 23-2 we see that the observed sum (17) is lower than the sum of the top 5 ranks in all but one (shown by an asterisk) of the ten random trials (trial 5), which suggests that there is a good chance (9 in 10) that the five best athletes will not have I.Q. scores that high by chance. But it might be well to deal some more to get a more reliable average. We add thirty hands, and thirty-nine of the total forty hands exceed the observed rank value, so the probability that the observed correlation of athletic and I.Q. scores would occur by chance is about .025. In other words, if there is no real association between the variables, the probability that the top 5 ranks would sum to a number this low or lower is only 1 in 40, and it therefore seems reasonable to believe that high athletic ability tends to accompany a high I.Q. Table 23-2 Results of 40 Random Trials of The Problem “Ability” (Note: Observed sum of IQ ranks: 17) Trial Sum of IQ Ranks Yes or No 1 26 No 2 23 No 3 22 No 4 37 No * 5 16 Yes 6 22 No 7 22 No 8 28 No 9 38 No 10 22 No 11 35 No 12 36 No 13 31 No 14 29 No 15 32 No 16 25 No 17 25 No 18 29 No 19 25 No 20 22 No 21 30 No 22 31 No 23 35 No 24 25 No 25 33 No 26 30 No 27 24 No 28 29 No 29 30 No 30 31 No 31 30 No 32 21 No 33 25 No 34 19 No 35 29 No 36 23 No 37 23 No 38 34 No 39 23 No 40 26 No The RESAMPLING STATS program “Ability1” creates an array containing the I.Q. rankings of the top 5 students in athletics. The SUM of these I.Q. rankings constitutes the observed result to be tested against randomly-drawn samples. We observe that the actual I.Q. rankings of the top five athletes sums to 17. The more frequently that the sum of 5 randomly-generated rankings (out of 10) is as low as this observed number, the higher is the probability that there is no relationship between athletic performance and I.Q. based on these data. First we record the NUMBERS “1” through “10” into vector A. Then we SHUFFLE the numbers so the rankings are in a random order. Then TAKE the first 5 of these numbers and put them in another array, D, and SUM them, putting the result in E. We repeat this procedure 1000 times, recording each result in a scorekeeping vector: Z. Graphing Z, we get a HIS- TOGRAM that shows us how often our randomly assigned sums are equal to or below 17. REPEAT 1000 Repeat the experiment 1000 times. NUMBERS 1,10 a Constitute the set of I.Q. ranks. SHUFFLE a b Shuffle them. TAKE b 1,5 d Take the first 5 ranks. SUM d e Sum those ranks. SCORE e z Keep track of the result of each trial. END End the experiment, go back and repeat. HISTOGRAM z Produce a histogram of trial results. ABILITY1: Random Selection of 5 Out of 10 Ranks Sum of top 5 ranks We see that in only about 2% of the trials did random selection of ranks produce a total of 17 or lower. RESAMPLING STATS will calculate this for us directly: COUNT z &lt;= 17 k Determine how many trials produced sums of ranks &lt;= 17 by chance. DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the results. Note: The file “ability1” on the Resampling Stats software disk contains this set of commands. Why do we sum the ranks of the first five athletes and compare them with the second five athletes, rather than comparing the top three, say, with the bottom seven? Indeed, we could have looked at the top three, two, four, or even six or seven. The first reason for splitting the group in half is that an even split uses the available information more fully, and therefore we obtain greater efficiency. (I cannot prove this formally here, but perhaps it makes intuitive sense to you.) A second reason is that getting into the habit of always looking at an even split reduces the chances that you will pick and choose in such a manner as to fool yourself. For example, if the I.Q. ranks of the top five athletes were 3, 2, 1, 10, and 9, we would be deceiving ourselves if, after looking the data over, we drew the line between athletes 3 and 4. (More generally, choosing an appropriate measure before examining the data will help you avoid fooling yourself in such matters.) A simpler but less efficient approach to this same problem is to classify the top-half athletes by whether or not they were also in the top half of the I.Q. scores. Of the first five athletes actually observed, four were in the top five I.Q. scores. We can then shuffle five black and five red cards and see how often four or more (that is, four or five) blacks come up with the first five cards. The proportion of times that four or more blacks occurs in the trial is the probability that an association as strong as that observed might occur by chance even if there is no association. Table 23-3 shows a proportion of five trials out of twenty. In the RESAMPLING STATS program “Ability2” we first note that the top 5 athletes had 4 of the top 5 I.Q. scores. So we constitute the set of 10 IQ rankings (vector A). We then SHUFFLE A and TAKE 5 I.Q. rankings (out of 10). We COUNT how many are in the top 5, and keep SCORE of the result. After REPEATing 1000 times, we find out how often we select 4 of the top 5. Table 23-3 Results of 20 Random Trials of the Problem “ABILITY2” Observed Score: 4 Trial Score Yes or No 1 4 Yes 2 2 No 3 2 No 4 2 No 5 3 No 6 2 No 7 4 Yes 8 3 No 9 3 No 10 4 Yes 11 3 No 12 1 No 13 3 No 14 3 No 15 4 Yes 16 3 No 17 2 No 18 2 No 19 2 No 20 4 Yes REPEAT 1000 Do 1000 experiments. NUMBERS 1,10 a Constitute the set of I.Q. ranks. SHUFFLE a b Shuffle them. TAKE b 1,5 c Take the first 5 ranks. COUNT c between 1 5 d Of those 5, count how many are among the top half of the ranks (1-5). SCORE d z Keep track of that result in z END End one experiment, go back and repeat until all 1000 are complete. COUNT z &gt;= 4 k Determine how many trials produced 4 or more top ranks by chance. DIVIDE k 1000 kk Convert to a proportion. PRINT kk Print the result. Note: The file “ability2” on the Resampling Stats software disk contains this set of commands. So far we have proceeded on the theory that if there is any relationship between athletics and I.Q., then the better athletes have higher rather than lower I.Q. scores. The justification for this assumption is that past research suggests that it is probably true. But if we had not had the benefit of that past research, we would then have had to proceed somewhat differently; we would have had to consider the possibility that the top five athletes could have I.Q. scores either higher or lower than those of the other students. The results of the “two-tail” test would have yielded odds weaker than those we observed. Example 23-2: Athletic Ability and I.Q. a Third Way. (Program “Ability3”). Example 23-1 investigated the relationship between I.Q. and athletic score by ranking the two sets of scores. But ranking of scores loses some efficiency because it uses only an “ordinal” (rank-ordered) rather than a “cardinal” (measured) scale; the numerical shadings and relative relationships are lost when we convert to ranks. Therefore let us consider a test of correlation that uses the original cardinal numerical scores. First a little background: Figures 23-1 and 23-2 show two hypothetical cases of very high association among the I.Q. and athletic scores used in previous examples. Figure 23-1 indicates that the higher the I.Q. score, the higher the athletic score. With a boy’s athletic score you can thus predict quite well his I.Q. score by means of a hand-drawn line—or vice versa. The same is true of Figure 23-2, but in the opposite direction. Notice that even though athletic score is on the x-axis (horizontal) and I.Q. score is on the y-axis (vertical), the athletic score does not cause the I.Q. score. (It is an unfortunate deficiency of such diagrams that some variable must arbitrarily be placed on the x-axis, whether you intend to suggest causation or not.) 120 x x I.Q. Score x x 110 x x x 100 x 75 85 95 Athletic Score Figure 23-1: Hypothetical Scores for I.Q. and Athletic Ability x 120 x x x I.Q. Score x 110 x x x x x 100 x x 75 85 95 Athletic Score Figure 23-2: Hypothetical Scores for I.Q. and Athletic Ability In Figure 23-3, which plots the scores as given in table 23-1 the prediction of athletic score given I.Q. score, or vice versa, is less clear-cut than in Figure 23-2. On the basis of Figure 23-3 alone, one can say only that there might be some association between the two variables. 120 x x 115 I.Q. Score x x 110 x x x 105 x 100 x x 80 85 90 95 100 Athletic Score Figure 23-3: Hypothetical Scores for I.Q. and Athletic Ability 26.4 Correlation: sum of products Now let us take advantage of a handy property of numbers. The more closely two sets of numbers match each other in order, the higher the sums of their products. Consider the following arrays of the numbers 1, 2, and 3: 1 x 1 = 1 2 x 2 = 4 (columns in matching order) 3 x 3 = 9 SUM = 14 1 x 2 = 2 2 x 3 = 6 (columns not in matching order) 3 x 1 = 3 SUM = 11 I will not attempt a mathematical proof, but the reader is encouraged to try additional combinations to be sure that the highest sum is obtained when the order of the two columns is the same. Likewise, the lowest sum is obtained when the two columns are in perfectly opposite order: 1 x 3 = 3 2 x 2 = 4 (columns in opposite order) 3 x 1 = 3 SUM = 10 Consider the cases in Table 23-4 which are chosen to illustrate a perfect (linear) association between x (Column 1) and y 1 (Column 2), and also between x (Column 1) and y 2 (Column 4); the numbers shown in Columns 3 and 5 are those that would be consistent with perfect associations. Notice the sum of the multiples of the x and y values in the two cases. It is either higher ( xy 1) or lower ( xy 2) than for any other possible way of arranging the y ’s. Any other arrangement of the y’s ( y 3, in Column 6, for example, chosen at random), when multiplied by the x ’s in Column 1, ( xy 3), produces a sum that falls somewhere between the sums of xy 1 and xy 2, as is the case with any other set of y 3’s which is not perfectly correlated with the x ’s. Table 23-5, below, shows that the sum of the products of the observed I.Q. scores multiplied by athletic scores (column 7) is between the sums that would occur if the I.Q. scores were ranked from best to worst (column 3) and worst to best (column 5). The extent of correlation (association) can thus be measured by whether the sum of the multiples of the observed x and y values is relatively much higher or much lower than are sums of randomly-chosen pairs of x and y . Table 23-4 Comparison of Sums of Multiplications Strong Positive Relationship Strong Negative Relationship Random Pairings X Y1 X*Y1 Y2 X*Y2 Y3 X*Y3 2 2 4 10 20 4 8 4 4 16 8 32 8 32 6 6 36 6 36 6 36 8 8 64 4 48 2 16 10 10 100 2 20 10 100 SUMS: 220 156 192 Table 23-5 Sums of Products: IQ and Athletic Scores 1 2 3 4 5 6 7 Athletic Hypothetical Col. 1 x Hypothetical Col. 1 x Actual Col. 1 x Score I.Q. Col.2 I.Q. Col. 4 I.Q. Col.6 97 120 11640 99 9603 114 11058 94 118 11092 100 9400 120 11280 93 114 10602 101 9393 107 9951 90 113 10170 107 9630 113 10170 87 110 9570 109 9483 118 10266 86 109 9374 110 8460 101 8686 86 107 9202 113 9718 109 9374 85 101 8585 114 9690 110 9350 81 100 8100 118 9558 100 8100 76 99 7524 120 9120 99 7524 SUMS: 95859 95055 95759 3 Cases: Perfect positive correlation (hypothetical); column 3 Perfect negative correlation (hypothetical); column 5 Observed; column 7 Now we attack the I.Q. and athletic-score problem using the property of numbers just discussed. First multiply the x and y values of the actual observations, and sum them to be 95,759 (Table 23-5). Then write the ten observed I.Q. scores on cards, and assign the cards in random order to the ten athletes, as shown in column 1 in Table 23-6. Multiply by the x’s, and sum as in Table 23-7. If the I.Q. scores and athletic scores are positively associated , that is, if high I.Q.s and high athletic scores go together, then the sum of the multiplications for the observed sample will be higher than for most of the random trials. (If high I.Q.s go with low athletic scores, the sum of the multiplications for the observed sample will be lower than most of the random trials.) Table 23-6 Random Drawing of I.Q. Scores and Pairing (Randomly) Against Athletic Scores (20 Trials) Trial Number Athletic 1 2 3 4 5 6 7 8 9 10 Score 97 114 109 110 118 107 114 107 120 100 114 94 101 113 113 101 118 100 110 109 120 107 93 107 118 100 99 120 101 114 99 110 113 90 113 101 118 114 101 113 100 118 99 99 87 120 100 101 100 110 107 113 114 101 118 86 100 110 120 107 113 110 118 101 118 101 86 110 107 99 109 100 120 120 113 114 120 85 99 99 104 120 99 109 101 107 109 109 81 118 120 114 110 114 99 99 100 107 109 76 109 114 109 113 109 118 109 110 113 110 Trial Number Athletic Score 11 12 13 14 15 16 17 18 19 20 97 109 118 101 109 107 100 99 113 99 110 94 101 110 114 118 101 107 114 101 109 113 93 120 120 100 120 114 113 100 100 120 100 90 110 118 109 110 99 109 107 109 110 99 87 100 100 120 99 118 114 110 110 107 101 86 118 99 107 100 109 118 113 118 100 118 86 99 101 99 101 100 99 101 107 114 120 85 107 114 110 114 120 110 120 120 118 100 81 114 107 113 113 110 101 109 114 101 100 76 113 109 118 107 113 120 118 99 118 107 Table 23-7 Results of Sum Products for Above 20 Random Trials Trial Sum of Multiplications Trial Sum of Multiplications 1 95,430 11 95,406 2 95,426 12 95,622 3 95,446 13 95,250 4 95,381 14 95,599 5 95,542 15 95,323 6 95,362 16 95,308 7 95,508 17 95,220 8 95,590 18 95,443 9 95,379 19 95,421 10 95,532 20 95,528 More specifically, by the steps: Step 1. Write the ten I.Q. scores on one set of cards, and the ten athletic scores on another set of cards. Step 2. Pair the I.Q. and athletic-score cards at random. Multiply the scores in each pair, and add the results of the ten multiplications. Step 3. Subtract the experimental sum in step 2 from the observed sum, 95,759. Step 4. Repeat steps 2 and 3 twenty times. Step 5. Compute the proportion of trials where the difference is negative, which estimates the probability that an association as strong as the observed would occur by chance. The sums of the multiplications for 20 trials are shown in Table 23-7. No random-trial sum was as high as the observed sum, which suggests that the probability of an association this strong happening by chance is so low as to approach zero. (An empirically-observed probability is never actually zero.) This program can be solved particularly easily with RESAMPLING STATS. The arrays A and B in program “Ability3” list the athletic scores and the I.Q. scores respectively of 10 “actual” students ordered from highest to lowest athletic score. We MULTIPLY the corresponding elements of these arrays and proceed to compare the sum of these multiplications to the sums of experimental multiplications in which the elements are selected randomly. Finally, we COUNT the trials in which the sum of the products of the randomly-paired athletic and I.Q. scores equals or exceeds the sum of the products in the observed data. NUMBERS (97 94 93 90 87 86 86 85 81 76) a Record athletic scores, highest to lowest. NUMBERS (114 120 107 113 118 101 109 110 100 99) b Record corresponding IQ scores for those students. MULTIPLY a b c Multiply the two sets of scores together. SUM c d Sum the results—the “observed value.” REPEAT 1000 Do 1000 experiments. SHUFFLE a e Shuffle the athletic scores so we can pair them against IQ scores. MULTIPLY e b f Multiply the shuffled athletic scores by the I.Q. scores. (Note that we could shuffle the I.Q. scores too but it would not achieve any greater randomization.) SUM f j Sum the randomized multiplications. SUBTRACT d j k Subtract the sum from the sum of the “observed” multiplication. SCORE k z Keep track of the result in z. END End one trial, go back and repeat until 1000 trials are complete. HISTOGRAM z Obtain a histogram of the trial results. Random Sums of Products ATHLETES &amp; IQ SCORES observed sum less random sum We see that obtaining a chance trial result as great as that observed was rare. RESAMPLING STATS will calculate this proportion for us: COUNT z &lt;= 0 k Determine in how many trials the random sum of products was less than the observed sum of products. DIVIDE k 1000 kk Convert to a proportion. PRINT kk Note: The file “ability3” on the Resampling Stats software disk contains this set of commands. Example 23-3: Correlation Between Adherence to Medication Regime and Change in Cholesterol Efron and Tibshirani (1993, p.72) show data on the extents to which 164 men a) took the drug prescribed to them (cholostyramine), and b) showed a decrease in total plasma cholesterol. Table 23-8 shows these values (note that a positive value in the “decrease in cholesterol” column denotes a decrease in cholesterol, while a negative value denotes an increase.) Table 23-8 Taken Taken Taken Taken 0 -5.25 27 -1.50 71 59.50 95 32.50 0 -7.25 28 23.50 71 14.75 95 70.75 0 -6.25 29 33.00 72 63.00 95 18.25 0 11.50 31 4.25 72 0.00 95 76.00 2 21.00 32 18.75 73 42.00 95 75.75 2 -23.00 32 8.50 74 41.25 95 78.75 2 5.75 33 3.25 75 36.25 95 54.75 3 3.25 33 27.75 76 66.50 95 77.00 3 8.75 34 30.75 77 61.75 96 68.00 4 8.75 34 -1.50 77 14.00 96 73.00 4 -10.25 34 1.00 78 36.00 96 28.75 7 -10.50 34 7.75 78 39.50 96 26.75 8 19.75 35 -15.75 81 1.00 96 56.00 8 -0.50 36 33.50 82 53.50 96 47.50 8 29.25 36 36.25 84 46.50 96 30.25 8 36.25 37 5.50 85 51.00 96 21.00 9 10.75 38 25.50 85 39.00 97 79.00 9 19.50 41 20.25 87 -0.25 97 69.00 9 17.25 43 33.25 87 1.00 97 80.00 10 3.50 45 56.75 87 46.75 97 86.00 10 11.25 45 4.25 87 11.50 98 54.75 11 -13.00 47 32.50 87 2.75 98 26.75 12 24.00 50 54.50 88 48.75 98 80.00 13 2.50 50 -4.25 89 56.75 98 42.25 15 3.00 51 42.75 90 29.25 98 6.00 15 5.50 51 62.75 90 72.50 98 104.75 16 21.25 52 64.25 91 41.75 98 94.25 16 29.75 53 30.25 92 48.50 98 41.25 17 7.50 54 14.75 92 61.25 98 40.25 18 -16.50 54 47.25 92 29.50 99 51.50 20 4.50 56 18.00 92 59.75 99 82.75 20 39.00 57 13.75 93 71.00 99 85.00 21 -5.75 57 48.75 93 37.75 99 70.00 21 -21.00 58 43.00 93 41.00 100 92.00 21 0.25 60 27.75 93 9.75 100 73.75 22 -10.25 62 44.50 93 53.75 100 54.00 24 -0.50 64 22.50 94 62.50 100 69.50 25 -19.00 64 -14.50 94 39.00 100 101.50 25 15.75 64 -20.75 94 3.25 100 68.00 26 6.00 67 46.25 94 60.00 100 44.75 27 10.50 68 39.50 95 113.25 100 86.75 % Prescribed Dosage Decrease in Cholesterol % Prescribed Dosage Decrease in Cholesterol % Prescribed Dosage Decrease in Cholesterol % Prescribed Dosage Decrease in Cholesterol The aim is to assess the effect of the compliance on the improvement. There are two related issues: What form of regression should be fitted to these data, which we address later, and Is there reason to believe that the relationship is meaningful? That is, we wish to ascertain if there is any meaningful correlation between the variables—because if there is no relationship between the variables, there is no basis for regressing one on the other. Sometimes people jump ahead in the latter question to first run the regression and then ask whether the regression slope coefficient(s) is (are) different than zero, but this usually is not sound practice. The sensible way to proceed is first to graph the data to see whether there is visible indication of a relationship. Efron and Tibshirani do this, and they find sufficient intuitive basis in the graph to continue the analysis. The next step is to investigate whether a measure of relationship is statistically significant; this we do as follows (program “inp10”): Multiply the observed values for each of the 164 participants on the independent x variable (cholostyramine—percent of prescribed dosage actually taken) and the dependent y variable (cholesterol), and sum the results—it’s 439,140. Randomly shuffle the dependent variable y values among the participants. The sampling is being done without replacement, though an equally good argument could be made for sampling with replacement; the results do not differ meaningfully, however, because the sample size is so large. Then multiply these x and y hypothetical values for each of the 164 participants, sum the results and record. Repeat steps 2 and 3 perhaps 1000 times. Determine how often the shuffled sum-of-products exceeds the observed value (439,140). The following program in RESAMPLING STATS provides the solution: READ FILE “inp10” x y Data MULTIPLY x y xy Step 1 above SUM xy xysum Note: xysum = 439,140 (4.3914e+05) REPEAT 1000 Do 1000 simulations (step 4 above) SHUFFLE x xrandom Step 2 above MULTIPLY xrandom y xy Step 3 above SUM xy newsum Step 3 above SCORE newsum scrboard Step 3 above END Step 4 above COUNT scorboard &gt;=439140 prob Step 5 above PRINT xysum prob Result: prob = 0. Interpretation: 1000 simulated random shufflings never produced a sum-of-products as high as the observed value. Hence we rule out random chance as an explanation for the observed correlation. Example 23-3: Is There A Relationship Between Drinking Beer And Being In Favor of Selling Beer? (Testing for a Relationship Between Counted-Data Variables.) (Program “Beerpoll”) The data for athletic ability and I.Q. were measured. Therefore, we could use them in their original “cardinal” form, or we could split them up into “high” and “low” groups. Often, however, the individual observations are recorded only as “yes” or “no,” which makes it more difficult to ascertain the existence of a relationship. Consider the poll responses in Table 23-8 to two public-opinion survey questions: “Do you drink beer?” and “Are you in favor of local option on the sale of beer?” \\[2\\] Table 23-9 Results of Observed Sample For Problem “Beerpoll” Do you favor local option on the sale of beer? Do you drink beer? Yes No Total Favor 45 20 65 Don’t Favor 7 6 13 Total 52 26 78 Here is the statistical question: Is a person’s opinion on “local option” related to whether or not he drinks beer? Our resampling solution begins by noting that there are seventy-eight respondents, sixty-five of whom approve local option and thirteen of whom do not. Therefore write “approve” on sixty-five index cards and “not approve” on thirteen index cards. Now take another set of seventy-eight index cards, preferably of a different color, and write “yes” on fifty-two of them and “no” on twenty-six of them, corresponding to the numbers of people who do and do not drink beer in the sample. Now lay them down in random pairs , one from each pile. If there is a high association between the variables, then real life observations will bunch up in the two diagonal cells in the upper left and lower right in Table 23-8. (Ignore the “total” data for now.) Therefore, subtract one sum of two diagonal cells from the other sum for the observed data: (45 + 6) - (20 + 7) = 24. Then compare this difference to the comparable differences found in random trials. The proportion of times that the simulated-trial difference exceeds the observed difference is the probability that the observed difference of +24 might occur by chance, even if there is no relationship between the two variables. (Notice that, in this case, we are working on the assumption that beer drinking is positively associated with approval of local option and not the inverse. We are interested only in differences that are equal to or exceed +24 when the northeast-southwest diagonal is subtracted from the northwest-southeast diagonal.) We can carry out a resampling test with this procedure: Step 1. Write “approve” on 65 and “disapprove” on 13 red index cards, respectively; write “Drink” and “Don’t drink” on 52 and 26 white cards, respectively. Step 2. Pair the two sets of cards randomly. Count the numbers of the four possible pairs: (1) “approve-drink,” (2) “disapprove-don’t drink,” (3) “disapprove-drink,” and (4) “approve-don’t drink.” Record the number of these combinations, as in Table 23-10, where columns 1-4 correspond to the four cells in Table 23-9. Step 3. Add (column 1 plus column 4) and (column 2 plus column 3), and subtract the result in the second parenthesis from the result in the first parenthesis. If the difference is equal to or greater than 24, record “yes,” otherwise “no.” Step 4. Repeat steps 2 and 3 perhaps a hundred times. Step 5. Calculate the proportion “yes,” which estimates the probability that an association this great or greater would be observed by chance. Table 23-10 Results of One Random Trial of the Problem “Beerpoll” (1) (2) (3) (4) (5) Trial Approve Yes Approve No Disappr ove Yes Disappr ove No (Col 1 + Col 4) - (Col 2 + Col 3) 1 43 22 9 4 47-31=16 A series of ten trials in this case (see Table 23-9) indicates that the observed difference is very often exceeded, which suggests that there is no relationship between beer drinking and opinion. The RESAMPLING STATS program “Beerpoll” does this repetitively. From the “actual” sample results we know that 52 respondents drink beer and 26 do not. We create the vector “drink” with 52 “1”s for those who drink beer, and 26 “2”s for those who do not. We also create the vector “sale” with 65 “1”s (approve) and 13 “2”s (disapprove). In the actual sample, 51 of the 78 respondents had “consistent” responses to the two questions—that is, people who both favor the sale of beer and drink beer, or who are against the sale of beer and do not drink beer. We want to randomly pair the responses to the two questions to compare against that observed result to test the relationship. To accomplish this aim, we REPEAT the following procedure 1000 times. We SHUFFLE drink to drink$ so that the responses are randomly ordered. Now when we SUBTRACT the corresponding elements of the two arrays, a “0” will appear in each element of the new array c for which there was consistency in the response of the two questions. We therefore COUNT the times that c equals “0” and place this result in d, and the number of times c does not equal 0, and place this result in e. Find the difference (d minus e), and SCORE this to z. SCORE Z stores for each trial the number of consistent responses minus inconsistent responses. To determine whether the results of the actual sample indicate a relationship between the responses to the two questions, we check how often the random trials had a difference (between consistent and inconsistent responses) as great as 24, the value in the observed sample. URN 52#1 26#0 drink Constitute the set of 52 beer drinkers, represented by 52 “1”s, and the set of 26 non-drinkers, represented by “2”s. URN 57#1 21#0 sale The same set of individuals classified by whether they favor (“1”) or don’t favor (“0”) the sale of beer. Note: F is now the vector {1 1 1 1 1 1 ... 0 0 0 0 0 ...} where 1 = people in favor, 0 = people opposed. REPEAT 1000 Repeat the experiment 1000 times. SHUFFLE drink drink$ Shuffle the beer drinkers/non-drinker, call the shuffled set drink*. Note: drink$ is now a vector like {1 1 1 0 1 0 0 1 0 1 1 0 0 ...} where 1 = drinker, 0 = non-drinker. END SUBTRACT drink$ sale c Subtract the favor/don’t favor set from the drink/don’t drink set. Consistent responses are someone who drinks favoring the sale of beer (a “1” and a “1”) or someone who doesn’t drink opposing the sale of beer. When subtracted, consistent responses (and only consistent responses) produce a “0.” COUNT c =0 d Count the number of consistent responses (those equal to “0”). COUNT c &lt;&gt; 0 e Count the “inconsistent” responses (those not equal to “0”). SUBTRACT d e f Find the difference SCORE f z Keep track of the results of each trial. End one trial, go back and repeat until all 1000 trials are complete. HISTOGRAM z Produce a histogram of the trial result. Note: The file “beerpoll” on the Resampling Stats software disk contains this set of commands. Are Drinkers More Likely to Favor Local Option &amp; Vice Versa # consistent responses thru chance draw The actual results showed a difference of 24. In the histogram we see that a difference that large or larger happened just by chance pairing—without any relationship between the two variables—23% of the time. Hence, we conclude that there is little evidence of a relationship between the two variables. Though the test just described may generally be appropriate for data of this sort, it may well not be appropriate in some particular case. Let’s consider a set of data where even if the test showed that an association existed, we would not believe the test result to be meaningful. Suppose the survey results had been as presented in Table 23-11. We see that non-beer drinkers have a higher rate of approval of allowing beer drinking, which does not accord with experience or reason. Hence, without additional explanation we would not believe that a meaningful relationship exists among these variables even if the test showed one to exist. (Still another reason to doubt that a relationship exists is that the absolute differences are too small—there is only a 6% difference in disapproval between drink and don’t drink groups— to mean anything to anyone. On both grounds, then, it makes sense simply to act as if there were no difference between the two groups and to run no test .). Table 23-11 Beer Poll In Which Results Are Not In Accord With Expectation Or Reason % Approve % Disapprove Total Beer Drinkers 71% 29% 100% Non-Beer Drinkers 77% 23% 100% The lesson to be learned from this is that one should inspect the data carefully before applying a statistical test, and only test for “significance” if the apparent relationships accord with theory, general understanding, and common sense. Example 23-4: Do Athletes Really Have “Slumps”? (Are Successive Events in a Series Independent, or is There a Relationship Between Them?) The important concept of independent events was introduced earlier. Various scientific and statistical decisions depend upon whether or not a series of events is independent. But how does one know whether or not the events are independent? Let us consider a baseball example. Baseball players and their coaches believe that on some days and during some weeks a player will bat better than on other days and during other weeks. And team managers and coaches act on the belief that there are periods in which players do poorly—slumps—by temporarily replacing the player with another after a period of poor performance. The underlying belief is that a series of failures indicates a temporary (or permanent) change in the player’s capacity to play well, and it therefore makes sense to replace him until the evil spirit passes on, either of its own accord or by some change in the player’s style. But even if his hits come randomly, a player will have runs of good luck and runs of bad luck just by chance—just as does a card player. The problem, then, is to determine whether (a) the runs of good and bad batting are merely runs of chance, and the probability of success for each event remains the same throughout the series of events—which would imply that the batter’s ability is the same at all times, and coaches should not take recent performance heavily into account when deciding which players should play; or (b) whether a batter really does have a tendency to do better at some times than at others, which would imply that there is some relationship between the occurrence of success in one trial event and the probability of success in the next trial event, and therefore that it is reasonable to replace players from time to time. Let’s analyze the batting of a player we shall call “Slug.” Here are the results of Slug’s first 100 times at bat during the 1987 season (“H” = hit, “X” = out): X X X X X X H X X H X H H X X X X X X X X H X X X X X H X X X X H H X X X X X H X X H X H X X X H H X X X X X H X H X X X X H H X H H X X X X X X X X X X H X X X H X X H X X H X H X X H X X X H X X X. Now, do Slug’s hits tend to come in bunches? That would be the case if he really did have a tendency to do better at some times than at others. Therefore, let us compare Slug’s results with those of a deck of cards or a set of random numbers that we know has no tendency to do better at some times than at others. During this period of 100 times at bat, Slug has averaged one hit in every four times at bat—a .250 batting average. This average is the same as the chance of one card suit’s coming up. We designate hearts as “hits” and prepare a deck of 100 cards, twenty-five “H”s (hearts, or “hit”) and seventy-five “X”s (other suit, or “out”). Here is the sequence in which the 100 randomly-shuffled cards fell: X X H X X X X H H X X X H H H X X X X X H X X X H X X H X X X X H X H H X X X X X X X X X H X X X X X X H H X X X X X H H H X X X X X X H X H X H X X H X H X X X X X X X X X H X X X X X X X H H H X X. Now we can compare whether or not Slug’s hits are bunched up more than they would be by random chance; we can do so by counting the clusters (also called “runs”) of consecutive hits and outs for Slug and for the cards. Slug had forty-three clusters, which is more than the thirty-seven clusters in the cards; it therefore does not seem that there is a tendency for Slug’s hits to cluster together. (A larger number of clusters indicates a lower tendency to cluster.) Of course, the single trial of 100 cards shown above might have an unusually high or low number of clusters. To be safer, lay out, (say,) ten trials of 100 cards each, and compare Slug’s number of clusters with the various trials. The proportion of trials with more clusters than Slug’s indicates whether or not Slug’s hits have a tendency to bunch up. (But caution: This proportion cannot be interpreted directly as a probability.) Now the steps: Step 1. Constitute a bucket with 3 slips of paper that say “out” and one that says “hit.” Or “01-25” = hits (H), “26-00” = outs (X), Slug’s long-run average. Step 2. Sample 100 slips of paper, with replacement, record “hit” or “out” each time, or write a series of “H’s” or “X’s” corresponding to 100 numbers, each selected randomly between 1 and 100. Step 3. Count the number of “clusters,” that is, the number of “runs” of the same event, “H”s or “X”s. Step 4. Compare the outcome in step 3 with Slug’s outcome, 43 clusters. If 43 or fewer; write “yes,” otherwise “no.” Step 5. Repeat steps 2-4 a hundred times. Step 6. Compute the proportion “yes.” This estimates the probability that Slug’s record is not characterized by more “slumps” than would be caused by chance. A very low proportion of “yeses” indicates longer (and hence fewer) “streaks” and “slumps” than would result by chance. In RESAMPLING STATS, we can do this experiment 1000 times. REPEAT 1000 URN 3#0 1#1 a SAMPLE 100 a b Sample 100 “at-bats” from a RUNS b &gt;=1 c How many runs (of any length &gt;=1) are there in the 100 at-bats? SCORE c z END HISTOGRAM z Note: The file “sluggo” on the Resampling Stats software disk contains this set of commands. Examining the histogram, we see that 43 runs is not at all an unusual occurrence: “Runs” in 100 At-Bats # “runs” of same outcome The manager wants to look at this matter in a somewhat different fashion, however. He insists that the existence of slumps is proven by the fact that the player sometimes does not get a hit for an abnormally long period of time. One way of testing whether or not the coach is right is by comparing an average player’s longest slump in a 100-at-bat season with the longest run of outs in the first card trial. Assume that Slug is a player picked at random . Then compare Slug’s longest slump—say, 10 outs in a row—with the longest cluster of a single simulated 100-at-bat trial with the cards, 9 outs. This result suggests that Slug’s apparent slump might well have resulted by chance. The estimate can be made more accurate by taking the average longest slump (cluster of outs) in ten simulated 400-at-bat trials. But notice that we do not compare Slug’s slump against the longest slump found in ten such simulated trials. We want to know the longest cluster of outs that would be found under average conditions, and the hand with the longest slump is not average or typical. Determining whether to compare Slug’s slump with the average longest slump or with the longest of the ten longest slumps is a decision of crucial importance. There are no mathematical or logical rules to help you. What is required is hard, clear thinking. Experience can help you think clearly, of course, but these decisions are not easy or obvious even to the most experienced statisticians. The coach may then refer to the protracted slump of one of the twenty-five players on his team to prove that slumps really occur. But, of twenty-five random 100-at-bat trials, one will contain a slump longer than any of the other twenty-four, and that slump will be considerably longer than average. A fair comparison, then, would be between the longest slump of his longest-slumping player, and the longest run of outs found among twenty-five random trials. In fact, the longest run among twenty-five hands of 100 cards was fifteen outs in a row. And, if we had set some of the hands for lower (and higher) batting averages than .250, the longest slump in the cards would have been even longer. Research by Roberts and his students at the University of Chicago shows that in fact slumps do not exist, as I conjectured in the first publication of this material in 1969. (Of course, a batter feels as if he has a better chance of getting a hit at some times than at other times. After a series of successful at-bats, sandlot players and professionals alike feel confident—just as gamblers often feel that they’re on a “streak.” But there seems to be no connection between a player’s performance and whether he feels hot or cold, astonishing as that may be.) Averages over longer periods may vary systematically, as Ty Cobb’s annual batting average varied non-randomly from season to season, Roberts found. But short-run analyses of dayto-day and week-to-week individual and team performances in most sports have shown results similar to the outcomes that a lottery-type random-number machine would produce. Remember, too, the study by Gilovich, Vallone, and Twersky of basketball mentioned in Chapter 9. To repeat, their analyses “provided no evidence for a positive correlation between the outcomes of successive shots.” That is, knowing whether a shooter has or has not scored on the previous sheet—or in any previous sequence of shots—is useless for predicting whether he will score again. The species homo sapiens apparently has a powerful propensity to believe that one can find a pattern even when there is no pattern to be found. Two decades ago I cooked up several series of random numbers that looked like weekly prices of publicly-traded stocks. Players in the experiment were told to buy and sell stocks as they chose. Then I repeatedly gave them “another week’s prices,” and allowed them to buy and sell again. The players did all kinds of fancy calculating, using a wild variety of assumptions—although there was no possible way that the figuring could help them. When I stopped the game before completing the 10 buy-andsell sessions they expected, subjects would ask that the game go on. Then I would tell them that there was no basis to believe that there were patterns in the data, because the “prices” were just randomly-generated numbers. Winning or losing therefore did not depend upon the subjects’ skill. Nevertheless, they demanded that the game not stop until the 10 “weeks” had been played, so they could find out whether they “won” or “lost.” This study of batting illustrates how one can test for independence among various trials. The trials are independent if each observation is randomly chosen with replacement from the universe, in which case there is no reason to believe that one observation will be related to the observations directly before and after; as it is said, “the coin has no memory.” The year-to-year level of Lake Michigan is an example in which observations are not independent. If Lake Michigan is very high in one year, it is likely to be higher than average the following year because some of the high level carries over from one year into the next. \\[3\\] We could test this hypothesis by writing down whether the level in each year from, say, 1860 to 1975 was higher or lower than the median level for those years. We would then count the number of runs of “higher” and “lower” and compare the number of runs of “black” and “red” with a deck of that many cards; we would find fewer runs in the lake level than in an average hand of 116 (1976-1860) cards, though this test is hardly necessary. (But are the changes in Lake Michigan’s level independent from year to year? If the level went up last year, is there a better than 50-50 chance that the level will also go up this year? The answer to this question is not so obvious. One could compare the numbers of runs of ups and downs against an average hand of cards, just as with the hits and outs in baseball.) Exercise for students: How could one check whether the successive numbers in a random-number table are independent? 26.5 Exercises Solutions for problems may be found in the section titled, “Exercise Solutions” at the back of this book. Exercise 23-1 Table 23-12 shows voter participation rates in the various states in the 1844 presidential election. Should we conclude that there was a negative relationship between the participation rate (a) and the vote spread (b) between the parties in the election? (Adapted from Noreen (1989), p. 20, Table 2-4) Table 23-12 Voter Participation In The 1844 Presidential Election State Participation (a) Spread (b) Maine 67.5 13 New Hampshire 65.6 19 Vermont 65.7 18 Massachusetts 59.3 12 Rhode Island 39.8 20 Connecticut 76.1 5 New York 73.6 1 New Jersey 81.6 1 Pennsylvania 75.5 2 Delaware 85.0 3 Maryland 80.3 5 Virginia 54.5 6 Nor th Carolina 79.1 5 Georgia 94.0 4 Kentucky 80.3 8 Tennessee 89.6 1 Louisiana 44.7 3 Alabama 82.7 8 Mississippi 89.7 13 Ohio 83.6 2 Indiana 84.9 2 Illinois 76.3 12 Missouri 74.7 17 Arkansas 68.8 26 Michigan 79.3 6 National Average 74.9 9 The observed correlation coefficient between voter participation and spread is -.37398. Is this more negative that what might occur by chance, if no correlation exists? Exercise 23-2 We would like to know whether, among major-league baseball players, home runs (per 500 at-bats) and strikeouts (per 500 at-bat’s) are correlated. We first use the procedure as used above for I.Q. and athletic ability—multiplying the elements within each pair. (We will later use a more “sophisticated” measure, the correlation coefficient.) The data for 18 randomly-selected players in the 1989 season are as follows, as they would appear in the first lines of the program. NUMBERS (14 20 0 38 9 38 22 31 33 11 40 5 15 32 3 29 5 32) homeruns NUMBERS (135 153 120 161 138 175 126 200 205 147 165 124 169 156 36 98 82 131) strikeout Exercise: Complete this program. Exercise 23-3 In the previous example relating strikeouts and home runs, we used the procedure of multiplying the elements within each pair. Now we use a more “sophisticated” measure, the correlation coefficient, which is simply a standardized form of the multiplicands, but sufficiently well known that we calculate it with a pre-set command. Exercise: Write a program that uses the correlation coefficient to test the significance of the association between home runs and strikeouts. Exercise 23-4 All the other things equal, an increase in a country’s money supply is inflationary and should have a negative impact on the exchange rate for the country’s currency. The data in the following table were computed using data from tables in the 1983/1984 Statistical Yearbook of the United Nations : Table 23-13 Money Supply and Exchange Rate Changes % Change % Change % Change % Change Exch. Rate Money Supply Exch. Rate Money Supply Australia 0.089 0.035 Belgium 0.134 0.003 Botswana 0.351 0.085 Burma 0.064 0.155 Burundi 0.064 0.064 Canada 0.062 0.209 Chile 0.465 0.126 China 0.411 0.555 Costa Rica 0.100 0.100 Cyprus 0.158 0.044 Denmark 0.140 0.351 Ecuador 0.242 0.356 Fiji 0.093 0.000 Finland 0.124 0.164 France 0.149 0.090 Germany 0.156 0.061 Greece 0.302 0.202 Hungary 0.133 0.049 India 0.187 0.184 Indonesia 0.080 0.132 Italy 0.167 0.124 Jamaica 0.504 0.237 Japan 0.081 0.069 Jordan 0.092 0.010 Kenya 0.144 0.141 Korea 0.040 0.006 Kuwait 0.038 -0.180 Lebanon 0.619 0.065 Madagascar 0.337 0.244 Malawi 0.205 0.203 Malaysia 0.037 -0.006 Malta 0.003 0.003 Mauritania 0.180 0.192 Mauritius 0.226 0.136 Mexico 0.338 0.599 Morocco 0.076 0.076 Netherlands 0.158 0.078 New Zealand 0.370 0.098 Nigeria 0.079 0.082 Norway 0.177 0.242 Papua 0.075 0.209 Philippines 0.411 0.035 Por tugal 0.288 0.166 Romania -0.029 0.039 Rwanda 0.059 0.083 Samoa 0.348 0.118 Saudi Arabia 0.023 0.023 Seychelles 0.063 0.031 Singapore 0.024 0.030 Solomon Is 0.101 0.526 Somalia 0.481 0.238 South Africa 0.624 0.412 Spain 0.107 0.086 Sri Lanka 0.051 0.141 Switzerland 0.186 0.186 Tunisia 0.193 0.068 Turkey 0.573 0.181 UK 0.255 0.154 USA 0.000 0.156 Vanatuva 0.008 0.331 Yemen 0.253 0.247 Yugoslavia 0.685 0.432 Zaire 0.343 0.244 Zambia 0.457 0.094 Zimbabwe 0.359 0.164 Percentage changes in exchange rates and money supply between 1983 and 1984 for various countries. Are changes in the exchange rates and in money supplies related to each other? That is, are they correlated? (adapted from Noreen, 1990) Exercise: Should the algorithm of non-computer resampling steps be similar to the algorithm for I.Q. and athletic ability shown in the text? One can also work with the correlation coefficient rather then the sum-of-products method, and expect to get the same result. Write a series of non-computer resampling steps to solve this problem. Write a computer program to implement those steps. 26.6 Endnotes For a much fuller discussion see Simon and Burstein (1985, Chapter 35), or previous editions by Simon (1960; 1979). These data are from an example in W. J. Dixon and F. J. Massey (1957, p. 226), in which the problem is tackled conventionally with a chi-square test. Example from Wallis and Roberts (1957), from p 565. References "],["how-large-a-sample.html", "27 How Large a Sample? 27.1 Issues in determining sample size 27.2 Some practical examples 27.3 Step-wise sample-size determination 27.4 Summary 27.5 Endnotes", " 27 How Large a Sample? 27.1 Issues in determining sample size Sometime in the course of almost every study—preferably early in the planning stage—the researcher must decide how large a sample to take. Deciding the size of sample to take is likely to puzzle and distress you at the beginning of your research career. You have to decide somehow, but there are no simple, obvious guides for the decision. For example, one of the first studies I worked on was a study of library economics (Fussler and Simon, 1961), which required taking a sample of the books from the library’s collections. Sampling was expensive, and we wanted to take a correctly sized sample. But how large should the sample be? The longer we searched the literature, and the more people we asked, the more frustrated we got because there just did not seem to be a clear-cut answer. Eventually we found out that, even though there are some fairly rational ways of fixing the sample size, most sample sizes in most studies are fixed simply (and irrationally) by the amount of money that is available or by the sample size that similar pieces of research have used in the past. The rational way to choose a sample size is by weighing the benefits you can expect in information against the cost of increasing the sample size. In principle you should continue to increase the sample size until the benefit and cost of an additional sampled unit are equal. 1 The benefit of additional information is not easy to estimate even in applied research, and it is extraordinarily difficult to estimate in basic research. Therefore, it has been the practice of researchers to set up target goals of the degree of accuracy they wish to achieve, or to consider various degrees of accuracy that might be achieved with various sample sizes, and then to balance the degree of accuracy with the cost of achieving that accuracy. The bulk of this chapter is devoted to learning how the sample size is related to accuracy in simple situations. In complex situations, however, and even in simple situations for beginners, you are likely to feel frustrated by the difficulties of relating accuracy to sample size, in which case you cry out to a supervisor, “Don’t give me complicated methods, just give me a rough number based on your greatest experience.” My inclination is to reply to you, “Sometimes life is hard and there is no shortcut.” On the other hand, perhaps you can get more information than misinformation out of knowing sample sizes that have been used in other studies. Table 24-1 shows the middle (modal), 25th percentile, and 75th percentile scores for—please keep this in mind— National Opinion Surveys in the top panel. The bottom panel shows how subgroup analyses affect sample size. Pretest sample sizes are smaller, of course, perhaps 25-100 observations. Samples in research for Master’s and Ph.D. theses are likely to be closer to a pretest than to national samples. Table 24-1 Most Common Sample Sizes Used for National and Regional Studies By Subject Matter Subject Matter National Regional Mode Q3 Q1 Mode Q3 Q1 Financial 1000+ — — 100 400 50 Medical 1000+ 1000+ 500 1000+ 1000+ 250 Other Behavior 1000+ — — 700 1000 300 Attitudes 1000+ 1000+ 1500 700 1000 400 Laboratory Experiments — — — 100 200 50 Typical Sample Sizes for Studies of Human and Institutional Populations People or Households Institutions Subgroup Analyses National Special National Special Average 1500-2500 500-1000 500-1000 200-500 Many 2500+ 1000+ 1000+ 500+ SOURCE: From Applied Sampling , by Seymour Sudman, pp. 86-87. Copyright 1976 by Academic Press, reprinted by permission. Once again, the sample size ought to depend on the proportions of the sample that have the characteristics you are interested in, the extent to which you want to learn about subgroups as well as the universe as a whole, and of course the purpose of your study, the value of the information, and the cost. Also, keep in mind that the added information that you obtain from an additional sample observation tends to be smaller as the sample size gets larger. You must quadruple the sample to halve the error. Now let us consider some specific cases. The first examples taken up here are from the descriptive type of study, and the latter deal with sample sizes in relationship research. 27.2 Some practical examples Example 24-1 What proportion of the homes in Countryville are tuned into television station WCNT’s ten o’clock news program? That is the question your telephone survey aims to answer, and you want to know how many randomly selected homes you must telephone to obtain a sufficiently large sample. Begin by guessing the likeliest answer, say 30 percent in this case. Do not worry if you are off by 5 per cent or even 10 per cent; and you will probably not be further off than that. Select a first-approximation sample size of perhaps 400; this number is selected from my general experience, but it is just a starting point. Then proceed through the first 400 numbers in the random-number table, marking down a yes for numbers 1-3 and no for numbers 4-10 (because 3/10 was your estimate of the proportion listening). Then add the number of yes and no . Carry out perhaps ten sets of such trials, the results of which are in Table 24-2. Table 24-2 % DIFFERENCE FROM Trial Number “Yes” Number “No” Expected Mean of 30% (120 “Yes”) 1 115 285 1.25 2 119 281 0.25 3 116 284 1.00 4 114 286 1.50 5 107 293 3.25 6 116 284 1.00 7 132 268 3.00 8 123 277 0.75 9 121 279 0.25 10 114 286 1.50 Mean 1.37 Based on these ten trials, you can estimate that if you take a sample of 400 and if the “real” viewing level is 30 percent, your average percentage error will be 1.375 percent on either side of 30 percent. That is, with a sample of 400, half the time your error will be greater than 1.375 percent if 3/10 of the universe is listening. Now you must decide whether the estimated error is small enough for your needs. If you want greater accuracy than a sample of 400 will give you, increase the sample size, using this important rule of thumb: To cut the error in half, you must quadruple the sample size. In other words, if you want a sample that will give you an error of only 0.55 percent on the average, you must increase the sample size to 1,600 interviews. Similarly, if you cut the sample size to 100, the average error will be only 2.75 percent (double 1.375 percent) on either side of 30 percent. If you distrust this rule of thumb, run ten or so trials on sample sizes of 100 or 1,600, and see what error you can expect to obtain on the average. If the “real” viewership is 20 percent or 40 percent, instead of 30 percent, the accuracy you will obtain from a sample size of 400 will not be very different from an “actual” viewership of 30 percent, so do not worry about that too much, as long as you are in the right general vicinity. Accuracy is slightly greater in smaller universes but only slightly. For example, a sample of 400 would give perfect accuracy if Countryville had only 400 residents. And a sample of 400 will give slightly greater accuracy for a town of 800 residents than for a city of 80,000 residents. But, beyond the point at which the sample is a large fraction of the total universe, there is no difference in accuracy with increases in the size of universe. This point is very important. For any given level of accuracy, identical sample sizes give the same level of accuracy for Podunk (population 8,000) or New York City (population 8 million). The ratio of the sample size to the population of Podunk or New York City means nothing at all, even though it intuitively seems to be important. The size of the sample must depend upon which population or subpopulations you wish to describe. For example, A. Kinsey’s sample size would have seemed large, by customary practice, for generalizations about the United States population as a whole. But, as Kinsey explains: “The chief concern of the present study is an understanding of the sexual behavior of each segment of the population , and it is only secondarily concerned with generalization for the population as a whole” (Kinsey, et al., 1948, p. 82, italics added). Therefore Kinsey’s sample had to include subsamples large enough to obtain the desired accuracy in each of these sub-universes. The U.S. Census offers a similar illustration. When the U.S. Bureau of the Census aims to estimate only a total or an average for the United States as a whole—as, for example, in the Current Population Survey estimate of unemployment—a sample of perhaps 50,000 is big enough. But the decennial census aims to make estimates for all the various communities in the country, estimates that require adequate subsamples in each of these sub-universes; such is the justification for the decennial census’ sample size of so many millions. Television ratings illustrate both types of purpose. Nielsen ratings, for example, are sold primarily to national network advertisers. These advertisers on national television networks usually sell their goods all across the country and are therefore interested primarily in the total United States viewership for a program, rather than in the viewership in various demographic subgroups. The appropriate calculations for Nielsen sample size will therefore refer to the total United States sample. But other organizations sell rating services to local television and radio stations for use in soliciting advertising over the local stations rather than over the network as a whole. Each local sample must then be large enough to provide reasonable accuracy, and, considered as a whole, the samples for the local stations therefore add up to a much larger sample than the Nielsen and other nationwide samples. The problem may be handled with the following RESAMPLING STATS program. This program represents viewers with random numbers between 1 and 100 and, consistent with our guess that 30% are tuned in, represents viewers with the numbers 1-30. It GENERATES a sample of 400 such numbers, COUNTS the “viewers,” then finds how much this sample diverges from the expected number of viewers (30% of 400 = 120). It repeats this procedure 1000 times, and then calculates the average divergence. REPEAT 1000 Do 1000 trials GENERATE 400 1,100 a Generate 400 numbers between 1 and 100, let 1-30 = viewer COUNT a &lt;=30 b Count the viewers SUBTRACT 120 b c How different from expected? ABS c d Absolute value of difference? DIVIDE d 400 e Express as a proportion of sample SCORE e z Keep score of the result END MEAN z k Find the mean divergence Note: The file “tvviewer” on the Resampling Stats software disk contains this set of commands. It is a simple matter to go back and try a sample size of (say) 1600 rather than 400, and examine the effect on the mean difference. Example 24-2 This example, like Example 24-1, illustrates the choice of sample size for estimating a summarization statistic. Later examples deal with sample sizes for probability statistics. Hark back to the pig-ration problems presented earlier, and consider the following set of pig weight-gains recorded for ration A: 31, 34, 29, 26, 32, 35, 38, 34, 31, 29, 32, 30. Assume that our purpose now is to estimate the average weight gain for ration A, so that the feed company can advertise to farmers how much weight gain to expect from ration A. If the universe is made up of pig weight-gains like those we observed, we can simulate the universe with, say, 1 million weight gains of thirty-one pounds, 1 million of thirty-four pounds, and so on for the twelve observed weight gains. Or, more conveniently, as accuracy will not be affected much, we can make up a universe of say, thirty cards for each thirty-one-pound gain, thirty cards for each thirty-four-pound gains and so forth, yielding a deck of 30 x 12 = 360 cards. Then shuffle, and, just for a starting point, try sample sizes of twelve pigs. The means of the samples for twenty such trials are as in Table 24-3. Now ask yourself whether a sample size of twelve pigs gives you enough accuracy. There is a .5 chance that the mean for the sample will be more than .65 or .92 pound (the two median deviations) or (say) .785 pound (the midpoint of the two medians) from the mean of the universe that generates such samples, which in this situation is 31.75 pounds. Is this close enough? That is up to you to decide in light of the purposes for which you are running the experiment. (The logic of the inference you make here is inevitably murky, and use of the term “real mean” can make it even murkier, as is seen in the discussion in Chapters 20-22 on confidence intervals.) To see how accuracy is affected by larger samples, try a sample size of forty-eight “pigs” dealt from the same deck. (But, if the sample size were to be much larger than forty-eight, you might need a “universe” greater than 360 cards.) The results of twenty trials are in Table 24-4. In half the trials with a sample size of forty-eight the difference between the sample mean and the “real” mean of 31.75 will be .36 or .37 pound (the median deviations), smaller than with the values of .65 and .92 for samples of 12 pigs. Again, is this too little accuracy for you? If so, increase the sample size further. Table 24-3 Trial Mean Absolut e Devisatio n of Trial Mean from Actual Mean Trial Mean Absolut e Deviation of Trial Mean from Actual Mean 1 31.77 .02 11 32.10 .35 2 32.27 1.52 12 30.67 1.08 3 31.75 .00 13 32.42 .67 4 30.83 .92 14 30.67 1.08 5 30.52 1.23 15 32.25 .50 6 31.60 .15 16 31.60 .15 7 32.46 .71 17 32.33 .58 8 31.10 .65 18 33.08 1.33 9 32.42 .35 19 33.01 1.26 10 30.60 1.15 20 30.60 1.15 Mean 31.75 The attentive reader of this example may have been troubled by this question: How do you know what kind of a distribution of values is contained in the universe before the sample is taken? The answer is that you guess, just as in Example 24-1 you guessed at the mean of the universe. If you guess wrong, you will get either more accuracy or less accuracy than you expected from a given sample size, but the results will not be fatal; if you obtain more accuracy than you wanted, you have wasted some money, and, if you obtain less accuracy, your sample dispersion will tell you so, and you can then augment the sample to boost the accuracy. But an error in guessing will not introduce error into your final results. Table 24-4 Trial Mean Absolut e Deviation of Trial Mean from Actual Mean Trial Mean Absolut e Deviation of Trial Mean from Actual Mean 1 31.80 .05 11 31.93 .18 2 32.27 .52 12 32.40 .65 3 31.82 .07 13 31.32 .43 4 31.39 .36 14 32.07 .68 5 31.22 .53 15 32.03 .28 6 31.88 .13 16 31.95 .20 7 31.37 .38 17 31.75 .00 8 31.48 .27 18 31.11 .64 9 31.20 .55 19 31.96 .21 10 32.01 .26 20 31.32 .43 Mean 31.75 The guess should be based on something, however. One source for guessing is your general knowledge of the likely dispersion; for example, if you were estimating male heights in Rhode Island, you would be able to guess what proportion of observations would fall within 2 inches, 4 inches, 6 inches, and 8 inches, perhaps, of the real value. Or, much better yet, a very small pretest will yield quite satisfactory estimates of the dispersion. Here is a RESAMPLING STATS program that will let you try different sample sizes, and then take bootstrap samples to determine the range of sampling error. You set the sample size with the DATA command, and the NUMBERS command records the data. Above I noted that we could sample without replacement from a “deck” of thirty “31”’s, thirty “34”’s, etc, as a substitute for creating a universe of a million “31”’s, a million “34”’s, etc. We can achieve the same effect if we replace each card after we sample it; this is equivalent to creating a “deck” of an infinite number of “31”’s, “34”’s, etc. That is what the SAMPLE command does, below. Note that the sample size is determined by the value of the “sampsize” variable, which you set at the beginning. From here on the program takes the MEAN of each sample, keeps SCORE of that result, and produces a HISTOGRAM. The PERCENTILE command will also tell you what values enclose 90% of all sample results, excluding those below the 5th percentile and above the 95th percentile. Here is a program for a sample size of 12. DATA (12) sampsize NUMBERS (31 34 29 26 32 35 38 34 32 31 30 29) a REPEAT 1000 SAMPLE sampsize a b MEAN b c SCORE c z END HISTOGRAM z PERCENTILE z (5 95) k PRINT k Bin Center Freq Pct Cum Pct 29.0 2 0.2 0.2 29.5 4 0.4 0.6 30.0 30 3.0 3.6 30.5 71 7.1 10.7 31.0 162 16.2 26.9 31.5 209 20.9 47.8 32.0 237 23.7 71.5 32.5 143 14.3 85.8 33.0 90 9.0 94.8 33.5 37 3.7 98.5 34.0 12 1.2 99.7 34.5 3 0.3 100.0 k = 30.417 33.25 Example 24-3 This is the first example of sample-size estimation for probability (testing) statistics, rather than the summarization statistics dealt with above. Recall the problem of the sex of fruit-fly offspring discussed in Example 15-1. The question now is, how large a sample is needed to determine whether the radiation treatment results in a sex ratio other than a 50-50 male-female split? The first step is, as usual, difficult but necessary. As the researcher, you must guess what the sex ratio will be if the treatment does have an effect. Let’s say that you use all your general knowledge of genetics and of this treatment and that you guess the sex ratio will be 75 percent males and 25 percent females if the treatment alters the ratio from 50-50. In the random-number table let “01-25” stand for females and “26-00” for males. Take twenty successive pairs of numbers for each trial, and run perhaps fifty trials, as in Table 24-5. Table 24-5 1 4 16 18 7 13 34 4 16 2 6 14 19 3 17 35 6 14 3 6 14 20 7 13 36 3 17 4 5 15 21 4 16 37 8 12 5 5 15 22 4 16 38 4 16 6 3 17 23 5 15 39 3 17 7 7 13 24 8 12 40 6 14 8 6 14 25 4 16 41 5 15 9 3 17 26 1 19 42 2 18 10 2 18 27 5 15 43 8 12 11 6 14 28 3 17 44 4 16 12 1 19 29 8 12 45 6 14 13 6 14 30 8 12 46 5 15 14 3 17 31 5 15 47 3 17 15 1 19 32 3 17 48 5 15 16 5 15 33 4 16 49 3 17 17 5 15 50 5 15 n Example 15-1 with a sample of twenty flies that contained Trial Females Males Trial Females Males Trial Females Males I fourteen or more males, we found only an 8% probability that such an extreme sample would result from a 50-50 universe. Therefore, if we observe such an extreme sample, we rule out a 50-50 universe. Now Table 24-5 tells us that, if the ratio is really 75 to 25, then a sample of twenty will show fourteen or more males forty-two of fifty times (84 percent of the time). If we take a sample of twenty flies and if the ratio is really 75-25, we will make the correct decision by deciding that the split is not 50-50 84 percent of the time. Perhaps you are not satisfied with reaching the right conclusion only 84 percent of the time. In that case, still assuming that the ratio will really be 75-25 if it is not 50-50, you need to take a sample larger than twenty flies. How much larger? That depends on how much surer you want to be. Follow the same procedure for a sample size of perhaps eighty flies. First work out for a sample of eighty, as was done in Example 15-1 for a sample of twenty, the number of males out of eighty that you would need to find for the odds to be, say, 9 to 1 that the universe is not 50-50; your estimate turns out to be forty-eight males. Then run fifty trials of eighty flies each on the basis of 75-25 probability, and see how often you would not get as many as forty-eight males in the sample. Table 24-6 shows the results we got. No trial was anywhere near as low as forty-eight, which suggests that a sample of eighty is larger than necessary if the split is really 75-25. Table 24-6 Trial Females Males Trial Females Males Trial Females Males 1 21 59 18 13 67 34 21 59 2 22 58 19 19 61 35 17 63 3 13 67 20 17 63 36 22 58 4 15 65 21 17 63 37 19 61 5 22 58 22 18 62 38 21 59 6 21 59 23 26 54 39 21 59 7 13 67 24 20 60 40 21 59 8 24 56 25 16 64 41 21 59 9 16 64 26 22 58 42 18 62 10 21 59 27 16 64 43 19 61 11 20 60 28 21 59 44 17 63 12 19 61 29 22 58 45 13 67 13 21 59 30 21 59 46 16 64 14 17 63 31 22 58 47 21 59 15 22 68 32 19 61 48 16 64 16 22 68 33 10 70 49 17 63 17 17 63 50 21 59 Table 24-7 Trial Females Males Trial Females Males Trial Females Males 1 35 45 18 32 48 34 35 45 2 36 44 19 28 52 35 36 44 3 35 45 20 32 48 36 29 51 4 35 45 21 33 47 37 36 44 5 36 44 22 37 43 38 36 44 6 36 44 23 36 44 39 31 49 7 36 44 24 31 49 40 29 51 8 34 46 25 27 53 41 30 50 9 34 46 26 30 50 42 35 45 10 29 51 27 31 49 43 32 48 11 29 51 28 33 47 44 30 50 12 32 48 29 37 43 45 37 43 13 29 51 30 30 50 46 31 49 14 31 49 31 31 49 47 36 44 15 28 52 32 32 48 48 34 64 16 33 47 33 34 46 49 29 51 17 36 44 50 37 43 It is obvious that, if the split you guess at is 60 to 40 rather than 75 to 25, you will need a bigger sample to obtain the “correct” result with the same probability. For example, run some eighty-fly random-number trials with 1-40 representing males and 51-100 representing females. Table 24-7 shows that only twenty-four of fifty (48 percent) of the trials reach the necessary cut-off at which one would judge that a sample of eighty really does not come from a universe that is split 50-50; therefore, a sample of eighty is not big enough if the split is 60-40. To review the main principles of this example: First, the closer together the two possible universes from which you think the sample might have come (50-50 and 60-40 are closer together than are 50-50 and 75-25), the larger the sample needed to distinguish between them. Second, the surer you want to be that you reach the right decision based upon the sample evidence, the larger the sample you need. The problem may be handled with the following RESAMPLING STATS program. We construct a benchmark universe that is 60-40 male-female, and take samples of size 80, observing whether the numbers of males and females differs enough in these resamples to rule out a 50-50 universe. Recall that we need at least 48 males to say that the proportion of males is not 50%. REPEAT 1000 Do 1000 trials GENERATE 80 1,10 a Generate 80 “flies,” each represented by a number between 1 and 10 where &lt;= 6 is a male COUNT a &lt;=6 b Count the males SCORE b z Keep score END COUNT z &gt;=48 k How many of the trials produced more than 48 males? DIVIDE k 1000 kk Convert to a proportion PRINT kk If the result “kk” is close to 1, we then know that samples of size 80 will almost always produce samples with enough males to avoid misleading us into thinking that they could have come from a universe in which males and females are split 50-50. Example 24-3 Referring back to Example 15-3, on the cable-television poll, how large a sample should you have taken? Pretend that the data have not yet been collected. You need some estimate of how the results will turn out before you can select a sample size. But you have not the foggiest idea how the results will turn out. Therefore, go out and take a very small sample, maybe ten people, to give you some idea of whether people will split quite evenly or unevenly. Seven of your ten initial interviews say they are for CATV. How large a sample do you now need to provide an answer of which you can be fairly sure? Using the techniques of the previous chapter, we estimate roughly that from a sample of fifty people at least thirty-two would have to vote the same way for you to believe that the odds are at least 19 to 1 that the sample does not misrepresent the universe, that is, that the sample does not show a majority different from that of the whole universe if you polled everyone. This estimate is derived from the resampling experiment described in example 15-3. The table shows that if half the people (or more) are against cable television, only one in twenty times will thirty-two (or more) people of a sample of fifty say that they are for cable television; that is, only one of twenty trials with a 50-50 universe will produce as many as thirty-two yeses if a majority of the population is against it. Therefore, designate numbers 1-30 as no and 31-00 as yes in the random-number table (that is, 70 percent, as in your estimate based on your presample of ten), work through a trial sample size of fifty, and count the number of yeses . Run through perhaps ten or fifteen trials, and reckon how often the observed number of yeses exceeds thirty-two, the number you must exceed for a result you can rely on. In Table 24-8 we see that a sample of fifty respondents, from a universe split 70-30, will show that many yeses a preponderant proportion of the time— in fact, in fifteen of fifteen experiments; therefore, the sample size of fifty is large enough if the split is “really” 70-30. Table 24-8 Trial No Yes Trial No Yes 1 13 37 9 15 35 2 14 36 10 9 41 3 18 32 11 15 35 4 10 40 12 15 35 5 13 37 13 9 41 6 15 35 14 16 34 7 14 36 15 17 33 The following RESAMPLING STATS program takes samples of size 50 from a universe that is 70% “yes.” It then observes how often such samples produce more than 32 “yeses”—the number we must get if we are to be sure that the sample is not from a 50/50 universe. REPEAT 1000 Do 1000 trials GENERATE 50 1,10 a Generate 50 numbers between 1 and 10, let 1-7 = yes. COUNT a &lt;=7 b Count the “yeses” SCORE b z Keep score of the result END COUNT z &gt;=32 k Count how often the sample result &gt;= our 32 cutoff (recall that samples with 32 or fewer “yeses” cannot be ruled out of a 50/50 universe) DIVIDE k 1000 kk Convert to a proportion If “kk” is close to 1, we can be confident that this sample will be large enough to avoid a result that we might mistakenly think comes from a 50/50 universe (provided that the real universe is 70% favorable). Example 24-4 How large a sample is needed to determine whether there is any difference between the two pig rations in Example 15-7? The first step is to guess the results of the tests. You estimate that the average for ration A will be a weight gain of thirty-two pounds. You further guess that twelve pigs on ration A might gain thirty-six, thirty-five, thirty-four, thirty-three, thirty-three, thirty-two, thirty-two, thirty-one, thirty-one, thirty, twentynine, and twenty-eight pounds. This set of guesses has an equal number of pigs above and below the average and more pigs close to the average than farther away. That is, there are more pigs at 33 and 31 pounds than at 36 and 28 pounds. This would seem to be a reasonable distribution of pigs around an average of 32 pounds. In similar fashion, you guess an average weight gain of 28 pounds for ration B and a distribution of 32, 31, 30, 29, 29, 28, 28, 27, 27, 26, 25, and 24 pounds. Let us review the basic strategy. We want to find a sample size large enough so that a large proportion of the time it will reveal a difference between groups big enough to be accepted as not attributable to chance. First, then, we need to find out how big the difference must be to be accepted as evidence that the difference is not attributable to chance. We do so from trials with samples that size from the benchmark universe. We state that a difference larger than the benchmark universe will usually produce is not attributable to chance. In this case, let us try samples of 12 pigs on each ration. First we draw two samples from a combined benchmark universe made up of the results that we have guessed will come from ration A and ration B. (The procedure is the same as was followed in Example 15-7.) We find that in 19 out of 20 trials the difference between the two observed groups of 12 pigs was 3 pounds or less. Now we investigate how often samples of 12 pigs, drawn from the separate universes, will show a mean difference as large as 3 pounds. We do so by making up a deck of 25 or 50 cards for each of the 12 hypothesized A’s and each of the 12 B’s, with the ration name and the weight gain written on it—that is, a deck of, say, 300 cards for each ration. Then from each deck we draw a set of 12 cards at random, record the group averages, and find the difference. Here is the same work done with more runs on the computer: NUMBERS (31 34 29 26 32 35 38 34 32 31 30 29) a NUMBERS (32 32 31 30 29 29 29 28 28 26 26 24) b REPEAT 1000 SAMPLE 12 a aa MEAN aa aaa SAMPLE 12 b bb MEAN bb bbb SUBTRACT aaa bbb c SCORE c z END HISTOGRAM z Difference in mean weights between resamples nough, and, in fact, even smaller samples might be suf Therefore, two samples of twelve pigs each are clearly large enough, and, in fact, even smaller samples might be sufficient if the universes are really like those we guessed at. If, on the other hand, the differences in the guessed universes had been smaller, then twelve-pig groups would have seemed too small and we would then have had to try out larger sample sizes, say forty-eight pigs in each group and perhaps 200 pigs in each group if forty-eight were not enough. And so on until the sample size is large enough to promise the accuracy we want. (In that case, the decks would also have to be much larger, of course.) If we had guessed different universes for the two rations, then the sample sizes required would have been larger or smaller. If we had guessed the averages for the two samples to be closer together, then we would have needed larger samples. Also, if we had guessed the weight gains within each universe to be less spread out, the samples could have been smaller and vice versa. The following RESAMPLING STATS program first records the data from the two samples, and then draws from decks of infinite size by sampling with replacement from the original samples. DATA (36 35 34 33 33 32 32 31 31 30 29 28) a DATA (32 31 30 29 29 28 28 27 27 26 25 24) b REPEAT 1000 SAMPLE 12 a aa Draw a sample of 12 from ration a with replacement (this is like drawing from a large deck made up of many replicates of the elements in a) SAMPLE 12 b bb Same for b MEAN aa aaa Find the averages of the resamples MEAN bb bbb SUBTRACT aaa bbb c Find the difference SCORE c z END COUNT z &gt;=3 k How often did the difference exceed the cutoff point for our significance test of 3 pounds? DIVIDE k 1000 kk PRINT kk If kk is close to zero, we know that the sample size is large enough that samples drawn from the universes we have hypothesized will not mislead us into thinking that they could come from the same universe. 27.3 Step-wise sample-size determination Often it is wisest to determine the sample size as you go along, rather than fixing it firmly in advance. In sequential sampling, you continue sampling until the split is sufficiently even to make you believe you have a reliable answer. Related techniques work in a series of jumps from sample size to sample size. Step-wise sampling makes it less likely that you will take a sample that is much larger than necessary. For example, in the cable-television case, if you took a sample of perhaps fifty you could see whether the split was as wide as 32-18, which you figure you need for 9 to 1 odds that your answer is right. If the split were not that wide, you would sample another fifty, another 100, or however large a sample you needed until you reached a split wide enough to satisfy you that your answer was reliable and that you really knew which way the entire universe would vote. Step-wise sampling is not always practical, however, and the cable-television telephone-survey example is unusually favorable for its use. One major pitfall is that the early responses to a mail survey, for example, do not provide a random sample of the whole, and therefore it is a mistake simply to look at the early returns when the split is not wide enough to justify a verdict. If you have listened to early radio or television reports of election returns, you know how misleading the reports from the first precincts can be if we regard them as a fair sample of the whole. 2 Stratified sampling is another device that helps reduce the sample size required, by balancing the amounts of information you obtain in the various strata. (Cluster sampling does not reduce the sample size. Rather, it aims to reduce the cost of obtaining a sample that will produce a given level of accuracy.) 27.4 Summary Sample sizes are too often determined on the basis of conven- tion or of the available budget. A more rational method of choosing the size of the sample is by balancing the diminution of error expected with a larger sample, and its value, against the cost of increasing the sample size. The relationship of various sample sizes to various degrees of accuracy can be estimated with resampling methods, which are illustrated here. 27.5 Endnotes R. Schlaifer (1961) attacks the sample-size problem in the tistically knowled w g i e d a e b r le c ore n a t ed x e t r o c f a d n e f c i i n s d i oa n n m e a x k ce i nll g e , n c t o d s i t sc , u a nss d io b n e no e f f s i a t sm . Tp h le e s s i t z a e - in M. Hansen, et al. A. Mace gives many examples of the appropriate calculation in an engineering framework. See J. Lorie and H. Roberts (pp. 155-157) for more discus- sion of the limitations of sequential sampling. And M. Hansen, et al., warn against the danger of increasing the sample size in this fashion: The investigator examines the returns from an initial sample to determine whether they appear acceptable to the investigator; if they do, he uses the results as they are; if they do not, he discards the sample results \\[or keeps the old sample\\] and draws a new sample, perhaps by a different method, in the hope that he will obtain a result more nearly like the one he expected. Such an approach can be utilized to obtain almost any results desired, or can “prove” any point even when unbiased or consistent methods of selecting the sample and making the individual estimates are used if the initial results are subject to relatively large sampling errors. (Hansen, et al., 1953: 78) "],["bayesian-analysis-by-simulation.html", "28 Bayesian Analysis by Simulation 28.1 Simple decision problems Fundamental problems in statistical practice 28.2 Problems based on normal and other distributions Conclusion 28.3 Endnotes", " 28 Bayesian Analysis by Simulation Bayesian analysis is a way of thinking about problems in probability and statistics that can help one reach otherwise-difficult decisions. It also can sometimes be used in science. The range of its recommended uses is controversial, but this chapter deals only with those uses of Bayesian analysis that are uncontroversial. Better than defining Bayesian analysis in formal terms is to demonstrate its use. Therefore, to make clear the nature of “Bayes’ rule,” we shall start with the simplest sort of problem, and proceed gradually from there. 28.1 Simple decision problems Assessing the Likelihood That a Used Car Will Be Sound Consider a problem in estimating the soundness of a used car one considers purchasing (after Wonnacott and Wonnacott, 1990, p. 93). Seventy percent of the cars are known to be OK on average, and 30 percent are faulty. Of the cars that are really OK, a mechanic correctly identifies 80 percent as “OK” but says that 20 percent are “faulty”; of those that are faulty, the mechanic correctly identifies 90 percent as faulty and says (incorrectly) that 10 percent are OK. We wish to know the probability that if the mechanic says a car is “OK,” it really is faulty. One can get the desired probabilities directly by simulation without knowing Bayes’ Law, as we shall see. But one must be able to model the physical problem correctly in order to proceed with the simulation; this requirement of a clearly-vi- sualized model is a strong point in favor of simulation. The following steps determine the probability that a car said to be “OK” will turn out to be really faulty: Model in percentages the universe of all cars as a bucket of 100 balls. Working from the data as given above, and referring to first principles, color (.9 * .3 =) .27 of the 100 balls violet (the 27 faulty balls said to be “faulty”), (.1 * .3 =) .03 of the 100 balls blue (3 balls said “OK” but faulty), (.2 * .7 =) .14 of the balls orange (14 OK cars said to be “faulty” balls), and (.8 * .7 = ) 56 balls maroon (said to be “OK” that really are OK). A Venn diagram may help with this step, but it is not necessary. An even better procedure would be to work directly from the concrete data. One would note, for example, that of 200 cars previously observed, 54 were faulty and were said to be “faulty,” 6 were faulty and were said to be “OK,” 28 were OK but were said to be “faulty,” and 112 were OK and were said to be “OK.” Then make a bucket of 54 violets, 6 blue, 28 orange, and 112 maroon. Draw a ball. If it is one of those said to be “faulty”—that is, violet or orange—draw (with replacement) another ball. Continue until obtaining a ball said to be “OK”—that is, a blue or maroon ball. Then record its color. Repeat step 2 perhaps 1000 times and compute the proportions of blues among the recorded results. -OR- Choose a number randomly between “1” and “100.” If “28-30” or \"31-45:, record; otherwise draw another number, and repeat until a number is recorded. Repeat step 2 and count the proportion “28-30” among the total “28-45.” The key modeling step is excluding a trial from consideration (without making any record of it) if it produces an irrelevant observation, and continuing to do so until the process produces an observation of the sort about which you are presently inquiring. Using RESAMPLING STATS, an answer may be produced as follows: “01 - 27” = actually faulty, said to be “faulty” “28 - 30 = faulty, said to be”OK\" “31 - 86” = OK, “OK” “87 - 100” = OK, “faulty” REPEAT 1000 Do 1000 repetitions GENERATE 1 1,100 a Generate a number between “1” and “100” IF a between 28 86 If it’s between “28” and “86” (those that say “good”) SCORE a z Score this number END End the IF condition END End REPEAT loop COUNT z between 28 30 k How many of the SCORED numbers were between “28--30” (faulty, “OK”) SIZE z s How many numbers were scored DIVIDE k s kk What proportion were faulty, “OK” PRINT kk Print result Result kk = 0.039 Hence, we estimate that the probability that a car identified as “OK” will really be faulty is .039. Estimating Driving Risk for Insurance Purposes Another sort of introductory problem, following after Feller (1968), p 122: A mutual insurance company charges its members according to the risk of having an auto accident. It is known that there are two classes of people—80 percent of the population with good driving judgment and with a probability of .06 of having an accident each year, and 20 percent with poor judgment and a probability of .6 of having an accident each year. The company’s policy is to charge (in addition to a fee to cover overhead expenses) $100 for each percent of risk, i. e., a driver with a probability of .6 should pay 60*$100 = $6000. If nothing is known of a driver except that he had an accident last year, what fee should he pay? This procedure will produce the answer: Construct bucket A with 6 red and 94 green balls, and bucket B with 60 red and 40 green balls. Randomly select a bucket with probabilities for A = .8 and B = .2, and record the bucket chosen. Select a ball at random from the chosen bucket. If the ball is green, go back to step 2; if red, continue to step 4. In either case, replace the ball selected. Select another ball from the bucket chosen in step 2. If it is red, record “Y,” if green, record “N.” Repeat steps 2 - 4 perhaps 1000 times, and determine the proportion “Y” in relation to (Y + N). The final answer should be approximately $4450. Screening for Disease This is a classic Bayesian problem (quoted by Tversky and Kahnemann, 1982, pp. 153-154, from Cascells, Schoenberger, and Grayboys, 1978, p. 999): If a test to detect a disease whose prevalence is 1/1000 has a false positive rate of 5%, what is the chance that a person found to have a positive result actually has the disease, assuming you know nothing about the persons’s symptoms or signs? Tversky and Kahnemann note that among the respondents— students and staff at Harvard Medical School—“the most common response, given by almost half of the participants, was 95%,” very much the wrong answer. To obtain an answer by simulation, we may rephrase the question above with (hypothetical) absolute numbers as follows: If a test to detect a disease whose prevalence has been estimated to be about 100,000 in the population of 100 million persons over age 40 (that is, about 1 in a thousand) has been observed to have a false positive rate of 60 in 1200 observations, and never gives a negative result if a person really has the disease, what is the chance that a person found to have a positive result actually has the disease, assuming you know nothing about the persons’s symptoms or signs? If the raw numbers are not available, the problem can be phrased in such terms as “about 1 case in 1000” and “about 5 false positives in 100 cases.”) One may obtain an answer as follows: Construct bucket A with 999 white beads and 1 black bead, and bucket B with 95 green beads and 5 red beads. A more complete problem that also discusses false negatives would need a third bucket. Pick a bead from bucket A. If black, record “T,” replace the bead, and end the trial. If white, continue to step 3. If a white bead is drawn from bucket A, select a bead from bucket B. If red, record “F” and replace the bead, and if green record “N” and replace the bead. Repeat steps 2-4 perhaps 10,000 times, and in the results count the proportion of “T”s to (“T”s plus “F”s) ignoring the “N”s). Of course 10,000 draws would be tedious, but even after a few hundred draws a person would be likely to draw the correct conclusion that the proportion of “T”s to (“T”s plus “F”s) would be small. And it is easy with a computer to do 10,000 trials very quickly. Note that the respondents in the Cascells et al. study were not naive; the medical staff members were supposed to understand statistics. Yet most doctors and other personnel offered wrong answers. If simulation can do better than the standard deductive method, then simulation would seem to be the method of choice. And only one piece of training for simulation is required: Teach the habit of saying “I’ll simulate it” and then actually doing so. Fundamental problems in statistical practice Box and Tiao begin their classic exposition of Bayesian statistics with the analysis of a famous problem first published by Fisher (1959). …there are mice of two colors, black and brown. The black mice are of two genetic kinds, homozygotes ( BB ) and heterozygotes ( Bb ), and the brown mice are of one kind ( bb ). It is known from established genetic theory that the probabilities associated with offspring from various matings are as \\[in Table 25-1\\]: Suppose we have a “test” mouse which is black and has been produced by a mating between two ( Bb ) mice. Using the information in the last line of the table, it is seen that, in this case, the prior probabilities of the test mouse being homozygous ( BB ) and heterozygous ( Bb ) are precisely known, and are 1/3 and 2/3 respectively. Given this prior information, Fisher supposed that the test mouse was now mated with a brown mouse and produced (by way of data) seven black offspring. One can then calculate, as Fisher (1959, p.17) did, the probabilities, posterior to the data, of the test mouse being homozygous ( BB ) and heterozygous ( Bb ) using Bayes’ theorem. . . We see that, given the genetic characteristics of the offspring, the mating results of 7 black offspring changes our knowledge considerably about the test mouse being ( BB ) or ( Bb ), from a prior probability ratio of 2:1 in favor of ( Bb ) to a posterior ratio of 64:1 against it (1973, pp. 12-14). Let us begin, as do Box and Tiao, by restricting our attention to the third line in Table 25-1, and let us represent those results with 4 balls—1 black with “BB” painted on it, 2 black with “Bb” painted on them, and 1 brown which we immediately throw away because we are told that the “test mouse” is black. The remaining 3 (black) balls are put into a bucket labeled “test.” Table 25-1 Probabilities for Genetic Character of Mice Offspring Mice BB (black) Bb (black) bb (brown) BB mated with bb 0 1 0 Bb mated with bb 0 ½ ½ Bb mated with Bb ¼ ½ ¼ Source: Box and Tiao, 1973, pp. 12-14 From prior knowledge we know that a BB black mouse mated with a bb brown mouse will produce all black mice (line 1 in the table), and a Bb black mouse mated with a bb brown mouse will produce 50 percent black mice and 50 percent brown mice. We therefore construct two more buckets, one with a single black ball (the bucket labeled “BB”) and the other with one black ball and one brown ball (the bucket labeled “Bb”). We now have three buckets. Take a ball from bucket “test.” If its label is “BB,” record that fact, take a ball (the only ball, which is black) from the BB bucket, record its color (we knew this already), and replace the ball into the BB bucket; the overall record of this trial is “BB-black.” If the ball drawn from bucket “test” says “Bb,” draw a ball from the Bb bucket, record, and replace; the record will either be “Bb-black” or “Bb-brown.” Repeat step 3 seven times. Examine whether the record of the seven balls drawn from the BB and Bb buckets are all black; if so, record “Y,” otherwise “N.” Repeat steps 3-5 perhaps 1000 times. Ignore all “N” records. Proceeding now if the result of step 5 is “Y”: Count the number of cases which are BB and the number which are Bb. The proportions of BB/“Y” and Bb/“Y” trials are the probabilities that the test mouse is BB and Bb respectively. Creating the correct simulation procedure is not easy, because Bayesian reasoning is very subtle—a reason it has been the cause of controversy for more than two centuries. But it certainly is not easier to create a correct procedure using analytic tools (except in the cookbook sense of plug in and pray). And the difficult mathematics that underlie the analytic method (see e. g., Box and Tiao, Appendix A1.1) make it almost impossible for the statistician to fully understand the procedure from be- ginning to end; if one is interested in insight, the simulation procedure might well be preferred. 2 A computer program to speed the above steps appears in the Appendix to this chapter. The result found with a set of 1000 repetitions is .987. 28.2 Problems based on normal and other distributions This section should be skipped by all except advanced practitioners of statistics. Much of the work in Bayesian analysis for scientific purposes treats the combining of prior distributions having Normal and other standard shapes with sample evidence which may also be represented with such standard functions. The mathematics involved often is formidable, though some of the calculational formulas are fairly simple and even intuitive. These problems may be handled with simulation by replacing the Normal (or other) distribution with the original raw data when data are available, or by a set of discrete sub-universes when distributions are subjective. Measured data from a continuous distribution present a special problem because the probability of any one observed value is very low, often approaching zero, and hence the probability of a given set of observed values usually cannot be estimated sensibly; this is the reason for the conventional practice of working with a continuous distribution itself, of course. But a simulation necessarily works with discrete values. A feasible procedure must bridge this gulf. The logic for a problem of Schlaifer’s will be only be sketched out. The procedure is rather novel, but it has not heretofore been published and therefore must be considered tentative and requiring particular scrutiny. An Intermediate Problem in Conditional Probability Schlaifer employs a quality-control problem for his leading example of Bayesian estimation with Normal sampling. A chemical manufacturer wants to estimate the amount of yield of a crucial ingredient X in a batch of raw material in order to decide whether it should receive special handling. The yield ranges between 2 and 3 pounds (per gallon), and the manufacturer has compiled the distribution of the last 100 batches. The manufacturer currently uses the decision rule that if the mean of nine samples from the batch (which vary only because of measurement error, which is the reason that he takes nine samples rather than just one) indicates that the batch mean is greater than 2.5 gallons, the batch is accepted. The first question Schlaifer asks, as a sampling-theory waystation to the more general question, is the likelihood that a given batch with any given yield—say 2.3 gallons—will produce a set of samples with a mean as great or greater than 2.5 gallons. We are told that the manufacturer has in hand nine samples from a given batch; they are 1.84, 1.75, 1.39, 1.65, 3.53, 1.03, 2.73, 2.86, and 1.96, with a mean of 2.08. Because we are also told that the manufacturer considers the extent of sample variation to be the same at all yield levels, we may—if we are again working with 2.3 as our example of a possible universe—therefore add (2.3 minus 2.08 =) 0.22 to each of these nine observations, so as to constitute a bootstrap-type universe; we do this on the grounds that this is our best guess about the constitution of that distribution with a mean at (say) 2.3. We then repeatedly draw samples of nine observations from this distribution (centered at 2.3) to see how frequently its mean exceeds 2.5. This work is so straightforward that we need not even state the steps in the procedure. Estimating the Posterior Distribution Next we estimate the posterior distribution. Figure 25-1 shows the prior distribution of batch yields, based on 100 previous batches. .2 .1 0 0 2.0 2.2 2.4 2.6 2.8 3.0 3.2 Figure 25-1 Notation: S m = set of batches (where total S = 100) with a particular mean m (say, m = 2.1). x i = particular observation (say, x 3 = 1.03). s = the set of x i . We now perform for each of the S m (categorized into the tenth-of-gallon divisions between 2.1 and 3.0 gallons), each corresponding to one of the yields ranging from 2.1 to 3.0, the same sort of sampling operation performed for S m=2.3 in the previous problem. But now, instead of using the manufacturer’s decision criterion of 2.5, we construct an interval of arbitrary width around the sample mean of 2.08—say at .1 intervals from 2.03 to 2.13—and then work with the weighted proportions of sample means that fall into this interval. Using a bootstrap-like approach, we presume that the sub-universe of observations related to each S m equals the mean of that S m —(say, 2.1) plus (minus) the mean of the x i (equals 2.05) added to (subtracted from) each of the nine x i , say, 1.03 + .05 = 1.08. For a distribution centered at 2.3, the values would be (1.84 + .22 = 2.06, 1.75 + .22 = 1.97…). Working with the distribution centered at 2.3 as an example: Constitute a universe of the values (1.84+.22=2.06, 1.75 + .22 = 1.97…). Here we may notice that the variability in the sample enters into the analysis at this point, rather than when the sample evidence is combined with the prior distribution; this is in contrast to conventional Bayesian practice where the posterior is the result of the prior and sample means weighted by the reciprocals of the variances (see e.g. Box-Tiao, 1973, p. 17 and Appendix A1.1). Draw nine observations from this universe (with replacement, of course), compute the mean, and record. Repeat step 2 perhaps 1000 times and plot the distribution of outcomes. Compute the percentages of the means within (say) .5 on each side of the sample mean, i. e. from 2.03–2.13. The resulting number—call it UP i —is the un-standardized (un-normalized) effect of this sub-distribution in the posterior distribution. Repeat steps 1-5 to cover each other possible batch yield from 2.0 to 3.0 (2.3 was just done). Weight each of these sub-distributions—actually, its UP i — by its prior probability, and call that WP i -. Standardize the WP i s to a total probability of 1.0. The result is the posterior distribution. The value found is 2.283, which the reader may wish to compare with a theoretically-obtained result (which Schlaifer does not give). This procedure must be biased because the numbers of “hits” will differ between the two sides of the mean for all sub-distributions except that one centered at the same point as the sample, but the extent and properties of this bias are as-yet unknown. The bias would seem to be smaller as the interval is smaller, but a small interval requires a large number of simulations; a satisfactorily narrow interval surely will contain relatively few trials, which is a practical problem of still-unknown dimensions. Another procedure—less theoretically justified and probably more biased—intended to get around the problem of the narrowness of the interval, is as follows: 5a. Compute the percentages of the means on each side of the sample mean, and note the smaller of the two (or in another possible process, the difference of the two). The resulting number—call it UP i —is the un-standardized (un-normalized) weight of this sub-distribution in the posterior distribution. Another possible criterion—a variation on the procedure in 5a—is the difference between the two tails; for a universe with the same mean as the sample, this difference would be zero. Conclusion All but the simplest problems in conditional probability are confusing to the intuition even if not difficult mathematically. But when one tackles Bayesian and other problems in probability with experimental simulation methods rather than with logic, neither simple nor complex problems need be difficult for experts or beginners. This chapter shows how simulation can be a helpful and illuminating way to approach problems in Bayesian analysis. Simulation has two valuable properties for Bayesian analysis: It can provide an effective way to handle problems whose analytic solution may be difficult or impossible. 2) Simulation can provide insight to problems that otherwise are difficult to understand fully, as is peculiarly the case with Bayesian analysis. Bayesian problems of updating estimates can be handled easily and straightforwardly with simulation, whether the data are discrete or continuous. The process and the results tend to be intuitive and transparent. Simulation works best with the original raw data rather than with abstractions from them via percentages and distributions. This can aid the understanding as well as facilitate computation. 28.3 Endnotes Darrell Huff provides the quote but without reference: “This branch of mathematics \\[probability\\] is the only one, I believe, in which good writers frequently get results entirely erroneous.” (Huff, 1959, frontispage) We can use a similar procedure to illustrate an aspect of the Bayesian procedure that Box and Tiao emphasize, its sequentially-consistent character. First let us carry out the above procedure but observe only three black balls in a row. The program to be used is the same except for the insertion of “3” for “7” where “7” appears. We then estimate the probability for BB, which turns out to be about 1/5 instead of about 1/65. We then substitute for bucket A a bucket A’ with appropriate numbers of black Bb’s and black BB’s, to represent the “updated” prior probability. We may then continue by substituting “4” for “3” above (to attain a total of seven observed black balls), and find that the probability is about what it was when we observed 7 black balls in a single sample (1/65). This shows that the Bayesian procedure accumulates information without “leakage” and with consistency. References "],["exercise-solutions.html", "29 Exercise Solutions 29.1 Solution 18-2 29.2 Solution 21-1 29.3 Solution 21-2 29.4 Solution 21-3 29.5 Solution 23-1 29.6 Solution 23-2 29.7 Solution 23-3 29.8 Solution 23-4", " 29 Exercise Solutions 29.1 Solution 18-2 Results: INTERVAL = -0.25921 0.039083 \\[estimated 95 percent confidence interval\\] 29.2 Solution 21-1 Result: INTERVAL = 0.035 0.105 \\[estimated 95 percent confidence interval\\] 29.3 Solution 21-2 We use the “bootstrap” technique of drawing many bootstrap re-samples with replacement from the original sample, and observing how the re-sample means are distributed. Do 1000 trials or simulations Draw 20 lifetimes from a, randomly and with replacement Find the average lifetime of the 20 Keep score Graph the experiment results Identify the 2.5th and 97.5th percentiles. These percentiles will enclose 95 percent of the resample means. Result: INTERVAL = 27.7 30.05 \\[estimated 95 percent confidence interval\\] 29.4 Solution 21-3 Result: INTERVAL = 0.0187 0.0219 \\[estimated 95 percent confidence interval\\] 29.5 Solution 23-1 Create two groups of paper cards: 25 with participation rates, and 25 with the spread values. Arrange the cards in pairs in accordance with the table, and compute the correlation coefficient between the shuffled participation and spread variables. Shuffle one of the sets, say that with participation, and compute correlation between shuffled participation and spread. Repeat step 2 many, say 1000, times. Compute the proportion of the trials in which correlation was at least as negative as that for the original data. compute correlation - it’s -.37 shuffle the participation rates compute re-sampled correlation keep the value in the scoreboard count the trials when result &lt;= -.37 compute the proportion of such trials Conclusion: The results of 5 Monte Carlo experiments each of a thousand such simulations are as follows: From this we may conclude that the voter participation rates probably are negatively related to the vote spread in the election. The actual value of the correlation (-.37398) cannot be explained by chance alone. In our Monte Carlo simulation of the null-hypothesis a correlation that negative is found only 3 percent–4 percent of the time. Distribution of the test statistic’s value in 1000 independent trials corresponding to the null-hypothesis: 29.6 Solution 23-2 Result: kk = 0 Interpretation: In 1000 simulations, random shuffling never produced a value as high as observed. Therefore, we conclude that random chance could not be responsible for the observed degree of correlation. 29.7 Solution 23-3 Result: kk = .001 Interpretation: A correlation coefficient as high as the observed value (.62) occurred only 1 out of 1000 times by chance. Hence, we rule out chance as an explanation for such a high value of the correlation coefficient. 29.8 Solution 23-4 read data from file compute correlation stat (it’s .419) shuffle money supply values compute correlation keep the value in a scoreboard Distribution of the correlation after permutation of the data: Result: prob = .001 Interpretation: The observed correlation (.419) between the exchange rate and the money supply is seldom exceeded by random experiments with these data. Thus, the observed result 0.419 cannot be explained by chance alone and we conclude that it is statistically significant. "],["technical-note-to-the-professional-reader.html", "30 Technical Note to the Professional Reader", " 30 Technical Note to the Professional Reader The material presented in this book fits together with the technical literature as follows: Though I had proceeded from first principles rather than from the literature, I have from the start cited work by J.H. Chung and D.A.S. Fraser (1958) and Meyer Dwass (1957). They suggested taking samples of permutations in a two-sample test as a way of extending the applicability of Fisher’s randomization test. Resampling with replacement from a single sample to determine sample statistic variability was suggested by Simon (1969). Independent work by Efron (1979) explored the properties of this technique (Efron termed it the “bootstrap”) and lent it theoretical support. The notion of using these techniques routinely and in preference to conventional techniques based on Gaussian assumptions was suggested by Simon (1969) and by Simon, Atkinson, and Shevokas (1976). "],["acknowledgements.html", "31 Acknowledgements", " 31 Acknowledgements Many people have helped in the long evolution of this work. First was the late Max Beberman, who in 1967 immediately recognized the potential of resampling statistics for high school students as well as for all others. Louis Guttman and Joseph Doob provided important encouragement about the theoretical and practical value of resampling statistics. Allen Holmes cooperated with me in teaching the first class at University High School in Urbana, Illinois, in 1967. Kenneth Travers found and supervised several PhD students—David Atkinson and Carolyn Shevokas outstanding among them—who experimented with resampling statistics in high school and college classrooms and proved its effectiveness; Travers also carried the message to many secondary school teachers in person and in his texts. In 1973 Dan Weidenfield efficiently wrote the first program for the mainframe (then called “Simple Stats”). Derek Kumar wrote the first interactive program for the Apple II. Chad McDaniel developed the IBM version, with touchup by Henry van Kuijk and Yoram Kochavi. Carlos Puig developed the powerful 1990 version of the program. William E. Kirwan, Robert Dorfman, and Rudolf Lamone have provided their good offices for us to harness the resources of the University of Maryland and, in particular, the College of Business and Management. Terry Oswald worked day and night with great dedication on the program and on commercial details to start the marketing of RESAMPLING STATS. In mid-1989, Peter Bruce assumed the overall stewardship of RESAMPLING STATS, and has been proceeding with energy, good judgment, and courage. He has contributed to this volume in many ways, always excellently (including the writing and re-writing of programs, as well as explanations of the bootstrap and of the interpretation of p-values). Vladimir Koliadin wrote the code for several of the problems in this edition, and Cheinan Marks programmed the Windows and Macintosh versions of Resampling Stats. Toni York handled the typesetting and desktop publishing through various iterations, Barbara Shaw provided expert proofreading and desktop publishing services for the second printing of the second edition, and Chris Brest produced many of the figures. Thanks to all of you, and to others who should be added to the list. "],["references.html", "References", " References Efron, Bradley. 1982. The Jackknife, the Bootstrap, and Other Resampling Plans. Vol. 38. SIAM. Feller, William. 1968. An Introduction to Probability Theory and Its Applications: Volume I. 3rd ed. Vol. 1. New York: John Wiley &amp; Sons. Kinsey, Alfred C, Wardell B Pomeroy, and Clyde E Martin. 1948. “Sexual Behavior in the Human Male.” W. B. Saunders Company. https://books.google.co.uk/books?id=pfMKrY3VvigC. Kotz, Samuel, and Norman Lloyd Johnson. 1992. Breakthroughs in Statistics. New York: Springer-Verlag. McCabe, George P, and Linda Doyle McCabe. 1989. Instructor’s Guide with Solutions for Introduction to the Practice of Statistics. New York: W. H. Freeman. Mosteller, Frederick, and Robert EK Rourke. 1973. Sturdy Statistics: Nonparametrics and Order Statistics. Addison-Wesley Publishing Company. Mosteller, Frederick, Robert E. K. Rourke, and George Brinton Thomas Jr. 1970. Probability with Statistical Applications. 2nd ed. Reading, Mass.: Addison-Wesley Publishing Company. Noreen, Eric W. 1989. Computer-Intensive Methods for Testing Hypotheses. New York: John Wiley &amp; Sons. Savage, Leonard J. 1972. The Foundations of Statistics. New York: Dover Publications, Inc. Savant, Marilyn vos. 1990. “Ask Marilyn.” 1990. https://web.archive.org/web/20160318182523/http://marilynvossavant.com/game-show-problem. Schuh, Fred. 1968. The Master Book of Mathematical Recreations. New York: Dover Publications, Inc. https://books.google.co.uk/books?id=Uk7a0IYXvIgC. Selvin, Steve. 1975. “Letters to the Editor.” The American Statistician 29 (1): 67. http://www.jstor.org/stable/2683689. Simon, Julian L, David T Atkinson, and Carolyn Shevokas. 1976. “Probability and Statistics: Experimental Results of a Radically Different Teaching Method.” The American Mathematical Monthly 83 (9): 733–39. Simon, Julian L, and Allen Holmes. 1969. “A New Way to Teach Probability Statistics.” The Mathematics Teacher 62 (4): 283–88. Simon, Julian Lincoln. 1969. Basic Research Methods in Social Science. 1st ed. New York: Random House. Simon, Julian Lincoln, and Paul Burstein. 1985. Basic Research Methods in Social Science. 3rd ed. New York: Random House. Tukey, John W. 1977. Exploratory Data Analysis. Reading, MA, USA: Addison-Wesley. Vazsonyi, Andrew. 1999. “Which Door Has the Cadillac.” Decision Line 30 (1): 17–19. https://web.archive.org/web/20140413131827/http://www.decisionsciences.org/DecisionLine/Vol30/30_1/vazs30_1.pdf. Wallis, Wilson Allen, and Harry V Roberts. 1957. Statistics, a New Approach. London: Methuen; Co. Ltd. Whitworth, William Allen. 1897. DCC Exercises in Choice and Chance. Cambridge, UK: Deighton Bell; Co. https://archive.org/details/dccexerciseschoi00whit. Wonnacott, Thomas H, and Ronald J Wonnacott. 1990. Introductory Statistics. 5th ed. New York: John Wiley &amp; Sons. "]]
